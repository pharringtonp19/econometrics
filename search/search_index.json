{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Summary","text":"Econometrics Summary <p>A course where we learn how to implement () statistical models and interpret () these models in various contexts.   </p> Components <ul> <li> <p>Approximation</p> <p>TBD</p> <p> Getting started</p> </li> <li> <p>Optimization</p> <p>TBD</p> <p> Getting started</p> </li> <li> <p>Probability Theory</p> <p>TBD</p> <p> Getting started</p> </li> <li> <p>Programming</p> <p>TBD</p> <p> Getting started</p> </li> </ul>"},{"location":"chapters/approximation/clusters/","title":"Clusters","text":""},{"location":"chapters/approximation/clusters/#convex-representation","title":"Convex Representation","text":"<p>The Caratheodori Theorem tells us that any point \\(x \\in \\textrm{Conv}(T)\\), where \\(T \\subset \\mathcal{R}^n\\), can be represented as a convex combination of at most \\(n+1\\) points.1</p> <p>So for example let's say that our features \\(x \\in \\mathcal{R}^n\\). Then for any point in the convex hull of the training set can be represented as the convex combination of at most ten points</p> \\[x = \\sum _{i=1}^{11} \\alpha_i(x)x_i^*(x), \\quad x \\in \\mathcal{R}^{10}\\] <ol> <li> <p>Reference: See here \u21a9</p> </li> </ol>"},{"location":"chapters/approximation/curse_of_dimensionality/","title":"Curse of Dimensionality","text":"<p>A pure sampling phenomena </p>"},{"location":"chapters/approximation/curse_of_dimensionality/#numerical-integration","title":"Numerical Integration","text":"\\[\\int _{[0,1]^d}f dx\\]"},{"location":"chapters/approximation/curse_of_dimensionality/#monte-carlo-integration","title":"Monte-Carlo Integration","text":"\\[\\begin{align*}\\hat{\\theta} &amp;= \\frac{1}{n}\\sum _{i=1}^n \\big(f(X_i) + \\varepsilon_i) \\\\  &amp;= \\frac{1}{n}\\sum _{i=1}^n f(X_i) + \\frac{1}{n}\\sum _{i=1}^n \\varepsilon_i \\\\ \\\\  \\mathbb{E}[\\hat{\\theta}] &amp;= \\mathbb{E} \\Bigg[\\frac{1}{n}\\sum _{i=1}^n f(X_i) + \\frac{1}{n}\\sum _{i=1}^n \\varepsilon_i  \\Bigg] \\\\  &amp;= \\frac{1}{n}\\sum _{i=1}^n \\mathbb{E}\\big[f(X_i)\\big] + \\frac{1}{n}\\sum _{i=1}^n \\mathbb{E}\\underbrace{\\big[\\varepsilon _i\\big]}_{=0} \\\\ &amp;= \\mathbb{E}\\big[f(X_i)\\big] \\\\ \\\\  \\mathbb{E}\\big[(\\hat{\\theta} - \\theta_0 \\big)^2] &amp;= \\textrm{Var}(\\hat{\\theta})\\end{align*}\\]"},{"location":"chapters/approximation/introduction/","title":"Introduction","text":"<p>Abstract</p> <p>In this note we introduce the fundamentals of the learning problem </p>"},{"location":"chapters/approximation/introduction/#probability-space","title":"Probability Space","text":"Warning <p>Is the Borel Sigma Algebra well defined here? Should we add restrictions on \\(\\mathcal{X}, \\mathcal{Y}\\)?</p> \\[\\Big( \\mathcal{X} \\times \\mathcal{Y}, \\mathcal{B}(\\mathcal{X} \\times \\mathcal{Y}), \\mathbb{P}\\Big)\\]"},{"location":"chapters/approximation/introduction/#function-space-hypothesis-class-model-class","title":"Function Space/ Hypothesis Class/ Model Class","text":"Warning <p>What additional restrictions should we place on \\(\\mathcal{H}\\)?</p> \\[ \\mathcal{H} := \\{h \\mid h : \\mathcal{X} \\to \\mathcal{Y} \\}\\]"},{"location":"chapters/approximation/introduction/#loss-function","title":"Loss Function","text":"Warning <p>Make note on parameterization</p> \\[\\begin{align*} &amp;l : \\mathcal{H} \\to \\mathcal{X} \\to \\mathcal{Y} \\\\ &amp;l(h, x, y) = (y - h(x))^2 \\end{align*}\\]"},{"location":"chapters/approximation/introduction/#population-risk","title":"Population Risk","text":"\\[\\begin{align*} &amp;L :: \\mathcal{H} \\to \\mathcal{R}_+ \\\\  &amp;L(h) := \\underset{(x,y)\\sim p}{\\mathbb{E}} \\big[l(h, x, y)\\big]\\end{align*}\\]"},{"location":"chapters/approximation/introduction/#empirical-risk","title":"Empirical Risk","text":"\\[ \\begin{align*} &amp;\\hat{L} :: \\{X, Y\\}^n \\to \\Theta \\to \\mathcal{R}_+ \\\\ &amp;\\hat{L}(\\{x_i, y_i\\}_{i=1}^n, \\theta) = \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i) \\end{align*}\\] Partial Evaluation <p>Partially evaluated at \\(\\theta\\), \\(\\hat{L}\\)  is a random variable. Taking its expectation</p> \\[ \\begin{align*} \\mathbb{E}\\big[ \\hat{L} _{\\theta}\\big] &amp;= \\mathbb{E}\\big[ \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i)\\big] \\\\  &amp;= \\frac{1}{n} \\sum _i \\mathbb{E}\\big[ l(\\theta, x_i, y_i)\\big] \\\\  &amp;= L(\\theta) \\end{align*}\\] <p>But we are not really interested in this relationship, becuase \\(\\hat{L}\\) is not partially evaluated in practice. In practice, we use our training data to determine \\(\\theta\\). That is we have an algorithm \\(\\mathcal{A}\\). </p>"},{"location":"chapters/approximation/introduction/#algorithm","title":"Algorithm","text":"\\[ \\begin{align*} \\mathcal{A} :: \\{\\mathcal{X}, \\mathcal{Y}\\}^n \\to \\Theta  \\end{align*} \\] <ul> <li>Empirical Risk Minimization</li> </ul> \\[ \\begin{align*} &amp;\\textrm{ERM} :: \\{X, Y\\}^n \\to \\Theta  \\\\ &amp;\\textrm{ERM}(\\{x_i, y_i\\}_{i=1}^n) = \\underset{\\theta \\in \\Theta}{\\textrm{minimize}} \\ \\hat{L}(\\{x_i, y_i\\}_{i=1}^n, \\theta)   \\end{align*}\\] <ul> <li> <p>Which is just a random variable. We can evaluate it as follows: </p> \\[ \\begin{align*} \\mathbb{E} \\big[ L \\circ \\mathcal{A} \\big] \\end{align*} \\] </li> <li> <p>Excess Risk:</p> </li> </ul> \\[  \\begin{align*} &amp;E :: \\mathcal{H} \\to \\mathcal{R}_+ \\\\ &amp;E(h) = L(h) - \\underset{g \\in \\mathcal{H}}{\\inf} L(g) \\end{align*}\\]"},{"location":"chapters/approximation/kernels/","title":"Kernels","text":"<p>These notes are taken from the following lectures: Lecture</p>"},{"location":"chapters/approximation/kernels/#definitions","title":"Definitions","text":"Dual Space <p>Let \\(X\\) be a vector space. Then the dual space of \\(X\\), denoted by \\(X^*\\), is the set of linear bounded functions on \\(X\\). </p>"},{"location":"chapters/approximation/kernels/#reproducing-kernel-hilbert-spaces","title":"Reproducing Kernel Hilbert Spaces","text":"<ul> <li>Let \\(\\mathcal{X}\\) be a set </li> <li>Let \\(F(\\mathcal{X}, \\mathcal{R})\\) be the vector space of funcions defined on \\(\\mathcal{X}\\). i.e. </li> </ul> \\[f_1, f_2 \\in F(\\mathcal{X}, \\mathcal{R}) \\implies \\alpha f_1 + \\beta f_2 \\in F(\\mathcal{X}, \\mathcal{R})\\] <ul> <li> <p>Then \\(\\mathcal{H}(\\mathcal{X}, \\mathcal{R}) \\subset F(\\mathcal{X}, \\mathcal{R})\\) is a Reproducing Kernel Hilbert Space if </p> <ol> <li>\\(\\mathcal{H}(\\mathcal{X}, \\mathcal{R})\\) is a subspace of \\(F(\\mathcal{X}, \\mathcal{R})\\)</li> <li>\\(\\Big(\\mathcal{H}(\\mathcal{X}, \\mathcal{R}), \\langle \\cdot, \\cdot  \\rangle _{\\mathcal{H}} \\Big)\\) is a Hilbert Space</li> <li>Evaluation Functionals, \\(\\textrm{Apply}_x\\), are continuous</li> </ol> \\[ \\textrm{Apply} : \\mathcal{X} \\to \\mathcal{H}(\\mathcal{X}, \\mathcal{R}) \\to \\mathcal{R}  \\] </li> </ul> <p>Note: </p> <ul> <li>We can think of Euclidean Spaces as a function space.</li> </ul> \\[\\mathcal{R} ^n \\equiv \\Big( F(\\textrm{Fin} \\ n, \\mathcal{R}), \\langle z_1, z_2  \\rangle _{F} :=  \\sum _{i=1}^n z_1(i)z_2(i)\\Big)\\] <ul> <li>We can generalize this structure to  \\(l^2(\\mathcal{X})\\)</li> </ul> \\[\\begin{align*} l^2(\\mathcal{X}):= \\Big\\{ f \\mid f:\\mathcal{X} \\to \\mathcal{R}, \\quad  \\sum _{x\\in \\mathcal{X}} |f(x)|^2 &lt; \\infty \\Big\\} \\end{align*}\\] <ul> <li>This structure (set \\(+\\) the norm/inner product)1 is an RKHS since </li> </ul> \\[\\| E_x \\| \\leq \\| f \\| _{l^2(\\mathcal{X})}\\] <p>Consider the following function </p> \\[\\begin{align*} &amp;\\Lambda :: \\mathcal{H} \\to \\mathcal{H} \\to \\mathcal{R}\\\\  &amp;\\Lambda \\ y \\ x = \\langle x, y \\rangle _{\\mathcal{H}} \\end{align*}\\] <p>We can re-write the signature of the function as follows:</p> \\[\\begin{align*} &amp;\\Lambda :: \\mathcal{H} \\to \\mathcal{H}^*\\\\  \\end{align*}\\] <p>If \\(H\\) is a RKHS, then by definition, \\(\\textrm{Apply} \\ x\\) is a linear bounded functional. By Reisz representation theorem, </p> \\[\\textrm{Apply} \\ x \\ f = \\langle r \\ x, f \\rangle _{\\mathcal{H}} = f \\ x \\] <p>Then we can define the Kernel as follows: </p> \\[\\begin{align*} &amp;K :: \\mathcal{X} \\to \\mathcal{X} \\to \\mathcal{R}  \\\\  &amp;K \\ x \\ y = r \\ x \\ y = \\langle r \\ y,  r \\ x \\rangle  \\end{align*}\\] <ol> <li> <p>I really think the key part of this structure is the inner product\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/optimization/differentiation/","title":"Differentiation","text":"<p>\"If you have a tricky problem to solve, finding a language in which the solution to that problem is compositional is like the most important step. Well, defining the problem clearly is the most important step. The second one is finding a vocabulary in which it's compositional.\"1</p> <p>This entire section is taken from Conal Elliot's presentation titled: \"Automatic Differentiation Made Easy Via Category Theory\"</p> <ul> <li> <p>A Derivative is a linear map </p> </li> <li> <p>Cateogy Theory is the abstract algebra of functions</p> </li> <li> <p>Let \\(a,b\\) be Banach spaces (complete normed vector spaces)</p> </li> </ul> \\[\\mathcal{D} :: (a \\to b) \\to (a \\to (a -\\circ b))\\] <ul> <li>Terminology: The derivative of \\(f\\) at \\(a\\)</li> </ul> \\[\\underset{\\varepsilon \\to 0}{\\lim} \\frac{ \\| f \\ (a + \\varepsilon) - f \\ a + \\mathcal{D} \\ f \\ a \\ \\varepsilon \\|}{\\| \\varepsilon \\|} = 0\\] <p>Differentiation preserves parallel composition </p> \\[\\begin{align*}&amp;(\\triangle) :: (a \\to b) \\to (a \\to d) \\to (a \\to b \\times d) \\\\  &amp;(f \\ \\triangle \\ g) \\ a = (f \\ a, g \\ a)  \\\\  &amp;\\mathcal{D} \\ (f \\ \\triangle \\ g)  = \\mathcal{D} \\ f \\ \\triangle \\ \\mathcal{D} \\ g \\end{align*}\\] \\[\\begin{align*} &amp;\\hat{\\mathcal{D}} :: (a \\to b) \\to (a \\to (b \\times (a \\to b))) \\\\  &amp; \\hat{\\mathcal{D}} \\ f = f \\ \\triangle \\ \\mathcal{D} \\ f\\end{align*}\\] <ol> <li> <p>Reference: See here \u21a9</p> </li> </ol>"}]}