{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Summary","text":"Econometrics <p>Applied Econometrics is a highly personalized endevour</p> Introduction <p>Applied econometrics is fundamentally about interpretation -- how to interpret the result of a statistical procedure in a given context. Generally, though, there isn't a \"valid\" or correct interpretation. This type of work is actually quite similair to a litterature or art-history class1, where the aim is to develop/enhance your ability to interpret different works/results. </p> <p>The general setup is as follows: we have beliefs about the world; we have beliefs about how a statical procedure behaves under certain conditions' the statistical procedure is applied to a data set providing use with numerical \"results\" which ultimately lead to new beliefs about the world. </p> <p>This is a highly personal process because your new beliefs depend on both your original beliefs about the world and your beliefs about the statistical procedure. Elaborate Here</p> Components <ul> <li> <p>Approximation</p> <p>TBD</p> <p> Getting started</p> </li> <li> <p>Optimization</p> <p>TBD</p> <p> Getting started</p> </li> <li> <p>Probability Theory</p> <p>TBD</p> <p> Getting started</p> </li> <li> <p>Programming</p> <p>TBD</p> <p> Getting started</p> </li> </ul> <ol> <li> <p>I have never taken an art-history class so I might be wrong about this!\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/approximation/clusters/","title":"Clusters","text":""},{"location":"chapters/approximation/clusters/#convex-representation","title":"Convex Representation","text":"<p>The Caratheodori Theorem tells us that any point \\(x \\in \\textrm{Conv}(T)\\), where \\(T \\subset \\mathcal{R}^n\\), can be represented as a convex combination of at most \\(n+1\\) points.1</p> <p>So for example let's say that our features \\(x \\in \\mathcal{R}^n\\). Then for any point in the convex hull of the training set can be represented as the convex combination of at most ten points</p> \\[x = \\sum _{i=1}^{11} \\alpha_i(x)x_i^*(x), \\quad x \\in \\mathcal{R}^{10}\\] <ol> <li> <p>Reference: See here \u21a9</p> </li> </ol>"},{"location":"chapters/approximation/curse_of_dimensionality/","title":"Curse of Dimensionality","text":"<p>A pure sampling phenomena </p>"},{"location":"chapters/approximation/curse_of_dimensionality/#numerical-integration","title":"Numerical Integration","text":"\\[\\int _{[0,1]^d}f dx\\]"},{"location":"chapters/approximation/curse_of_dimensionality/#monte-carlo-integration","title":"Monte-Carlo Integration","text":"\\[\\begin{align*}\\hat{\\theta} &amp;= \\frac{1}{n}\\sum _{i=1}^n \\big(f(X_i) + \\varepsilon_i) \\\\  &amp;= \\frac{1}{n}\\sum _{i=1}^n f(X_i) + \\frac{1}{n}\\sum _{i=1}^n \\varepsilon_i \\\\ \\\\  \\mathbb{E}[\\hat{\\theta}] &amp;= \\mathbb{E} \\Bigg[\\frac{1}{n}\\sum _{i=1}^n f(X_i) + \\frac{1}{n}\\sum _{i=1}^n \\varepsilon_i  \\Bigg] \\\\  &amp;= \\frac{1}{n}\\sum _{i=1}^n \\mathbb{E}\\big[f(X_i)\\big] + \\frac{1}{n}\\sum _{i=1}^n \\mathbb{E}\\underbrace{\\big[\\varepsilon _i\\big]}_{=0} \\\\ &amp;= \\mathbb{E}\\big[f(X_i)\\big] \\\\ \\\\  \\mathbb{E}\\big[(\\hat{\\theta} - \\theta_0 \\big)^2] &amp;= \\textrm{Var}(\\hat{\\theta})\\end{align*}\\]"},{"location":"chapters/approximation/introduction/","title":"Introduction","text":"<p>Abstract</p> <p>We're interested in the behavior/performance of Estimators/Algorithms </p> <p>In applied microeconometrics, where we are generally interested in the causal effect of some policy/intervention, we use estimators with the following signature. </p> \\[\\begin{align*} \\mathcal{A} :: \\{ \\mathcal{X} \\times \\mathcal{Y} \\}^n \\to \\mathcal{R}^p\\\\  \\end{align*}\\] <p>Occasionally, these estimators will have an analytical form. For example, if we are interested in the average outcome we may use the following estimator. </p> \\[\\begin{align*} &amp;\\mathcal{A} :: \\{ \\mathcal{X} \\times \\mathcal{Y} \\}^n \\to \\mathcal{R}\\\\  &amp;\\mathcal{A} \\big(\\{x_i, y_i \\})_{i=1}^n\\big) = \\frac{1}{n} \\sum y_i \\\\  \\end{align*}\\] <p>Or if we are interested in the linear approximation to the CEF we may use the following estimator. </p> \\[\\begin{align*} &amp;\\mathcal{A} :: \\{ \\mathcal{X} \\times \\mathcal{Y} \\}^n \\to \\mathcal{R}^p\\\\  &amp;\\mathcal{A} \\big(\\{x_i, y_i \\})_{i=1}^n\\big) = \\big( X^TX)^{-1}X^TY \\\\  \\end{align*}\\] <p>We may, though, be interested in more \"complex\" estimators that involve neural networks.</p> \\[\\begin{align*} &amp;\\mathcal{A} :: \\{ \\mathcal{X} \\times \\mathcal{Y} \\}^n \\to \\mathcal{R}\\\\  &amp;\\mathcal{A} \\big(\\{x_i, y_i \\})_{i=1}^n\\big) = \\sum f(\\theta_1, x_i) - f(\\theta_2, x_i) \\\\  &amp; \\quad \\textrm{where} \\ \\theta_i = m^*\\big(\\{x_i, y_i \\})_{i=1}^n\\big) \\end{align*}\\] To Do <p>Make the connection to kernel methods</p>"},{"location":"chapters/approximation/introduction/#probability-space","title":"Probability Space","text":"Warning <p>Is the Borel Sigma Algebra well defined here? Should we add restrictions on \\(\\mathcal{X}, \\mathcal{Y}\\)?</p> \\[\\Big( \\mathcal{X} \\times \\mathcal{Y}, \\mathcal{B}(\\mathcal{X} \\times \\mathcal{Y}), \\mathbb{P}\\Big)\\]"},{"location":"chapters/approximation/introduction/#function-space-hypothesis-class-model-class","title":"Function Space/ Hypothesis Class/ Model Class","text":"Warning <p>What additional restrictions should we place on \\(\\mathcal{H}\\)?</p> \\[ \\mathcal{H} := \\{h \\mid h : \\mathcal{X} \\to \\mathcal{Y} \\}\\]"},{"location":"chapters/approximation/introduction/#loss-function","title":"Loss Function","text":"Warning <p>Make note on parameterization</p> \\[\\begin{align*} &amp;l : \\mathcal{H} \\to \\mathcal{X} \\to \\mathcal{Y} \\\\ &amp;l(h, x, y) = (y - h(x))^2 \\end{align*}\\]"},{"location":"chapters/approximation/introduction/#population-risk","title":"Population Risk","text":"\\[\\begin{align*} &amp;L :: \\mathcal{H} \\to \\mathcal{R}_+ \\\\  &amp;L(h) := \\underset{(x,y)\\sim p}{\\mathbb{E}} \\big[l(h, x, y)\\big] \\\\ &amp;L(h)= \\int _{\\mathcal{X} \\times \\mathcal{Y}}l(h, X, Y)d\\mathbb{P} \\end{align*}\\]"},{"location":"chapters/approximation/introduction/#empirical-risk","title":"Empirical Risk","text":"\\[ \\begin{align*} &amp;\\hat{L} :: \\{X, Y\\}^n \\to \\Theta \\to \\mathcal{R}_+ \\\\ &amp;\\hat{L}(\\{x_i, y_i\\}_{i=1}^n, \\theta) = \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i) \\end{align*}\\] Partial Evaluation: Expectation <p>Partially evaluated at \\(\\theta\\), \\(\\hat{L}\\)  is a random variable. Taking its expectation</p> \\[ \\begin{align*} \\mathbb{E}\\big[ \\hat{L} _{\\theta}\\big] &amp;= \\mathbb{E}\\Big[ \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i)\\Big] \\\\  &amp;= \\frac{1}{n} \\sum _i \\mathbb{E}\\big[ l(\\theta, x_i, y_i)\\big] \\\\  &amp;= L(\\theta) \\end{align*}\\] Partial Evaluation: Variance \\[ \\begin{align*} \\textrm{Var}(\\hat{L} _{\\theta})  &amp;= \\textrm{Var}\\Big[ \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i)\\Big] \\\\  &amp;= \\frac{1}{n^2} \\sum _i \\textrm{Var}\\big[ l(\\theta, x_i, y_i)\\big] \\\\  &amp;= \\frac{\\textrm{Var}\\big[ l(\\theta, x_i, y_i)\\big]}{n} \\end{align*}\\] Partial Evaluation: Variance <p>Partially evaluated at \\(\\theta\\), \\(\\hat{L}\\)  is a random variable. Taking its expectation</p> \\[ \\begin{align*} \\mathbb{E}\\big[ \\hat{L} _{\\theta}\\big] &amp;= \\mathbb{E}\\Big[ \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i)\\Big] \\\\  &amp;= \\frac{1}{n} \\sum _i \\mathbb{E}\\big[ l(\\theta, x_i, y_i)\\big] \\\\  &amp;= L(\\theta) \\end{align*}\\]"},{"location":"chapters/approximation/introduction/#hmm","title":"Hmm","text":"<p>But we are not really interested in this relationship, becuase \\(\\hat{L}\\) is not partially evaluated in practice. In practice, we use our training data to determine \\(\\theta\\). That is we have an algorithm \\(\\mathcal{A}\\). </p>"},{"location":"chapters/approximation/introduction/#algorithm","title":"Algorithm","text":"\\[ \\begin{align*} \\mathcal{A} :: \\{\\mathcal{X}, \\mathcal{Y}\\}^n \\to \\Theta  \\end{align*} \\] <ul> <li>Empirical Risk Minimization</li> </ul> \\[ \\begin{align*} &amp;\\textrm{ERM} :: \\{X, Y\\}^n \\to \\Theta  \\\\ &amp;\\textrm{ERM}(\\{x_i, y_i\\}_{i=1}^n) = \\underset{\\theta \\in \\Theta}{\\textrm{minimize}} \\ \\hat{L}(\\{x_i, y_i\\}_{i=1}^n, \\theta)   \\end{align*}\\] <ul> <li> <p>Consistency: </p> \\[\\mathbb{P}_n ( \\| \\mathcal{A}_n - \\theta _0 \\| &gt; \\varepsilon) \\to 0  \\] </li> <li> <p>Which is just a random variable. We can evaluate it as follows: </p> \\[ \\begin{align*} \\mathbb{E} \\big[ L \\circ \\mathcal{A} \\big] \\end{align*} \\] </li> <li> <p>Excess Risk:</p> </li> </ul> \\[  \\begin{align*} &amp;E :: \\mathcal{H} \\to \\mathcal{R}_+ \\\\ &amp;E(h) = L(h) - \\underset{g \\in \\mathcal{H}}{\\inf} L(g) \\end{align*}\\]"},{"location":"chapters/approximation/kernels/","title":"Kernels","text":"<p>These notes are taken from the following lectures: Lecture</p>"},{"location":"chapters/approximation/kernels/#definitions","title":"Definitions","text":"Dual Space <p>Let \\(X\\) be a vector space. Then the dual space of \\(X\\), denoted by \\(X^*\\), is the set of linear bounded functions on \\(X\\). </p>"},{"location":"chapters/approximation/kernels/#reproducing-kernel-hilbert-spaces","title":"Reproducing Kernel Hilbert Spaces","text":"<ul> <li>Let \\(\\mathcal{X}\\) be a set </li> <li>Let \\(F(\\mathcal{X}, \\mathcal{R})\\) be the vector space of funcions defined on \\(\\mathcal{X}\\). i.e. </li> </ul> \\[f_1, f_2 \\in F(\\mathcal{X}, \\mathcal{R}) \\implies \\alpha f_1 + \\beta f_2 \\in F(\\mathcal{X}, \\mathcal{R})\\] <ul> <li> <p>Then \\(\\mathcal{H}(\\mathcal{X}, \\mathcal{R}) \\subset F(\\mathcal{X}, \\mathcal{R})\\) is a Reproducing Kernel Hilbert Space if </p> <ol> <li>\\(\\mathcal{H}(\\mathcal{X}, \\mathcal{R})\\) is a subspace of \\(F(\\mathcal{X}, \\mathcal{R})\\)</li> <li>\\(\\Big(\\mathcal{H}(\\mathcal{X}, \\mathcal{R}), \\langle \\cdot, \\cdot  \\rangle _{\\mathcal{H}} \\Big)\\) is a Hilbert Space</li> <li>Evaluation Functionals, \\(\\textrm{Apply}_x\\), are continuous</li> </ol> \\[ \\textrm{Apply} : \\mathcal{X} \\to \\mathcal{H}(\\mathcal{X}, \\mathcal{R}) \\to \\mathcal{R}  \\] </li> </ul> <p>Note: </p> <ul> <li>We can think of Euclidean Spaces as a function space.</li> </ul> \\[\\mathcal{R} ^n \\equiv \\Big( F(\\textrm{Fin} \\ n, \\mathcal{R}), \\langle z_1, z_2  \\rangle _{F} :=  \\sum _{i=1}^n z_1(i)z_2(i)\\Big)\\] <ul> <li>We can generalize this structure to  \\(l^2(\\mathcal{X})\\)</li> </ul> \\[\\begin{align*} l^2(\\mathcal{X}):= \\Big\\{ f \\mid f:\\mathcal{X} \\to \\mathcal{R}, \\quad  \\sum _{x\\in \\mathcal{X}} |f(x)|^2 &lt; \\infty \\Big\\} \\end{align*}\\] <ul> <li>This structure (set \\(+\\) the norm/inner product)1 is an RKHS since </li> </ul> \\[\\| E_x \\| \\leq \\| f \\| _{l^2(\\mathcal{X})}\\] <p>Consider the following function </p> \\[\\begin{align*} &amp;\\Lambda :: \\mathcal{H} \\to \\mathcal{H} \\to \\mathcal{R}\\\\  &amp;\\Lambda \\ y \\ x = \\langle x, y \\rangle _{\\mathcal{H}} \\end{align*}\\] <p>We can re-write the signature of the function as follows:</p> \\[\\begin{align*} &amp;\\Lambda :: \\mathcal{H} \\to \\mathcal{H}^*\\\\  \\end{align*}\\] <p>If \\(H\\) is a RKHS, then by definition, \\(\\textrm{Apply} \\ x\\) is a linear bounded functional. By Reisz representation theorem, </p> \\[\\textrm{Apply} \\ x \\ f = \\langle r \\ x, f \\rangle _{\\mathcal{H}} = f \\ x \\] <p>Then we can define the Kernel as follows: </p> \\[\\begin{align*} &amp;K :: \\mathcal{X} \\to \\mathcal{X} \\to \\mathcal{R}  \\\\  &amp;K \\ x \\ y = r \\ x \\ y = \\langle r \\ y,  r \\ x \\rangle  \\end{align*}\\] <ol> <li> <p>I really think the key part of this structure is the inner product\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/optimization/differentiation/","title":"Differentiation","text":"<p>\"If you have a tricky problem to solve, finding a language in which the solution to that problem is compositional is like the most important step. Well, defining the problem clearly is the most important step. The second one is finding a vocabulary in which it's compositional.\"1</p> <p>This entire section is taken from Conal Elliot's presentation titled: \"Automatic Differentiation Made Easy Via Category Theory\"</p> <ul> <li> <p>A Derivative is a linear map </p> </li> <li> <p>Cateogy Theory is the abstract algebra of functions</p> </li> <li> <p>Let \\(a,b\\) be Banach spaces (complete normed vector spaces)</p> </li> </ul> \\[\\mathcal{D} :: (a \\to b) \\to (a \\to (a -\\circ b))\\] <ul> <li>Terminology: The derivative of \\(f\\) at \\(a\\)</li> </ul> \\[\\underset{\\varepsilon \\to 0}{\\lim} \\frac{ \\| f \\ (a + \\varepsilon) - f \\ a + \\mathcal{D} \\ f \\ a \\ \\varepsilon \\|}{\\| \\varepsilon \\|} = 0\\] <p>Differentiation preserves parallel composition </p> \\[\\begin{align*}&amp;(\\triangle) :: (a \\to b) \\to (a \\to d) \\to (a \\to b \\times d) \\\\  &amp;(f \\ \\triangle \\ g) \\ a = (f \\ a, g \\ a)  \\\\  &amp;\\mathcal{D} \\ (f \\ \\triangle \\ g)  = \\mathcal{D} \\ f \\ \\triangle \\ \\mathcal{D} \\ g \\end{align*}\\] \\[\\begin{align*} &amp;\\hat{\\mathcal{D}} :: (a \\to b) \\to (a \\to (b \\times (a \\to b))) \\\\  &amp; \\hat{\\mathcal{D}} \\ f = f \\ \\triangle \\ \\mathcal{D} \\ f\\end{align*}\\] <ol> <li> <p>Reference: See here \u21a9</p> </li> </ol>"},{"location":"chapters/optimization/overparam/","title":"Overparameterization","text":""},{"location":"chapters/optimization/overparam/#belkins-claim","title":"Belkin's Claim:","text":"<p>(1) If we have a solution manifold and (2) if this manifold has curvature then the loss function is not locally convex. </p> <p>Proof</p> <p>Assume we have a solution manifold. </p> \\[f(y) \\geq f(x) + \\nabla f(x)^T(y-x)\\] Lemma <p>Minimizers of Convex Function form a Convex Set</p> <ul> <li>Let \\(x_1, x_2\\) be minimizers of a convex function \\(f\\). i.e. \\(\\forall x \\ f(x) \\geq x_1, x_2\\)</li> <li>Then \\(\\forall \\alpha \\in (0,1), \\alpha x_1 + (1-\\alpha x_2)\\) is also a minimizer of \\(f\\).</li> </ul>"},{"location":"chapters/optimization/papers/","title":"Papers","text":"<ul> <li>The loss landscape of overparameterized neural networks</li> </ul>"},{"location":"chapters/other/LLM/introduction/","title":"Introduction","text":"References <ul> <li>A Neural Probabilistic Language Model</li> </ul>"},{"location":"chapters/other/LLM/introduction/#aim","title":"Aim","text":"<p>The aim is to \"learn\" \\(\\mathbb{P}\\). </p> \\[\\Big(\\{\\textrm{Sequences of Words}\\}, \\mathcal{F}, \\mathbb{P} \\Big)\\]"},{"location":"chapters/other/LLM/introduction/#high-level-approach","title":"High Level Approach","text":"<p>We can achieve this objective as follows: </p> <ul> <li>Learn a distribution over the first word of a sequence. </li> </ul> \\[\\big(\\mathcal{V}, \\mathcal{F}, \\mathbb{P} \\big)\\] <ul> <li>Learn the conditional distribution </li> </ul> \\[\\textrm{Model} :: \\textrm{Params} \\to \\{\\textrm{context}\\} \\to \\mathbb{P}_{\\mid \\textrm{context}}\\] To Do <p>This should be made more exact</p>"},{"location":"chapters/other/LLM/introduction/#a-neural-probabilistic-language-model","title":"A Neural Probabilistic Language Model","text":"<p>Essence</p> <ul> <li>transfer probability mass</li> <li>In the proposed model, it will so generalize because \u201csimilar\u201d words are expected to have a similar feature vector, and because the probability function is a smooth function of these feature values, a small change in the features will induce a small change in the probability</li> </ul> <p>We can construct the condition distribution as follows. </p> <ul> <li>First we introduce an embedding function. Given that our vocab is finite, we can represent this function as a matrix. i.e. \\(h\\) is isomorphic to \\(\\theta \\in {| \\mathcal{V} | \\times m}\\)</li> </ul> \\[h :: \\mathcal{V} \\to \\mathcal{R}^p\\] <ul> <li> <p>We introduce a function \\(g\\) which maps subsequences of these embeddings into a conditional distribution</p> </li> <li> <p>Given this level of detail we could augment the signature of our model as follows:</p> </li> </ul> \\[\\textrm{Model} :: \\textrm{Embedding Functions} \\to \\textrm{Forward Functions} \\to \\{\\textrm{context}\\} \\to \\mathbb{P}_{\\mid \\textrm{context}}\\] To Do <p>What should the name of this forward function be?</p>"},{"location":"chapters/other/LLM/introduction/#key-insights","title":"Key Insights","text":"<ul> <li>\"In high dimensions, it is crucial to distribute probability mass where it matters rather than uniformly in all directions around each training point.\"1</li> </ul> <ol> <li> <p>A Neural Probabilistic Language Model \u21a9</p> </li> </ol>"},{"location":"chapters/probability%20theory/conditioning/","title":"Conditioning","text":""},{"location":"chapters/probability%20theory/conditioning/#conditioning","title":"Conditioning","text":"<p>Given a measurable space</p> \\[\\big(\\Omega, \\mathcal{F}\\big)\\] <p>Let \\(\\mathcal{M}\\) be the set of probability measures defined on this space. Then conditioning can be defined as follows:</p> \\[\\begin{align*} &amp;C :: \\mathcal{M} \\to \\mathcal{F}_+ \\to \\mathcal{M} \\\\ &amp; C \\ \\mathbb{P} \\ A  \\ B = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(A)} \\end{align*}\\] To Do <p>Is it possible to encode the postivity requirement of \\(A\\) into the signature of \\(C\\)? Is this an example of a dependent type? As in, the set of possible events that we can condition on depends on the probability measure. </p>"},{"location":"chapters/probability%20theory/conditioning/#independence","title":"Independence","text":"<p>Given a probability space</p> \\[\\big(\\Omega, \\mathcal{F}, \\mathbb{P}\\big)\\] <p>\\(A, B\\) are independent under \\(\\mathbb{P}\\) if </p> \\[\\mathbb{P}( A \\cap B) = \\mathbb{P}(A) \\mathbb{P}(B)\\]"},{"location":"chapters/probability%20theory/convergence/","title":"Convergence","text":"<p>TLDR</p> <p>The defining feature of convergence is the restriction on the underlying probability space. </p> <ul> <li>In convergence in Law/distribution, we do not require \\(X_n\\) and \\(X\\) to be defined on the same probability space. </li> <li>In convergence in probability or expectation, we require that for each \\(n\\),  \\(X_n\\) and \\(X\\) are defined on the same probability space. i.e. they share the same probability space, but this probability space is allowed to change with \\(n\\). </li> <li>In convergence almost surley, the underlying probability space for \\(X_n\\) and \\(X\\) must be the same and fixed for all \\(n\\). </li> </ul>"},{"location":"chapters/probability%20theory/convergence/#convergence-in-distribution","title":"Convergence in Distribution","text":"<ul> <li>A random variable \\(X_n\\) convergences in distribution (or in law )to \\(X\\) when the corresponding sequence of CDFs converge \"pointwise\" to the CDF of \\(X\\)</li> </ul> \\[\\begin{align} X_n \\to_D X \\implies F_{X_n} \\to F_X  \\end{align}\\]"},{"location":"chapters/probability%20theory/convergence/#convergence-in-probability","title":"Convergence in Probability","text":"<p>If the limit is a random variable, then we care aboout the joint distribution!</p> <ul> <li>A random variable \\(X_n\\) converges in probability to \\(X\\) when for all \\(\\varepsilon &gt; 0\\) the following holds:</li> </ul> \\[\\begin{align} \\underset{n \\to \\infty}{\\lim}\\mathbb{P}(\\|X_n - X \\| &gt; \\varepsilon)= 0 \\end{align}\\]"}]}