{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Econometrics Introduction Applied econometrics is fundamentally about interpretation -- how to interpret the result of a statistical procedure in a given context. Often, there isn't often a \"valid\" or \"correct\" interpretation. In this course, we'll introduce you to a broad set of statitical tools and real life examples so that you can further develop your own judgement . It's important to keep in mind that you will often need to communicate this judgement to others. You'll have to explain to them why you consider something the way you do. This can be difficult though because learning from data is a highly personalized process. We all have our own beliefs about the world, our own understanding of how algorithms behave, and therefore our own takeaways from seeing the data (see figure below). And so I don't think the purpose of presenting statistical results is to convince people to agree with you. I would focus more on being clear about how you arrive at your own conclusions and to be open minded that others might think differently.","title":"Summary"},{"location":"chapters/approximation/asymptotics/","text":"","title":"Asymptotics"},{"location":"chapters/approximation/clusters/","text":"Convex Representation The Caratheodori Theorem tells us that any point \\(x \\in \\textrm{Conv}(T)\\) , where \\(T \\subset \\mathcal{R}^n\\) , can be represented as a convex combination of at most \\(n+1\\) points. 1 So for example let's say that our features \\(x \\in \\mathcal{R}^n\\) . Then for any point in the convex hull of the training set can be represented as the convex combination of at most ten points \\[x = \\sum _{i=1}^{11} \\alpha_i(x)x_i^*(x), \\quad x \\in \\mathcal{R}^{10}\\] Reference: See here \u21a9","title":"Clusters"},{"location":"chapters/approximation/clusters/#convex-representation","text":"The Caratheodori Theorem tells us that any point \\(x \\in \\textrm{Conv}(T)\\) , where \\(T \\subset \\mathcal{R}^n\\) , can be represented as a convex combination of at most \\(n+1\\) points. 1 So for example let's say that our features \\(x \\in \\mathcal{R}^n\\) . Then for any point in the convex hull of the training set can be represented as the convex combination of at most ten points \\[x = \\sum _{i=1}^{11} \\alpha_i(x)x_i^*(x), \\quad x \\in \\mathcal{R}^{10}\\] Reference: See here \u21a9","title":"Convex Representation"},{"location":"chapters/approximation/curse_of_dimensionality/","text":"A pure sampling phenomena Numerical Integration \\[\\int _{[0,1]^d}f dx\\] Monte-Carlo Integration \\[\\begin{align*}\\hat{\\theta} &= \\frac{1}{n}\\sum _{i=1}^n \\big(f(X_i) + \\varepsilon_i) \\\\ &= \\frac{1}{n}\\sum _{i=1}^n f(X_i) + \\frac{1}{n}\\sum _{i=1}^n \\varepsilon_i \\\\ \\\\ \\mathbb{E}[\\hat{\\theta}] &= \\mathbb{E} \\Bigg[\\frac{1}{n}\\sum _{i=1}^n f(X_i) + \\frac{1}{n}\\sum _{i=1}^n \\varepsilon_i \\Bigg] \\\\ &= \\frac{1}{n}\\sum _{i=1}^n \\mathbb{E}\\big[f(X_i)\\big] + \\frac{1}{n}\\sum _{i=1}^n \\mathbb{E}\\underbrace{\\big[\\varepsilon _i\\big]}_{=0} \\\\ &= \\mathbb{E}\\big[f(X_i)\\big] \\\\ \\\\ \\mathbb{E}\\big[(\\hat{\\theta} - \\theta_0 \\big)^2] &= \\textrm{Var}(\\hat{\\theta})\\end{align*}\\]","title":"Curse of Dimensionality"},{"location":"chapters/approximation/curse_of_dimensionality/#numerical-integration","text":"\\[\\int _{[0,1]^d}f dx\\]","title":"Numerical Integration"},{"location":"chapters/approximation/curse_of_dimensionality/#monte-carlo-integration","text":"\\[\\begin{align*}\\hat{\\theta} &= \\frac{1}{n}\\sum _{i=1}^n \\big(f(X_i) + \\varepsilon_i) \\\\ &= \\frac{1}{n}\\sum _{i=1}^n f(X_i) + \\frac{1}{n}\\sum _{i=1}^n \\varepsilon_i \\\\ \\\\ \\mathbb{E}[\\hat{\\theta}] &= \\mathbb{E} \\Bigg[\\frac{1}{n}\\sum _{i=1}^n f(X_i) + \\frac{1}{n}\\sum _{i=1}^n \\varepsilon_i \\Bigg] \\\\ &= \\frac{1}{n}\\sum _{i=1}^n \\mathbb{E}\\big[f(X_i)\\big] + \\frac{1}{n}\\sum _{i=1}^n \\mathbb{E}\\underbrace{\\big[\\varepsilon _i\\big]}_{=0} \\\\ &= \\mathbb{E}\\big[f(X_i)\\big] \\\\ \\\\ \\mathbb{E}\\big[(\\hat{\\theta} - \\theta_0 \\big)^2] &= \\textrm{Var}(\\hat{\\theta})\\end{align*}\\]","title":"Monte-Carlo Integration"},{"location":"chapters/approximation/introduction/","text":"Abstract We're interested in the behavior/performance of Estimators/Algorithms In applied microeconometrics, where we are generally interested in the causal effect of some policy/intervention, we use estimators with the following signature. \\[\\begin{align*} \\mathcal{A} :: \\{ \\mathcal{X} \\times \\mathcal{Y} \\}^n \\to \\mathcal{R}^p\\\\ \\end{align*}\\] Occasionally, these estimators will have an analytical form. For example, if we are interested in the average outcome we may use the following estimator. \\[\\begin{align*} &\\mathcal{A} :: \\{ \\mathcal{X} \\times \\mathcal{Y} \\}^n \\to \\mathcal{R}\\\\ &\\mathcal{A} \\big(\\{x_i, y_i \\})_{i=1}^n\\big) = \\frac{1}{n} \\sum y_i \\\\ \\end{align*}\\] Or if we are interested in the linear approximation to the CEF we may use the following estimator. \\[\\begin{align*} &\\mathcal{A} :: \\{ \\mathcal{X} \\times \\mathcal{Y} \\}^n \\to \\mathcal{R}^p\\\\ &\\mathcal{A} \\big(\\{x_i, y_i \\})_{i=1}^n\\big) = \\big( X^TX)^{-1}X^TY \\\\ \\end{align*}\\] We may, though, be interested in more \"complex\" estimators that involve neural networks. \\[\\begin{align*} &\\mathcal{A} :: \\{ \\mathcal{X} \\times \\mathcal{Y} \\}^n \\to \\mathcal{R}\\\\ &\\mathcal{A} \\big(\\{x_i, y_i \\})_{i=1}^n\\big) = \\sum f(\\theta_1, x_i) - f(\\theta_2, x_i) \\\\ & \\quad \\textrm{where} \\ \\theta_i = m^*\\big(\\{x_i, y_i \\})_{i=1}^n\\big) \\end{align*}\\] To Do Make the connection to kernel methods Probability Space Warning Is the Borel Sigma Algebra well defined here? Should we add restrictions on \\(\\mathcal{X}, \\mathcal{Y}\\) ? \\[\\Big( \\mathcal{X} \\times \\mathcal{Y}, \\mathcal{B}(\\mathcal{X} \\times \\mathcal{Y}), \\mathbb{P}\\Big)\\] Function Space/ Hypothesis Class/ Model Class Warning What additional restrictions should we place on \\(\\mathcal{H}\\) ? \\[ \\mathcal{H} := \\{h \\mid h : \\mathcal{X} \\to \\mathcal{Y} \\}\\] Loss Function Warning Make note on parameterization \\[\\begin{align*} &l : \\mathcal{H} \\to \\mathcal{X} \\to \\mathcal{Y} \\\\ &l(h, x, y) = (y - h(x))^2 \\end{align*}\\] Population Risk \\[\\begin{align*} &L :: \\mathcal{H} \\to \\mathcal{R}_+ \\\\ &L(h) := \\underset{(x,y)\\sim p}{\\mathbb{E}} \\big[l(h, x, y)\\big] \\\\ &L(h)= \\int _{\\mathcal{X} \\times \\mathcal{Y}}l(h, X, Y)d\\mathbb{P} \\end{align*}\\] Empirical Risk \\[ \\begin{align*} &\\hat{L} :: \\{X, Y\\}^n \\to \\Theta \\to \\mathcal{R}_+ \\\\ &\\hat{L}(\\{x_i, y_i\\}_{i=1}^n, \\theta) = \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i) \\end{align*}\\] Partial Evaluation: Expectation Partially evaluated at \\(\\theta\\) , \\(\\hat{L}\\) is a random variable. Taking its expectation \\[ \\begin{align*} \\mathbb{E}\\big[ \\hat{L} _{\\theta}\\big] &= \\mathbb{E}\\Big[ \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i)\\Big] \\\\ &= \\frac{1}{n} \\sum _i \\mathbb{E}\\big[ l(\\theta, x_i, y_i)\\big] \\\\ &= L(\\theta) \\end{align*}\\] Partial Evaluation: Variance \\[ \\begin{align*} \\textrm{Var}(\\hat{L} _{\\theta}) &= \\textrm{Var}\\Big[ \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i)\\Big] \\\\ &= \\frac{1}{n^2} \\sum _i \\textrm{Var}\\big[ l(\\theta, x_i, y_i)\\big] \\\\ &= \\frac{\\textrm{Var}\\big[ l(\\theta, x_i, y_i)\\big]}{n} \\end{align*}\\] Partial Evaluation: Variance Partially evaluated at \\(\\theta\\) , \\(\\hat{L}\\) is a random variable. Taking its expectation \\[ \\begin{align*} \\mathbb{E}\\big[ \\hat{L} _{\\theta}\\big] &= \\mathbb{E}\\Big[ \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i)\\Big] \\\\ &= \\frac{1}{n} \\sum _i \\mathbb{E}\\big[ l(\\theta, x_i, y_i)\\big] \\\\ &= L(\\theta) \\end{align*}\\] Hmm But we are not really interested in this relationship, becuase \\(\\hat{L}\\) is not partially evaluated in practice. In practice, we use our training data to determine \\(\\theta\\) . That is we have an algorithm \\(\\mathcal{A}\\) . Algorithm \\[ \\begin{align*} \\mathcal{A} :: \\{\\mathcal{X}, \\mathcal{Y}\\}^n \\to \\Theta \\end{align*} \\] Empirical Risk Minimization \\[ \\begin{align*} &\\textrm{ERM} :: \\{X, Y\\}^n \\to \\Theta \\\\ &\\textrm{ERM}(\\{x_i, y_i\\}_{i=1}^n) = \\underset{\\theta \\in \\Theta}{\\textrm{minimize}} \\ \\hat{L}(\\{x_i, y_i\\}_{i=1}^n, \\theta) \\end{align*}\\] Consistency: \\[\\mathbb{P}_n ( \\| \\mathcal{A}_n - \\theta _0 \\| > \\varepsilon) \\to 0 \\] Which is just a random variable. We can evaluate it as follows: \\[ \\begin{align*} \\mathbb{E} \\big[ L \\circ \\mathcal{A} \\big] \\end{align*} \\] Excess Risk: \\[ \\begin{align*} &E :: \\mathcal{H} \\to \\mathcal{R}_+ \\\\ &E(h) = L(h) - \\underset{g \\in \\mathcal{H}}{\\inf} L(g) \\end{align*}\\]","title":"Introduction"},{"location":"chapters/approximation/introduction/#probability-space","text":"Warning Is the Borel Sigma Algebra well defined here? Should we add restrictions on \\(\\mathcal{X}, \\mathcal{Y}\\) ? \\[\\Big( \\mathcal{X} \\times \\mathcal{Y}, \\mathcal{B}(\\mathcal{X} \\times \\mathcal{Y}), \\mathbb{P}\\Big)\\]","title":"Probability Space"},{"location":"chapters/approximation/introduction/#function-space-hypothesis-class-model-class","text":"Warning What additional restrictions should we place on \\(\\mathcal{H}\\) ? \\[ \\mathcal{H} := \\{h \\mid h : \\mathcal{X} \\to \\mathcal{Y} \\}\\]","title":"Function Space/ Hypothesis Class/ Model Class"},{"location":"chapters/approximation/introduction/#loss-function","text":"Warning Make note on parameterization \\[\\begin{align*} &l : \\mathcal{H} \\to \\mathcal{X} \\to \\mathcal{Y} \\\\ &l(h, x, y) = (y - h(x))^2 \\end{align*}\\]","title":"Loss Function"},{"location":"chapters/approximation/introduction/#population-risk","text":"\\[\\begin{align*} &L :: \\mathcal{H} \\to \\mathcal{R}_+ \\\\ &L(h) := \\underset{(x,y)\\sim p}{\\mathbb{E}} \\big[l(h, x, y)\\big] \\\\ &L(h)= \\int _{\\mathcal{X} \\times \\mathcal{Y}}l(h, X, Y)d\\mathbb{P} \\end{align*}\\]","title":"Population Risk"},{"location":"chapters/approximation/introduction/#empirical-risk","text":"\\[ \\begin{align*} &\\hat{L} :: \\{X, Y\\}^n \\to \\Theta \\to \\mathcal{R}_+ \\\\ &\\hat{L}(\\{x_i, y_i\\}_{i=1}^n, \\theta) = \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i) \\end{align*}\\] Partial Evaluation: Expectation Partially evaluated at \\(\\theta\\) , \\(\\hat{L}\\) is a random variable. Taking its expectation \\[ \\begin{align*} \\mathbb{E}\\big[ \\hat{L} _{\\theta}\\big] &= \\mathbb{E}\\Big[ \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i)\\Big] \\\\ &= \\frac{1}{n} \\sum _i \\mathbb{E}\\big[ l(\\theta, x_i, y_i)\\big] \\\\ &= L(\\theta) \\end{align*}\\] Partial Evaluation: Variance \\[ \\begin{align*} \\textrm{Var}(\\hat{L} _{\\theta}) &= \\textrm{Var}\\Big[ \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i)\\Big] \\\\ &= \\frac{1}{n^2} \\sum _i \\textrm{Var}\\big[ l(\\theta, x_i, y_i)\\big] \\\\ &= \\frac{\\textrm{Var}\\big[ l(\\theta, x_i, y_i)\\big]}{n} \\end{align*}\\] Partial Evaluation: Variance Partially evaluated at \\(\\theta\\) , \\(\\hat{L}\\) is a random variable. Taking its expectation \\[ \\begin{align*} \\mathbb{E}\\big[ \\hat{L} _{\\theta}\\big] &= \\mathbb{E}\\Big[ \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i)\\Big] \\\\ &= \\frac{1}{n} \\sum _i \\mathbb{E}\\big[ l(\\theta, x_i, y_i)\\big] \\\\ &= L(\\theta) \\end{align*}\\]","title":"Empirical Risk"},{"location":"chapters/approximation/introduction/#hmm","text":"But we are not really interested in this relationship, becuase \\(\\hat{L}\\) is not partially evaluated in practice. In practice, we use our training data to determine \\(\\theta\\) . That is we have an algorithm \\(\\mathcal{A}\\) .","title":"Hmm"},{"location":"chapters/approximation/introduction/#algorithm","text":"\\[ \\begin{align*} \\mathcal{A} :: \\{\\mathcal{X}, \\mathcal{Y}\\}^n \\to \\Theta \\end{align*} \\] Empirical Risk Minimization \\[ \\begin{align*} &\\textrm{ERM} :: \\{X, Y\\}^n \\to \\Theta \\\\ &\\textrm{ERM}(\\{x_i, y_i\\}_{i=1}^n) = \\underset{\\theta \\in \\Theta}{\\textrm{minimize}} \\ \\hat{L}(\\{x_i, y_i\\}_{i=1}^n, \\theta) \\end{align*}\\] Consistency: \\[\\mathbb{P}_n ( \\| \\mathcal{A}_n - \\theta _0 \\| > \\varepsilon) \\to 0 \\] Which is just a random variable. We can evaluate it as follows: \\[ \\begin{align*} \\mathbb{E} \\big[ L \\circ \\mathcal{A} \\big] \\end{align*} \\] Excess Risk: \\[ \\begin{align*} &E :: \\mathcal{H} \\to \\mathcal{R}_+ \\\\ &E(h) = L(h) - \\underset{g \\in \\mathcal{H}}{\\inf} L(g) \\end{align*}\\]","title":"Algorithm"},{"location":"chapters/approximation/kernels/","text":"These notes are taken from the following lectures: Lecture Definitions Dual Space Let \\(X\\) be a vector space. Then the dual space of \\(X\\) , denoted by \\(X^*\\) , is the set of linear bounded functions on \\(X\\) . Reproducing Kernel Hilbert Spaces Let \\(\\mathcal{X}\\) be a set Let \\(F(\\mathcal{X}, \\mathcal{R})\\) be the vector space of funcions defined on \\(\\mathcal{X}\\) . i.e. \\[f_1, f_2 \\in F(\\mathcal{X}, \\mathcal{R}) \\implies \\alpha f_1 + \\beta f_2 \\in F(\\mathcal{X}, \\mathcal{R})\\] Then \\(\\mathcal{H}(\\mathcal{X}, \\mathcal{R}) \\subset F(\\mathcal{X}, \\mathcal{R})\\) is a Reproducing Kernel Hilbert Space if \\(\\mathcal{H}(\\mathcal{X}, \\mathcal{R})\\) is a subspace of \\(F(\\mathcal{X}, \\mathcal{R})\\) \\(\\Big(\\mathcal{H}(\\mathcal{X}, \\mathcal{R}), \\langle \\cdot, \\cdot \\rangle _{\\mathcal{H}} \\Big)\\) is a Hilbert Space Evaluation Functionals, \\(\\textrm{Apply}_x\\) , are continuous \\[ \\textrm{Apply} : \\mathcal{X} \\to \\mathcal{H}(\\mathcal{X}, \\mathcal{R}) \\to \\mathcal{R} \\] Note : We can think of Euclidean Spaces as a function space. \\[\\mathcal{R} ^n \\equiv \\Big( F(\\textrm{Fin} \\ n, \\mathcal{R}), \\langle z_1, z_2 \\rangle _{F} := \\sum _{i=1}^n z_1(i)z_2(i)\\Big)\\] We can generalize this structure to \\(l^2(\\mathcal{X})\\) \\[\\begin{align*} l^2(\\mathcal{X}):= \\Big\\{ f \\mid f:\\mathcal{X} \\to \\mathcal{R}, \\quad \\sum _{x\\in \\mathcal{X}} |f(x)|^2 < \\infty \\Big\\} \\end{align*}\\] This structure (set \\(+\\) the norm/inner product) 1 is an RKHS since \\[\\| E_x \\| \\leq \\| f \\| _{l^2(\\mathcal{X})}\\] Consider the following function \\[\\begin{align*} &\\Lambda :: \\mathcal{H} \\to \\mathcal{H} \\to \\mathcal{R}\\\\ &\\Lambda \\ y \\ x = \\langle x, y \\rangle _{\\mathcal{H}} \\end{align*}\\] We can re-write the signature of the function as follows: \\[\\begin{align*} &\\Lambda :: \\mathcal{H} \\to \\mathcal{H}^*\\\\ \\end{align*}\\] If \\(H\\) is a RKHS, then by definition, \\(\\textrm{Apply} \\ x\\) is a linear bounded functional. By Reisz representation theorem, \\[\\textrm{Apply} \\ x \\ f = \\langle r \\ x, f \\rangle _{\\mathcal{H}} = f \\ x \\] Then we can define the Kernel as follows: \\[\\begin{align*} &K :: \\mathcal{X} \\to \\mathcal{X} \\to \\mathcal{R} \\\\ &K \\ x \\ y = r \\ x \\ y = \\langle r \\ y, r \\ x \\rangle \\end{align*}\\] I really think the key part of this structure is the inner product \u21a9","title":"Kernels"},{"location":"chapters/approximation/kernels/#definitions","text":"Dual Space Let \\(X\\) be a vector space. Then the dual space of \\(X\\) , denoted by \\(X^*\\) , is the set of linear bounded functions on \\(X\\) .","title":"Definitions"},{"location":"chapters/approximation/kernels/#reproducing-kernel-hilbert-spaces","text":"Let \\(\\mathcal{X}\\) be a set Let \\(F(\\mathcal{X}, \\mathcal{R})\\) be the vector space of funcions defined on \\(\\mathcal{X}\\) . i.e. \\[f_1, f_2 \\in F(\\mathcal{X}, \\mathcal{R}) \\implies \\alpha f_1 + \\beta f_2 \\in F(\\mathcal{X}, \\mathcal{R})\\] Then \\(\\mathcal{H}(\\mathcal{X}, \\mathcal{R}) \\subset F(\\mathcal{X}, \\mathcal{R})\\) is a Reproducing Kernel Hilbert Space if \\(\\mathcal{H}(\\mathcal{X}, \\mathcal{R})\\) is a subspace of \\(F(\\mathcal{X}, \\mathcal{R})\\) \\(\\Big(\\mathcal{H}(\\mathcal{X}, \\mathcal{R}), \\langle \\cdot, \\cdot \\rangle _{\\mathcal{H}} \\Big)\\) is a Hilbert Space Evaluation Functionals, \\(\\textrm{Apply}_x\\) , are continuous \\[ \\textrm{Apply} : \\mathcal{X} \\to \\mathcal{H}(\\mathcal{X}, \\mathcal{R}) \\to \\mathcal{R} \\] Note : We can think of Euclidean Spaces as a function space. \\[\\mathcal{R} ^n \\equiv \\Big( F(\\textrm{Fin} \\ n, \\mathcal{R}), \\langle z_1, z_2 \\rangle _{F} := \\sum _{i=1}^n z_1(i)z_2(i)\\Big)\\] We can generalize this structure to \\(l^2(\\mathcal{X})\\) \\[\\begin{align*} l^2(\\mathcal{X}):= \\Big\\{ f \\mid f:\\mathcal{X} \\to \\mathcal{R}, \\quad \\sum _{x\\in \\mathcal{X}} |f(x)|^2 < \\infty \\Big\\} \\end{align*}\\] This structure (set \\(+\\) the norm/inner product) 1 is an RKHS since \\[\\| E_x \\| \\leq \\| f \\| _{l^2(\\mathcal{X})}\\] Consider the following function \\[\\begin{align*} &\\Lambda :: \\mathcal{H} \\to \\mathcal{H} \\to \\mathcal{R}\\\\ &\\Lambda \\ y \\ x = \\langle x, y \\rangle _{\\mathcal{H}} \\end{align*}\\] We can re-write the signature of the function as follows: \\[\\begin{align*} &\\Lambda :: \\mathcal{H} \\to \\mathcal{H}^*\\\\ \\end{align*}\\] If \\(H\\) is a RKHS, then by definition, \\(\\textrm{Apply} \\ x\\) is a linear bounded functional. By Reisz representation theorem, \\[\\textrm{Apply} \\ x \\ f = \\langle r \\ x, f \\rangle _{\\mathcal{H}} = f \\ x \\] Then we can define the Kernel as follows: \\[\\begin{align*} &K :: \\mathcal{X} \\to \\mathcal{X} \\to \\mathcal{R} \\\\ &K \\ x \\ y = r \\ x \\ y = \\langle r \\ y, r \\ x \\rangle \\end{align*}\\] I really think the key part of this structure is the inner product \u21a9","title":"Reproducing Kernel Hilbert Spaces"},{"location":"chapters/approximation/topology/","text":"I am interested in topological spaces (in contrast to metric spaces) because they can capture multiple notions of \"closeness\". The most basic object is a set. To make things concrete, let's consider the set of eviction complaints filed by landlords in Connecticut between February 2022 and October 2022. How's that for a concrete example? Given this set, we can define a topology, \\(\\mathcal{T}\\) , which is a set whose elements consists of eviction complaints. That is, each element of the topology is a subset of the orginal set: \\(A \\in \\mathcal{T}, A \\subset X\\) . A topology must also include arbitrary unions of elements of the topology (referred to as open sets) and must include finite intersections of elements as well.","title":"Topology"},{"location":"chapters/fundamentals/difference-in-means/","text":"In the previous note, we motivated why we are interested in estimating the average treatment effect. Given this objective, a natural starting point, it would seem, would be to estamate this effect by taking the difference between the average outcomes in the treated group and the average outcomes in the control group. Assuming for the moment, that we observe the entire population (i.e we're not thinking about sampling from a larger population), our difference-in-means strategy can be formally defined as follows. \\[\\mathbb{E}[Y_i \\vert D_i = 1] - \\mathbb{E}[Y_i \\vert D_i=0]\\] These conditional expectations correspond to the average outcomes for those individuals in the treated/control group respectively. We use the binary variable \\(D_i\\) to denote the treatment status. ( \\(D_i=1\\) ) means that the person is treated. Whenever some proposes something, it's natural to question whether this is a good idea. In our context, this would be - does the difference in means estimator recover the average treatment effect? To assess this, we need to relate the terms in the above expression to the potential outcome function. Let's start with the first term, the expected outcome with the treatment for those who receive treatment. Read that sentence again. It's not easy to read but it conveys important information. There are two key pieces of information that are related to condtioning on \\(D_i=1\\) . The first piece of information is that we are observing the potential outcome associated with treatment \\(\\tilde{Y}_i(1)\\) , and piece of information is that we observe this outcome for those individuals in the treated group. With this in mind, we can re-write the above expression as follows: \\[\\begin{align*}\\mathbb{E}[Y_i \\vert D_i = 1] - \\mathbb{E}[Y_i \\vert D_i=0] &= \\mathbb{E}[\\tilde{Y}_i(1) \\vert D_i = 1] - \\mathbb{E}[\\tilde{Y}_i(0) \\vert D_i = 0] \\end{align*}\\] To be able to interpret this term further, we're goind to add and subtract the following term \\(\\textcolor{blue}{\\mathbb{E}[\\tilde{Y}_i(0) \\vert D_i=1]}\\) . \\[\\begin{align*}\\Big( \\mathbb{E}[\\tilde{Y}_i(1) \\vert D_i = 1] - \\textcolor{blue}{\\mathbb{E}[\\tilde{Y}_i(0) \\vert D_i=1]}\\Big) + \\Big(\\textcolor{blue}{\\mathbb{E}[\\tilde{Y}_i(0) \\vert D_i=1]} - \\mathbb{E}[\\tilde{Y}_i(0) \\vert D_i = 0]\\Big) \\end{align*}\\] We know have two bracketed expressions. The first captures the the average treatment effect on the treated group. The second captures the average difference between the treated and control groups in a counterfacutal world where no one is treated. We refer to this second expression as the Selection Bias . We've shown that the difference-in-means is equivalent to the average treatment on the treated plus the selection bias. Consider What is the average treatment effect on the treated and selection bias in a randomized control trial?","title":"Difference-in-Means"},{"location":"chapters/fundamentals/potential%20outcomes/","text":"If I were to ask you to define the causal effect of a treatment, how would you do so? In this brief note, we'll introduce a framework for defining causal effects. Intuitively, we know that the causal effect of a treatment is the difference in some outcome when a person receives the treatment and when a person does not receive the treatment. Re-reading the line above, hopefully it jumps out to you that we cannot in fact observe a person in both of the states of the world. For instance, if we are interested in understanding the effects of a college education on annual earnings at the age of 40, we can observe each person either with a college education or without one. This highlights that the funamental issue of a causal inference is a missing data problem. While we cannot ever know the causal effects at the individual level, its nevertheless important to be able to define it at this level. To do so, we introduce the potential outcome function which maps levels of the treatment variable into an outcome space. 1 Continuing with our example of college education, we can define the potential outcome function as follows. \\[\\tilde{Y}_i : \\{\\textrm{College Education}, \\textrm{No College Education}\\} \\to \\mathcal{R}\\] We can define the individual level treatment effect then as the difference, where we have replaced the college/no college status with an binary variable (i.e. a variable which takes the values either 1 or 0). \\[\\tilde{Y}_i(1) -\\tilde{Y}_i(0)\\] With this set-up, we can then define the average treatment effect as the expectation of this difference over the entire population of interest. In many contexts, we will be interested in estimating this term. In our running example, this would be the return on education averaged over the entire population of interest. \\[\\textrm{ATE} = \\mathbb{E}[\\tilde{Y}_i(1) -\\tilde{Y}_i(0)]\\] Question: What's the difference between a set and a space? \u21a9","title":"Potential Outcomes"},{"location":"chapters/fundamentals/residualized%20regression/","text":"Instrumental Variables Linear instrumental variables consists of a the following two step process. We first regress the treatment variable \\(D_i\\) on the controls \\(X_i\\) and the instrument \\(Z_i\\) . We then regress the outcome variable \\(Y_i\\) on the predicted treatment \\(\\hat{D}_i\\) and the controls. \\[\\begin{align*} D_i &= \\gamma_1 X_i + \\gamma_2 Z_i + v_i \\\\ Y_i &= \\beta_1 \\hat{D}_i + \\beta_2 X_i + \\varepsilon_i \\end{align*}\\] As emphasized in class, I prefer to interpret the coefficients in a linear model via a residualized approach. For one, it makes the source of the variation clear. We can interpret the coefficient \\(\\beta_1\\) in the second equation above via a residualized regression as follows where \\(\\bar{\\hat{D}}\\) is the predicted predicted values! Not a typo. We first predict treatment given the controls and the instrument. We then predict this predicted value given only the controls. Via the law of iterated expectations it is equivalent to the predicted treatment given the controls. \\[\\begin{align*} Y_i &= \\beta_1 (\\hat{D}_i - \\bar{\\hat{D}}_i) + u_i \\\\ \\end{align*}\\] As I also emphasize in class, I tent to think of linear models as approximations to the underlying conditional expectation function. Therefore, we can re-write the above regression in its nonparametric form as follows: \\[\\begin{align*} Y_i &= \\beta_1 (\\mathbb{E}[D_i \\vert X_i, Z_i] - \\mathbb{E}[D_i \\vert X_i] ) + u_i \\\\ \\end{align*}\\] When I see a linear IV model in a paper, I try to interpret is as an approximation to the above regression. IV keeps only the local source of the treatment due to the instrument. IV is a local correction of the treatment variable.","title":"Residualized Regression"},{"location":"chapters/fundamentals/residualized%20regression/#instrumental-variables","text":"Linear instrumental variables consists of a the following two step process. We first regress the treatment variable \\(D_i\\) on the controls \\(X_i\\) and the instrument \\(Z_i\\) . We then regress the outcome variable \\(Y_i\\) on the predicted treatment \\(\\hat{D}_i\\) and the controls. \\[\\begin{align*} D_i &= \\gamma_1 X_i + \\gamma_2 Z_i + v_i \\\\ Y_i &= \\beta_1 \\hat{D}_i + \\beta_2 X_i + \\varepsilon_i \\end{align*}\\] As emphasized in class, I prefer to interpret the coefficients in a linear model via a residualized approach. For one, it makes the source of the variation clear. We can interpret the coefficient \\(\\beta_1\\) in the second equation above via a residualized regression as follows where \\(\\bar{\\hat{D}}\\) is the predicted predicted values! Not a typo. We first predict treatment given the controls and the instrument. We then predict this predicted value given only the controls. Via the law of iterated expectations it is equivalent to the predicted treatment given the controls. \\[\\begin{align*} Y_i &= \\beta_1 (\\hat{D}_i - \\bar{\\hat{D}}_i) + u_i \\\\ \\end{align*}\\] As I also emphasize in class, I tent to think of linear models as approximations to the underlying conditional expectation function. Therefore, we can re-write the above regression in its nonparametric form as follows: \\[\\begin{align*} Y_i &= \\beta_1 (\\mathbb{E}[D_i \\vert X_i, Z_i] - \\mathbb{E}[D_i \\vert X_i] ) + u_i \\\\ \\end{align*}\\] When I see a linear IV model in a paper, I try to interpret is as an approximation to the above regression. IV keeps only the local source of the treatment due to the instrument. IV is a local correction of the treatment variable.","title":"Instrumental Variables"},{"location":"chapters/fundamentals/selection%20on%20observables/","text":"Do We Need Controls? The question that has been lurking in the back of your head up to this point is whether it would be appropriate to use a difference-in-means estimator when the selection on observable assumption holds. That is, if we can think of our data as coming from a \"local\" randomized control trial, does the difference in means estimator recover the average treatment effect? Let's think through this. Applying the Law of Total Probability, we can express the first term in the estimator as follows: \\[\\begin{align*}\\mathbb{E}[Y_i \\vert D_i=1] &= \\sum \\mathbb{E}[Y_i \\vert D_i=1, X_i=x] p_{X\\vert D=1}(x) \\\\ &= \\sum \\mathbb{E}[\\tilde{Y}_i(1) \\vert D_i=1, X_i=x] p_{X\\vert D=1}(x) \\\\ &= \\sum \\mathbb{E}[\\tilde{Y}_i(1) \\vert X_i=x] p_{X\\vert D=1}(x)\\end{align*}\\] The immediate issue that jumps out to us is that the conditional distribution \\(p_{X \\vert D=1}(x)\\) may not equal the unconditional distribution, \\(p_X(x)\\) , in which case our estimator would be biased. Another way to frame this problem is as follows: We observe a random variable following a distribution \\(q(x)\\) , but we are interested in taking the expected value with respect to a different distribution \\(v(x)\\) . That is: \\[\\begin{align*} \\textrm{Observe} := \\sum f(x) q(x)\\\\ \\textrm{Estimand} := \\sum f(x) v(x)\\\\ \\end{align*}\\] One approach in this context is to transform the random variable via a correction term. That is, we introduce a new random variable which is a scaled version of our original random variable \\(h(x)= f(x)\\frac{v(x)}{q(x)}\\) , and take the average of this random variable with respect to \\(q(x)\\) . \\[\\begin{align*} \\textrm{Observe} := \\sum h(x) q(x) = f(x)\\frac{v(x)}{q(x)} q(x) = \\sum f(x) v(x) = \\textrm{Estimand} \\end{align*}\\] In our context, \\(q(x) = p_{X \\vert D=1}(x)\\) and \\(v(x) = p(x)\\) . Therefore our \"correction term\" is the ratio of these two values: the unconditional probability of treatment and the propensity score. \\[\\begin{align*} \\textrm{Correction Term} &:= \\frac{p(x)}{ p_{X \\vert D=1}(x)} \\\\ &= p_X(x) \\div \\frac{ p_{X,D}(x, 1)}{p_D(1)} \\\\ &= \\frac{p_D(1) p_X(x)}{ p_{X,D}(x, 1)} \\\\ &= \\frac{p_D(1)}{p_{D \\vert X=x}(1)} \\end{align*}\\] Let's now check that if we use this correction term, the difference-in-means estimator will return the average treatment effect. To do so, let \\(W_i = \\frac{Y_i p_D(1)}{p_{D\\vert X_i}(1)}\\) \\[\\begin{align*} \\mathbb{E}[W_i \\vert D_i=1] &= \\sum \\mathbb{E}[W_i \\vert D_i=1, X_i=x] p_{X\\vert D=1}(x) \\\\ &= \\sum \\mathbb{E}[\\frac{Y_i p_D(1)}{p_{D\\vert X_i=x}(1)} \\vert D_i=1, X_i=x] p_{X\\vert D=1}(x) \\\\ &= p_D(1)\\sum \\mathbb{E}[Y_i \\vert D_i=1, X_i=x]\\frac{p_{X\\vert D=1}(x)}{p_{D\\vert X_i=x}(1)}\\\\ &= p_D(1)\\sum \\mathbb{E}[\\tilde{Y}_i(1) \\vert D_i=1, X_i=x]\\frac{p_{X\\vert D=1}(x)}{p_{D\\vert X_i=x}(1)}\\\\ &= p_D(1)\\sum \\mathbb{E}[\\tilde{Y}_i(1) \\vert X_i=x]\\frac{p_{X\\vert D=1}(x)}{p_{D\\vert X_i=x}(1)} \\\\ &= p_D(1)\\sum \\mathbb{E}[\\tilde{Y}_i(1) \\vert X_i=x] \\frac{\\frac{p_{X,D}(x,1)}{p_{D}(1)}}{\\frac{p_{X,D}(x,1)}{p_{X}(x)}} \\\\ &= p_D(1)\\sum \\mathbb{E}[\\tilde{Y}_i(1) \\vert X_i=x] \\frac{p_X(x)}{p_D(1)} \\\\ &= \\sum \\mathbb{E}[\\tilde{Y}_i(1) \\vert X_i=x] p_X(x) \\\\ &= \\mathbb{E}[\\tilde{Y}_i(1)] \\end{align*}\\] Can Linear Regression be Wrong in this Context? The above section highlights that even if we can think of our data as generated by local randomized control trails, we need to account for the relative distribution over the covariates. The most immediate question that follows is: does linear regression adjust the relative distribution?","title":"Selection on Observables"},{"location":"chapters/fundamentals/selection%20on%20observables/#do-we-need-controls","text":"The question that has been lurking in the back of your head up to this point is whether it would be appropriate to use a difference-in-means estimator when the selection on observable assumption holds. That is, if we can think of our data as coming from a \"local\" randomized control trial, does the difference in means estimator recover the average treatment effect? Let's think through this. Applying the Law of Total Probability, we can express the first term in the estimator as follows: \\[\\begin{align*}\\mathbb{E}[Y_i \\vert D_i=1] &= \\sum \\mathbb{E}[Y_i \\vert D_i=1, X_i=x] p_{X\\vert D=1}(x) \\\\ &= \\sum \\mathbb{E}[\\tilde{Y}_i(1) \\vert D_i=1, X_i=x] p_{X\\vert D=1}(x) \\\\ &= \\sum \\mathbb{E}[\\tilde{Y}_i(1) \\vert X_i=x] p_{X\\vert D=1}(x)\\end{align*}\\] The immediate issue that jumps out to us is that the conditional distribution \\(p_{X \\vert D=1}(x)\\) may not equal the unconditional distribution, \\(p_X(x)\\) , in which case our estimator would be biased. Another way to frame this problem is as follows: We observe a random variable following a distribution \\(q(x)\\) , but we are interested in taking the expected value with respect to a different distribution \\(v(x)\\) . That is: \\[\\begin{align*} \\textrm{Observe} := \\sum f(x) q(x)\\\\ \\textrm{Estimand} := \\sum f(x) v(x)\\\\ \\end{align*}\\] One approach in this context is to transform the random variable via a correction term. That is, we introduce a new random variable which is a scaled version of our original random variable \\(h(x)= f(x)\\frac{v(x)}{q(x)}\\) , and take the average of this random variable with respect to \\(q(x)\\) . \\[\\begin{align*} \\textrm{Observe} := \\sum h(x) q(x) = f(x)\\frac{v(x)}{q(x)} q(x) = \\sum f(x) v(x) = \\textrm{Estimand} \\end{align*}\\] In our context, \\(q(x) = p_{X \\vert D=1}(x)\\) and \\(v(x) = p(x)\\) . Therefore our \"correction term\" is the ratio of these two values: the unconditional probability of treatment and the propensity score. \\[\\begin{align*} \\textrm{Correction Term} &:= \\frac{p(x)}{ p_{X \\vert D=1}(x)} \\\\ &= p_X(x) \\div \\frac{ p_{X,D}(x, 1)}{p_D(1)} \\\\ &= \\frac{p_D(1) p_X(x)}{ p_{X,D}(x, 1)} \\\\ &= \\frac{p_D(1)}{p_{D \\vert X=x}(1)} \\end{align*}\\] Let's now check that if we use this correction term, the difference-in-means estimator will return the average treatment effect. To do so, let \\(W_i = \\frac{Y_i p_D(1)}{p_{D\\vert X_i}(1)}\\) \\[\\begin{align*} \\mathbb{E}[W_i \\vert D_i=1] &= \\sum \\mathbb{E}[W_i \\vert D_i=1, X_i=x] p_{X\\vert D=1}(x) \\\\ &= \\sum \\mathbb{E}[\\frac{Y_i p_D(1)}{p_{D\\vert X_i=x}(1)} \\vert D_i=1, X_i=x] p_{X\\vert D=1}(x) \\\\ &= p_D(1)\\sum \\mathbb{E}[Y_i \\vert D_i=1, X_i=x]\\frac{p_{X\\vert D=1}(x)}{p_{D\\vert X_i=x}(1)}\\\\ &= p_D(1)\\sum \\mathbb{E}[\\tilde{Y}_i(1) \\vert D_i=1, X_i=x]\\frac{p_{X\\vert D=1}(x)}{p_{D\\vert X_i=x}(1)}\\\\ &= p_D(1)\\sum \\mathbb{E}[\\tilde{Y}_i(1) \\vert X_i=x]\\frac{p_{X\\vert D=1}(x)}{p_{D\\vert X_i=x}(1)} \\\\ &= p_D(1)\\sum \\mathbb{E}[\\tilde{Y}_i(1) \\vert X_i=x] \\frac{\\frac{p_{X,D}(x,1)}{p_{D}(1)}}{\\frac{p_{X,D}(x,1)}{p_{X}(x)}} \\\\ &= p_D(1)\\sum \\mathbb{E}[\\tilde{Y}_i(1) \\vert X_i=x] \\frac{p_X(x)}{p_D(1)} \\\\ &= \\sum \\mathbb{E}[\\tilde{Y}_i(1) \\vert X_i=x] p_X(x) \\\\ &= \\mathbb{E}[\\tilde{Y}_i(1)] \\end{align*}\\]","title":"Do We Need Controls?"},{"location":"chapters/fundamentals/selection%20on%20observables/#can-linear-regression-be-wrong-in-this-context","text":"The above section highlights that even if we can think of our data as generated by local randomized control trails, we need to account for the relative distribution over the covariates. The most immediate question that follows is: does linear regression adjust the relative distribution?","title":"Can Linear Regression be Wrong in this Context?"},{"location":"chapters/optimization/differentiation/","text":"\"If you have a tricky problem to solve, finding a language in which the solution to that problem is compositional is like the most important step. Well, defining the problem clearly is the most important step. The second one is finding a vocabulary in which it's compositional.\" 1 This entire section is taken from Conal Elliot's presentation titled: \" Automatic Differentiation Made Easy Via Category Theory \" A Derivative is a linear map Cateogy Theory is the abstract algebra of functions Let \\(a,b\\) be Banach spaces (complete normed vector spaces) \\[\\mathcal{D} :: (a \\to b) \\to (a \\to (a -\\circ b))\\] Terminology: The derivative of \\(f\\) at \\(a\\) \\[\\underset{\\varepsilon \\to 0}{\\lim} \\frac{ \\| f \\ (a + \\varepsilon) - f \\ a + \\mathcal{D} \\ f \\ a \\ \\varepsilon \\|}{\\| \\varepsilon \\|} = 0\\] Differentiation preserves parallel composition \\[\\begin{align*}&(\\triangle) :: (a \\to b) \\to (a \\to d) \\to (a \\to b \\times d) \\\\ &(f \\ \\triangle \\ g) \\ a = (f \\ a, g \\ a) \\\\ &\\mathcal{D} \\ (f \\ \\triangle \\ g) = \\mathcal{D} \\ f \\ \\triangle \\ \\mathcal{D} \\ g \\end{align*}\\] \\[\\begin{align*} &\\hat{\\mathcal{D}} :: (a \\to b) \\to (a \\to (b \\times (a \\to b))) \\\\ & \\hat{\\mathcal{D}} \\ f = f \\ \\triangle \\ \\mathcal{D} \\ f\\end{align*}\\] Reference: See here \u21a9","title":"Differentiation"},{"location":"chapters/optimization/introduction/","text":"","title":"Introduction"},{"location":"chapters/optimization/overparam/","text":"Belkin's Claim : (1) If we have a solution manifold and (2) if this manifold has curvature then the loss function is not locally convex. Proof Assume we have a solution manifold. \\[f(y) \\geq f(x) + \\nabla f(x)^T(y-x)\\] Lemma Minimizers of Convex Function form a Convex Set Let \\(x_1, x_2\\) be minimizers of a convex function \\(f\\) . i.e. \\(\\forall x \\ f(x) \\geq x_1, x_2\\) Then \\(\\forall \\alpha \\in (0,1), \\alpha x_1 + (1-\\alpha x_2)\\) is also a minimizer of \\(f\\) .","title":"Overparameterization"},{"location":"chapters/optimization/overparam/#belkins-claim","text":"(1) If we have a solution manifold and (2) if this manifold has curvature then the loss function is not locally convex. Proof Assume we have a solution manifold. \\[f(y) \\geq f(x) + \\nabla f(x)^T(y-x)\\] Lemma Minimizers of Convex Function form a Convex Set Let \\(x_1, x_2\\) be minimizers of a convex function \\(f\\) . i.e. \\(\\forall x \\ f(x) \\geq x_1, x_2\\) Then \\(\\forall \\alpha \\in (0,1), \\alpha x_1 + (1-\\alpha x_2)\\) is also a minimizer of \\(f\\) .","title":"Belkin's Claim:"},{"location":"chapters/optimization/papers/","text":"The loss landscape of overparameterized neural networks","title":"Papers"},{"location":"chapters/other/LLM/introduction/","text":"References A Neural Probabilistic Language Model Aim The aim is to \"learn\" \\(\\mathbb{P}\\) . \\[\\Big(\\{\\textrm{Sequences of Words}\\}, \\mathcal{F}, \\mathbb{P} \\Big)\\] High Level Approach We can achieve this objective as follows: Learn a distribution over the first word of a sequence. \\[\\big(\\mathcal{V}, \\mathcal{F}, \\mathbb{P} \\big)\\] Learn the conditional distribution \\[\\textrm{Model} :: \\textrm{Params} \\to \\{\\textrm{context}\\} \\to \\mathbb{P}_{\\mid \\textrm{context}}\\] To Do This should be made more exact A Neural Probabilistic Language Model Essence transfer probability mass In the proposed model, it will so generalize because \u201csimilar\u201d words are expected to have a similar feature vector, and because the probability function is a smooth function of these feature values, a small change in the features will induce a small change in the probability We can construct the condition distribution as follows. First we introduce an embedding function. Given that our vocab is finite, we can represent this function as a matrix. i.e. \\(h\\) is isomorphic to \\(\\theta \\in {| \\mathcal{V} | \\times m}\\) \\[h :: \\mathcal{V} \\to \\mathcal{R}^p\\] We introduce a function \\(g\\) which maps subsequences of these embeddings into a conditional distribution Given this level of detail we could augment the signature of our model as follows: \\[\\textrm{Model} :: \\textrm{Embedding Functions} \\to \\textrm{Forward Functions} \\to \\{\\textrm{context}\\} \\to \\mathbb{P}_{\\mid \\textrm{context}}\\] To Do What should the name of this forward function be? Key Insights \"In high dimensions, it is crucial to distribute probability mass where it matters rather than uniformly in all directions around each training point.\" 1 A Neural Probabilistic Language Model \u21a9","title":"Introduction"},{"location":"chapters/other/LLM/introduction/#aim","text":"The aim is to \"learn\" \\(\\mathbb{P}\\) . \\[\\Big(\\{\\textrm{Sequences of Words}\\}, \\mathcal{F}, \\mathbb{P} \\Big)\\]","title":"Aim"},{"location":"chapters/other/LLM/introduction/#high-level-approach","text":"We can achieve this objective as follows: Learn a distribution over the first word of a sequence. \\[\\big(\\mathcal{V}, \\mathcal{F}, \\mathbb{P} \\big)\\] Learn the conditional distribution \\[\\textrm{Model} :: \\textrm{Params} \\to \\{\\textrm{context}\\} \\to \\mathbb{P}_{\\mid \\textrm{context}}\\] To Do This should be made more exact","title":"High Level Approach"},{"location":"chapters/other/LLM/introduction/#a-neural-probabilistic-language-model","text":"Essence transfer probability mass In the proposed model, it will so generalize because \u201csimilar\u201d words are expected to have a similar feature vector, and because the probability function is a smooth function of these feature values, a small change in the features will induce a small change in the probability We can construct the condition distribution as follows. First we introduce an embedding function. Given that our vocab is finite, we can represent this function as a matrix. i.e. \\(h\\) is isomorphic to \\(\\theta \\in {| \\mathcal{V} | \\times m}\\) \\[h :: \\mathcal{V} \\to \\mathcal{R}^p\\] We introduce a function \\(g\\) which maps subsequences of these embeddings into a conditional distribution Given this level of detail we could augment the signature of our model as follows: \\[\\textrm{Model} :: \\textrm{Embedding Functions} \\to \\textrm{Forward Functions} \\to \\{\\textrm{context}\\} \\to \\mathbb{P}_{\\mid \\textrm{context}}\\] To Do What should the name of this forward function be?","title":"A Neural Probabilistic Language Model"},{"location":"chapters/other/LLM/introduction/#key-insights","text":"\"In high dimensions, it is crucial to distribute probability mass where it matters rather than uniformly in all directions around each training point.\" 1 A Neural Probabilistic Language Model \u21a9","title":"Key Insights"},{"location":"chapters/other/LLM/rhlf/","text":"Reinforcement Learning Overview David Silver in his introductory class on RL makes the following distinctions between reinforcement learning and supervised learning: \"There is no supervisor, only a reward signal\". I am not a fan of this distinction because in supervised learning we don't have a supervisor. Generally, we're not trying to perfectly interpolate the data. Rather, we have a reward signal of the loss function \\((y - f_{\\theta}(x))^2\\) \"Feedback is delayed, not instantaneous\" \"Time really matters (sequential, non i.i.d data)\" and \"Agent's actions affect the subsequent data it receives\" The RL Problem Let \\(\\Theta\\) be a parameter space and \\(\\theta \\in \\Theta\\) a parameter vector that governs the dynamics of a stochastic process: \\(\\big(\\Omega, \\mathcal{F}, \\mathbb{P}_{\\theta}\\big)\\) . We are interested in identifying the parameter \\(\\theta^*\\) that maximizes the expected cumulative reward over time. Formally, the optimization problem can be stated as: We can define the following two random variables (Trajectories, and Rewards of Trajectories) \\[\\begin{align*} \\tau : \\Omega \\to \\textrm{Path Space} \\\\ R : \\textrm{Path Space} \\to \\mathcal{R} \\end{align*}\\] Then the expected reward is denoted by: \\[\\begin{align*} \\int _{\\Omega} R \\circ \\tau \\mathbb{P}_{\\theta} \\end{align*}\\] Policy Search The idea behind policy search is that we are going to approximate the above value by constructing a new probability space which is just the `n' product of the original probability space: \\[\\begin{align*} \\big( \\Omega_n, \\mathcal{F}_n, \\mathbb{P}_{\\theta, n}) \\end{align*}\\] On this probability space, we can define our estimator: \\[\\begin{align*} \\hat{V} &: \\Omega_n \\to \\mathcal{R} \\\\ \\omega_n &\\longmapsto \\sum _i R \\circ \\tau_i \\end{align*}\\] RHLF the challenge with this set-up is the effective domain of the reward model We use the KL constraint to try and account for this issue","title":"RHLF"},{"location":"chapters/other/LLM/rhlf/#reinforcement-learning-overview","text":"David Silver in his introductory class on RL makes the following distinctions between reinforcement learning and supervised learning: \"There is no supervisor, only a reward signal\". I am not a fan of this distinction because in supervised learning we don't have a supervisor. Generally, we're not trying to perfectly interpolate the data. Rather, we have a reward signal of the loss function \\((y - f_{\\theta}(x))^2\\) \"Feedback is delayed, not instantaneous\" \"Time really matters (sequential, non i.i.d data)\" and \"Agent's actions affect the subsequent data it receives\"","title":"Reinforcement Learning Overview"},{"location":"chapters/other/LLM/rhlf/#the-rl-problem","text":"Let \\(\\Theta\\) be a parameter space and \\(\\theta \\in \\Theta\\) a parameter vector that governs the dynamics of a stochastic process: \\(\\big(\\Omega, \\mathcal{F}, \\mathbb{P}_{\\theta}\\big)\\) . We are interested in identifying the parameter \\(\\theta^*\\) that maximizes the expected cumulative reward over time. Formally, the optimization problem can be stated as: We can define the following two random variables (Trajectories, and Rewards of Trajectories) \\[\\begin{align*} \\tau : \\Omega \\to \\textrm{Path Space} \\\\ R : \\textrm{Path Space} \\to \\mathcal{R} \\end{align*}\\] Then the expected reward is denoted by: \\[\\begin{align*} \\int _{\\Omega} R \\circ \\tau \\mathbb{P}_{\\theta} \\end{align*}\\]","title":"The RL Problem"},{"location":"chapters/other/LLM/rhlf/#policy-search","text":"The idea behind policy search is that we are going to approximate the above value by constructing a new probability space which is just the `n' product of the original probability space: \\[\\begin{align*} \\big( \\Omega_n, \\mathcal{F}_n, \\mathbb{P}_{\\theta, n}) \\end{align*}\\] On this probability space, we can define our estimator: \\[\\begin{align*} \\hat{V} &: \\Omega_n \\to \\mathcal{R} \\\\ \\omega_n &\\longmapsto \\sum _i R \\circ \\tau_i \\end{align*}\\]","title":"Policy Search"},{"location":"chapters/other/LLM/rhlf/#rhlf","text":"the challenge with this set-up is the effective domain of the reward model We use the KL constraint to try and account for this issue","title":"RHLF"},{"location":"chapters/probability%20theory/clusters/","text":"Propensity Score The propensity score is observable: \\[\\pi(X_i) = \\mathbb{P}(D_i = 1 \\mid X_i)\\] That is, given access to the following probability space, we can compute the probability measure. \\[\\big( \\mathcal{X}\\times \\mathcal{D} \\times \\mathcal{Y}, \\mathcal{F}, \\mathcal{P} \\big)\\] To Do Add in the details","title":"Clusters"},{"location":"chapters/probability%20theory/clusters/#propensity-score","text":"The propensity score is observable: \\[\\pi(X_i) = \\mathbb{P}(D_i = 1 \\mid X_i)\\] That is, given access to the following probability space, we can compute the probability measure. \\[\\big( \\mathcal{X}\\times \\mathcal{D} \\times \\mathcal{Y}, \\mathcal{F}, \\mathcal{P} \\big)\\] To Do Add in the details","title":"Propensity Score"},{"location":"chapters/probability%20theory/conditional_expectation/","text":"Although the CEF is random variable, it's not \"defined\" pointwise. \\[\\begin{align*} &\\mathbb{E}[Y|X] :: \\Omega \\to \\mathcal{R} \\\\ & \\mathcal{I} \\ \\mathbb{P} \\ A \\ Y = \\mathcal{I} \\ \\mathbb{P} \\ A \\ \\mathbb{E}[Y|X] \\quad \\forall \\ A \\in \\sigma(X) \\end{align*}\\] The law of iteration of expectations is part of the definition \\[\\begin{align*} & \\mathcal{I} \\ \\mathbb{P} \\ \\Omega \\ Y = \\mathcal{I} \\ \\mathbb{P} \\ \\Omega \\ \\mathbb{E}[Y|X] \\end{align*}\\]","title":"Conditional Expectation"},{"location":"chapters/probability%20theory/conditioning/","text":"Conditioning Given a measurable space, \\((\\Omega, \\mathcal{F})\\) , let \\(\\mathcal{M}\\) be the set of probability measures defined on this space. Then conditioning can be defined as follows: Events \\[\\begin{align*} &C :: \\mathcal{M} \\to \\mathcal{F}_+ \\to \\mathcal{M} \\\\ & C \\ \\mathbb{P} \\ A \\ B = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(A)} \\end{align*}\\] Random Variables \\[\\begin{align*} &\\tilde{C} :: \\mathcal{M} \\to (\\Omega \\to \\mathcal{R}) \\to \\mathcal{F}_{\\sigma(X)}\\to \\mathcal{M} \\\\ & C \\ \\mathbb{P} \\ A \\ B = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(A)} \\end{align*}\\] Two things to note here: Notice the dependent structure between \\(\\mathcal{M}\\) and \\(\\mathcal{F}_+\\) . The measure restricts the set of events that we can condition on. Notice also that if we rearrange the signature of the function, as done below, \\(C \\ \\Omega\\) would be the identity function. \\[C ::\\mathcal{F}_+ \\to \\mathcal{M} \\to \\mathcal{M}\\]","title":"Conditioning"},{"location":"chapters/probability%20theory/conditioning/#conditioning","text":"Given a measurable space, \\((\\Omega, \\mathcal{F})\\) , let \\(\\mathcal{M}\\) be the set of probability measures defined on this space. Then conditioning can be defined as follows: Events \\[\\begin{align*} &C :: \\mathcal{M} \\to \\mathcal{F}_+ \\to \\mathcal{M} \\\\ & C \\ \\mathbb{P} \\ A \\ B = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(A)} \\end{align*}\\] Random Variables \\[\\begin{align*} &\\tilde{C} :: \\mathcal{M} \\to (\\Omega \\to \\mathcal{R}) \\to \\mathcal{F}_{\\sigma(X)}\\to \\mathcal{M} \\\\ & C \\ \\mathbb{P} \\ A \\ B = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(A)} \\end{align*}\\] Two things to note here: Notice the dependent structure between \\(\\mathcal{M}\\) and \\(\\mathcal{F}_+\\) . The measure restricts the set of events that we can condition on. Notice also that if we rearrange the signature of the function, as done below, \\(C \\ \\Omega\\) would be the identity function. \\[C ::\\mathcal{F}_+ \\to \\mathcal{M} \\to \\mathcal{M}\\]","title":"Conditioning"},{"location":"chapters/probability%20theory/convergence/","text":"References STATS 203 - Large Sample Theory Master Program: Probability Theory - Lecture 3: Applications of independence Introduction TLDR The defining feature of convergence is the restriction on the underlying probability space. In convergence in Law/distribution, we do not require \\(X_n\\) and \\(X\\) to be defined on the same probability space. In convergence in probability or expectation, we require that for each \\(n\\) , \\(X_n\\) and \\(X\\) are defined on the same probability space, but this probability space is allowed to change with \\(n\\) . In convergence almost surley, the underlying probability space for \\(X_n\\) and \\(X\\) must be the same and fixed for all \\(n\\) . Weak Law of Large Numbers Presentation It seems like there are at least two distinct ways to illustrate this result. The first places more emphasis on random variables, the second, which I do below, places more emphasis on empirical measures. The relationship between these two approaches is the following: \\[\\mathcal{I} \\ (x \\mapsto x) \\ \\hat{\\mathbb{P}}_n \\equiv \\frac{X_1 + X_2 + \\dots + X_n}{n}\\] We begin by defining a probability space parameterized by \\(n\\) . \\[\\Big(\\mathcal{R}^n, \\mathcal{B}(\\mathcal{R}^n), \\mathbb{P}^n\\Big)\\] Projection Random Variables Defined as follows: \\[\\omega \\in \\Omega_n \\longmapsto X_i(\\omega) = \\omega[i] \\] with \\(n\\) i.i.d projection random variables . \\[ \\begin{align*} \\hat{\\mathbb{P}}_n &:: \\mathcal{R}^n \\to \\mathcal{B}(\\mathcal{R}) \\to [0,1] \\\\ \\mathcal{I}_n &:: (\\mathcal{R}^n \\to \\mathcal{R}) \\to (\\mathcal{B}(\\mathcal{R}^n) \\to [0,1]) \\to \\mathcal{R} \\\\ \\mathcal{I}_1 \\ (x \\mapsto x) \\ \\hat{\\mathbb{P}}_n &:: \\mathcal{R}^n \\to [0,1] \\end{align*}\\] From this, we can define the following sequence of random variables: \\[\\begin{align*} d_n &:: \\mathcal{R}^n \\to [0,1] \\\\ & := \\mathcal{I}_1 \\ (x \\mapsto x) \\ \\hat{\\mathbb{P}}_n - \\mathcal{I}_1 \\ (x \\mapsto x) \\ \\mathbb{P} \\end{align*}\\] We can show that the following result. \\[\\underset{n \\to \\infty}{\\lim} \\mathcal{I}_n \\ d_n \\ \\mathbb{P}_n = 0 \\] Result We say \\[\\mathcal{I}_1 \\ (x \\mapsto x) \\ \\hat{\\mathbb{P}}_n \\ \\textrm{converges in} \\ L_2 \\ \\textrm{to} \\ \\mathcal{I}_1 \\ (x \\mapsto x) \\ \\mathbb{P}\\] Convergence in Distribution (or Law) A random variable \\(X_n\\) convergences in distribution (or in law )to \\(X\\) when the corresponding sequence of CDFs converge \"pointwise\" to the CDF of \\(X\\) \\[\\begin{align} X_n \\to_D X \\implies F_{X_n} \\to F_X \\end{align}\\] Helly-Bray Theorem TLDR If the parameters of our model converge in distribution or law, then we have an asymptotically unbiased estimate. Let \\(\\theta_n \\to_D \\theta\\) . Let \\(g\\) be a continuous and bounded function. Then: \\[I \\ \\Omega_1 \\ \\mathcal{P}_n \\ (g \\circ \\theta_n) \\to I \\ \\Omega_2 \\ \\mathcal{P} \\ (g \\circ \\theta)\\] Note, if we define our estimator \\(\\hat{\\beta}_n\\) as follows. \\[\\hat{\\beta}_n := g \\circ \\theta_n = \\int f(\\theta_n, X) d\\mathbb{P}_X\\] Then this theorem tells us that \\[\\int \\hat{\\beta}_n d\\mathbb{P}_n \\to \\beta\\] Convergence in Probability If the limit is a random variable, then we care aboout the joint distribution! A random variable \\(X_n\\) converges in probability to \\(X\\) when for all \\(\\varepsilon > 0\\) the following holds: \\[\\begin{align} \\underset{n \\to \\infty}{\\lim}\\mathbb{P}(\\|X_n - X \\| > \\varepsilon)= 0 \\end{align}\\] Applications Weak Law of Large Numbers Proof given here Let \\(X_1, \\dots, X_n\\) be a family of i.i.d random variables with a finite second moment. Then \\[\\underset{n \\to \\infty}{\\lim} \\mathbb{E}\\Big[\\Big( \\frac{X_1 + \\dots + X_n}{n} - \\mathbb{E}[X]\\Big)^2\\Big] = 0\\]","title":"Convergence"},{"location":"chapters/probability%20theory/convergence/#introduction","text":"TLDR The defining feature of convergence is the restriction on the underlying probability space. In convergence in Law/distribution, we do not require \\(X_n\\) and \\(X\\) to be defined on the same probability space. In convergence in probability or expectation, we require that for each \\(n\\) , \\(X_n\\) and \\(X\\) are defined on the same probability space, but this probability space is allowed to change with \\(n\\) . In convergence almost surley, the underlying probability space for \\(X_n\\) and \\(X\\) must be the same and fixed for all \\(n\\) .","title":"Introduction"},{"location":"chapters/probability%20theory/convergence/#weak-law-of-large-numbers","text":"Presentation It seems like there are at least two distinct ways to illustrate this result. The first places more emphasis on random variables, the second, which I do below, places more emphasis on empirical measures. The relationship between these two approaches is the following: \\[\\mathcal{I} \\ (x \\mapsto x) \\ \\hat{\\mathbb{P}}_n \\equiv \\frac{X_1 + X_2 + \\dots + X_n}{n}\\] We begin by defining a probability space parameterized by \\(n\\) . \\[\\Big(\\mathcal{R}^n, \\mathcal{B}(\\mathcal{R}^n), \\mathbb{P}^n\\Big)\\] Projection Random Variables Defined as follows: \\[\\omega \\in \\Omega_n \\longmapsto X_i(\\omega) = \\omega[i] \\] with \\(n\\) i.i.d projection random variables . \\[ \\begin{align*} \\hat{\\mathbb{P}}_n &:: \\mathcal{R}^n \\to \\mathcal{B}(\\mathcal{R}) \\to [0,1] \\\\ \\mathcal{I}_n &:: (\\mathcal{R}^n \\to \\mathcal{R}) \\to (\\mathcal{B}(\\mathcal{R}^n) \\to [0,1]) \\to \\mathcal{R} \\\\ \\mathcal{I}_1 \\ (x \\mapsto x) \\ \\hat{\\mathbb{P}}_n &:: \\mathcal{R}^n \\to [0,1] \\end{align*}\\] From this, we can define the following sequence of random variables: \\[\\begin{align*} d_n &:: \\mathcal{R}^n \\to [0,1] \\\\ & := \\mathcal{I}_1 \\ (x \\mapsto x) \\ \\hat{\\mathbb{P}}_n - \\mathcal{I}_1 \\ (x \\mapsto x) \\ \\mathbb{P} \\end{align*}\\] We can show that the following result. \\[\\underset{n \\to \\infty}{\\lim} \\mathcal{I}_n \\ d_n \\ \\mathbb{P}_n = 0 \\] Result We say \\[\\mathcal{I}_1 \\ (x \\mapsto x) \\ \\hat{\\mathbb{P}}_n \\ \\textrm{converges in} \\ L_2 \\ \\textrm{to} \\ \\mathcal{I}_1 \\ (x \\mapsto x) \\ \\mathbb{P}\\]","title":"Weak Law of Large Numbers"},{"location":"chapters/probability%20theory/convergence/#convergence-in-distribution-or-law","text":"A random variable \\(X_n\\) convergences in distribution (or in law )to \\(X\\) when the corresponding sequence of CDFs converge \"pointwise\" to the CDF of \\(X\\) \\[\\begin{align} X_n \\to_D X \\implies F_{X_n} \\to F_X \\end{align}\\] Helly-Bray Theorem TLDR If the parameters of our model converge in distribution or law, then we have an asymptotically unbiased estimate. Let \\(\\theta_n \\to_D \\theta\\) . Let \\(g\\) be a continuous and bounded function. Then: \\[I \\ \\Omega_1 \\ \\mathcal{P}_n \\ (g \\circ \\theta_n) \\to I \\ \\Omega_2 \\ \\mathcal{P} \\ (g \\circ \\theta)\\] Note, if we define our estimator \\(\\hat{\\beta}_n\\) as follows. \\[\\hat{\\beta}_n := g \\circ \\theta_n = \\int f(\\theta_n, X) d\\mathbb{P}_X\\] Then this theorem tells us that \\[\\int \\hat{\\beta}_n d\\mathbb{P}_n \\to \\beta\\]","title":"Convergence in Distribution (or Law)"},{"location":"chapters/probability%20theory/convergence/#convergence-in-probability","text":"If the limit is a random variable, then we care aboout the joint distribution! A random variable \\(X_n\\) converges in probability to \\(X\\) when for all \\(\\varepsilon > 0\\) the following holds: \\[\\begin{align} \\underset{n \\to \\infty}{\\lim}\\mathbb{P}(\\|X_n - X \\| > \\varepsilon)= 0 \\end{align}\\]","title":"Convergence in Probability"},{"location":"chapters/probability%20theory/convergence/#applications","text":"","title":"Applications"},{"location":"chapters/probability%20theory/convergence/#weak-law-of-large-numbers_1","text":"Proof given here Let \\(X_1, \\dots, X_n\\) be a family of i.i.d random variables with a finite second moment. Then \\[\\underset{n \\to \\infty}{\\lim} \\mathbb{E}\\Big[\\Big( \\frac{X_1 + \\dots + X_n}{n} - \\mathbb{E}[X]\\Big)^2\\Big] = 0\\]","title":"Weak Law of Large Numbers"},{"location":"chapters/probability%20theory/expectations/","text":"Introduction Integration can be thought of as a function that takes a probability measure, a set, and a random variable \\[\\begin{align*} &\\mathcal{I} :: \\mathcal{F} \\to \\mathcal{M} \\to \\{\\Omega \\to \\mathcal{R} \\} \\to \\mathcal{R} \\cup \\{ \\pm \\infty\\}\\\\ &\\mathcal{I} \\ A \\ \\mathbb{P} \\ X = \\int _A X d\\mathbb{P} \\end{align*}\\] Lebesgue Integral The lebesgue intergral is simply the following where \\(\\lambda\\) denotes the lebesgue measure \\[\\mathcal{I}_{\\lambda}\\] Linearity You have probably heard in various classes that integration is a linear function. What people mean by this is that the following higher-order function is linear \\[\\mathcal{I} \\ \\mathcal{P} \\ A \\] Working Across Probability Spaces \\[\\int _A fd\\mathbb{P} = \\int_{f(A)} x d\\mathbb{P}_f\\] To Do is \\(f(A)\\) a measurable set??","title":"Expectation"},{"location":"chapters/probability%20theory/expectations/#introduction","text":"Integration can be thought of as a function that takes a probability measure, a set, and a random variable \\[\\begin{align*} &\\mathcal{I} :: \\mathcal{F} \\to \\mathcal{M} \\to \\{\\Omega \\to \\mathcal{R} \\} \\to \\mathcal{R} \\cup \\{ \\pm \\infty\\}\\\\ &\\mathcal{I} \\ A \\ \\mathbb{P} \\ X = \\int _A X d\\mathbb{P} \\end{align*}\\]","title":"Introduction"},{"location":"chapters/probability%20theory/expectations/#lebesgue-integral","text":"The lebesgue intergral is simply the following where \\(\\lambda\\) denotes the lebesgue measure \\[\\mathcal{I}_{\\lambda}\\]","title":"Lebesgue Integral"},{"location":"chapters/probability%20theory/expectations/#linearity","text":"You have probably heard in various classes that integration is a linear function. What people mean by this is that the following higher-order function is linear \\[\\mathcal{I} \\ \\mathcal{P} \\ A \\]","title":"Linearity"},{"location":"chapters/probability%20theory/expectations/#working-across-probability-spaces","text":"\\[\\int _A fd\\mathbb{P} = \\int_{f(A)} x d\\mathbb{P}_f\\] To Do is \\(f(A)\\) a measurable set??","title":"Working Across Probability Spaces"},{"location":"chapters/probability%20theory/independence/","text":"Events Given a probability space \\[\\big(\\Omega, \\mathcal{F}, \\mathbb{P}\\big)\\] Two Events : \\(A, B\\) are independent under \\(\\mathbb{P}\\) if \\[\\mathbb{P}( A \\cap B) = \\mathbb{P}(A) \\mathbb{P}(B)\\] Finite Collection of Events : \\(A_1, A_2, \\dots, A_n\\) are independent if \\[\\forall I_0 \\subset \\{1,2, \\dots, n\\}, \\quad \\mathbb{P}\\big(\\cap _{i \\in I_0} A_i \\big) = \\prod _{i \\in I_0} \\mathbb{P}(A_i)\\] Arbitrary Collection of Events : \\(\\{A_i, i \\in I\\}\\) Independent if for any finite subset, the independent condition defined above holds Sub- \\(\\sigma\\) -algebras Two sub- \\(\\sigma\\) -algebras : \\(\\mathcal{F}_1, \\mathcal{F}_2\\) \\[\\forall A_1 \\in \\mathcal{F}_1, \\forall A_2 \\in \\mathcal{F}_2, \\quad \\mathbb{P}( A_1 \\cap A_2) = \\mathbb{P}(A_1) \\mathbb{P}(A_2)\\] Arbitrary Collection of Sub- \\(\\sigma\\) -algebras : \\(\\{\\mathcal{F}_i, i \\in I\\}\\) Are independent if for any subset of \\(\\Omega\\) selected from any sub- \\(\\sigma\\) -algebras, that collection of subsets is indepdent as defined above Random Variables \\(X, Y\\) are independent if the \\(\\sigma\\) -algebras generated by these random variables are independent as defined above \\(\\sigma\\) -algebra generated by the random variable \\(X\\) \\[\\sigma(X) := \\{A \\in \\mathcal{F} \\mid A = X^{-1}(B) \\ \\textrm{for} \\ B \\in \\mathcal{B}(\\mathcal{R})\\}\\]","title":"Independence"},{"location":"chapters/probability%20theory/independence/#events","text":"Given a probability space \\[\\big(\\Omega, \\mathcal{F}, \\mathbb{P}\\big)\\] Two Events : \\(A, B\\) are independent under \\(\\mathbb{P}\\) if \\[\\mathbb{P}( A \\cap B) = \\mathbb{P}(A) \\mathbb{P}(B)\\] Finite Collection of Events : \\(A_1, A_2, \\dots, A_n\\) are independent if \\[\\forall I_0 \\subset \\{1,2, \\dots, n\\}, \\quad \\mathbb{P}\\big(\\cap _{i \\in I_0} A_i \\big) = \\prod _{i \\in I_0} \\mathbb{P}(A_i)\\] Arbitrary Collection of Events : \\(\\{A_i, i \\in I\\}\\) Independent if for any finite subset, the independent condition defined above holds","title":"Events"},{"location":"chapters/probability%20theory/independence/#sub-sigma-algebras","text":"Two sub- \\(\\sigma\\) -algebras : \\(\\mathcal{F}_1, \\mathcal{F}_2\\) \\[\\forall A_1 \\in \\mathcal{F}_1, \\forall A_2 \\in \\mathcal{F}_2, \\quad \\mathbb{P}( A_1 \\cap A_2) = \\mathbb{P}(A_1) \\mathbb{P}(A_2)\\] Arbitrary Collection of Sub- \\(\\sigma\\) -algebras : \\(\\{\\mathcal{F}_i, i \\in I\\}\\) Are independent if for any subset of \\(\\Omega\\) selected from any sub- \\(\\sigma\\) -algebras, that collection of subsets is indepdent as defined above","title":"Sub-\\(\\sigma\\)-algebras"},{"location":"chapters/probability%20theory/independence/#random-variables","text":"\\(X, Y\\) are independent if the \\(\\sigma\\) -algebras generated by these random variables are independent as defined above \\(\\sigma\\) -algebra generated by the random variable \\(X\\) \\[\\sigma(X) := \\{A \\in \\mathcal{F} \\mid A = X^{-1}(B) \\ \\textrm{for} \\ B \\in \\mathcal{B}(\\mathcal{R})\\}\\]","title":"Random Variables"},{"location":"chapters/probability%20theory/inference/","text":"The Inference Ladder Our estimator can be defined via the following components: \\[\\begin{align*}\\theta_n &:: \\Omega_n \\to \\mathcal{R}^{d(n)} \\\\ \\\\ f_n &:: \\mathcal{R}^{d(n)} \\to \\mathcal{X} \\to \\mathcal{R} \\\\ \\\\ \\gamma _n &:: (\\mathcal{X} \\to \\mathcal{R}) \\to \\Omega_n \\to \\mathcal{R} \\end{align*}\\] Our estimator is constructed by composing these elements as follows: \\[\\begin{align*}\\theta_n &:: \\Omega_n \\to \\mathcal{R}^{d(n)} \\\\ \\\\ f_n \\circ \\theta_n &:: \\Omega_n \\to \\mathcal{X} \\to \\mathcal{R} \\\\ \\\\ \\gamma _n \\circ f_n \\circ \\theta_n &:: \\Omega_n \\to \\Omega_n \\to \\mathcal{R} \\end{align*}\\] Random Function(als) A random variable has the following type signature: \\[Z :: \\Omega \\to \\mathcal{R}\\] A random function as the following type signature: \\[Z :: \\Omega \\to \\mathcal{X} \\to \\mathcal{R}\\] A random funtional as the following type signature: \\[Z :: \\Omega \\to (\\mathcal{X} \\to \\mathcal{R}) \\to \\mathcal{R}\\] From this, we observe that \\(f_n \\circ \\theta_n\\) is a random function and that \\(\\gamma _n\\) is a random functional Other \\[\\begin{align*} \\hat{g} :: \\Omega_n \\to \\mathcal{X} \\to \\mathcal{R}\\end{align*}\\] Partial Convergence","title":"Inference"},{"location":"chapters/probability%20theory/inference/#the-inference-ladder","text":"Our estimator can be defined via the following components: \\[\\begin{align*}\\theta_n &:: \\Omega_n \\to \\mathcal{R}^{d(n)} \\\\ \\\\ f_n &:: \\mathcal{R}^{d(n)} \\to \\mathcal{X} \\to \\mathcal{R} \\\\ \\\\ \\gamma _n &:: (\\mathcal{X} \\to \\mathcal{R}) \\to \\Omega_n \\to \\mathcal{R} \\end{align*}\\] Our estimator is constructed by composing these elements as follows: \\[\\begin{align*}\\theta_n &:: \\Omega_n \\to \\mathcal{R}^{d(n)} \\\\ \\\\ f_n \\circ \\theta_n &:: \\Omega_n \\to \\mathcal{X} \\to \\mathcal{R} \\\\ \\\\ \\gamma _n \\circ f_n \\circ \\theta_n &:: \\Omega_n \\to \\Omega_n \\to \\mathcal{R} \\end{align*}\\] Random Function(als) A random variable has the following type signature: \\[Z :: \\Omega \\to \\mathcal{R}\\] A random function as the following type signature: \\[Z :: \\Omega \\to \\mathcal{X} \\to \\mathcal{R}\\] A random funtional as the following type signature: \\[Z :: \\Omega \\to (\\mathcal{X} \\to \\mathcal{R}) \\to \\mathcal{R}\\] From this, we observe that \\(f_n \\circ \\theta_n\\) is a random function and that \\(\\gamma _n\\) is a random functional Other \\[\\begin{align*} \\hat{g} :: \\Omega_n \\to \\mathcal{X} \\to \\mathcal{R}\\end{align*}\\]","title":"The Inference Ladder"},{"location":"chapters/probability%20theory/inference/#partial-convergence","text":"","title":"Partial Convergence"},{"location":"chapters/probability%20theory/introduction/","text":"In many applied contexts we are interested in the behavior of the estimator and the interpretation of the estimate. Our starting point for this discussion is a probability space. Well, actually several of them, but they can be understood as transformations of the original one, so let's start there. Unobserved \\[\\Big( \\mathcal{Y} \\times \\mathcal{Y} \\times \\mathcal{X} \\times \\mathcal{D}, \\mathcal{F}, \\mathbb{P}_0\\Big)\\] Observed \\[\\Big( \\mathcal{Y} \\times \\mathcal{X} \\times \\mathcal{D}, \\mathcal{F} , \\mathbb{P}\\Big)\\] \\(Sample\\) \\[\\begin{align*} &D_{i}:: \\Omega \\to \\mathcal{R} \\\\ &D_{i} \\ (\\_ \\ \\_ \\ d) = d \\end{align*}\\] \\(Y_i\\) \\[\\begin{align*} &Y_{i}:: \\Omega \\to \\mathcal{R} \\\\ &Y_{i} \\ (y_0 \\ y_1 \\ d) = dy_1 + (1-d)y_0 \\end{align*}\\] Given this probability space, we can then define the random variables of interest as follows: \\(Y_{i0}\\) \\[\\begin{align*} &Y_{i0}:: \\Omega \\to \\mathcal{R} \\\\ &Y_{i0} \\ (y_0 \\ \\_ \\ \\_) = y_0 \\end{align*}\\] \\(Y_{i1}\\) \\[\\begin{align*} &Y_{i1}:: \\Omega \\to \\mathcal{R} \\\\ &Y_{i1} \\ (\\_ \\ y_1 \\ \\_) = y_1 \\end{align*}\\] \\(D_i\\) \\[\\begin{align*} &D_{i}:: \\Omega \\to \\mathcal{R} \\\\ &D_{i} \\ (\\_ \\ \\_ \\ d) = d \\end{align*}\\] \\(Y_i\\) \\[\\begin{align*} &Y_{i}:: \\Omega \\to \\mathcal{R} \\\\ &Y_{i} \\ (y_0 \\ y_1 \\ d) = dy_1 + (1-d)y_0 \\end{align*}\\] We can say that treatment is indepdent of the potential outcome if the corresponding \\(\\sigma\\) -algebras are independent. More intuitively, this is equivalent (as shown here ). \\[\\forall B_1,B_2 \\in \\mathcal{B}(\\mathcal{R}), \\quad \\mathbb{P}(D_i \\in B_1, Y_{i0} \\in B_2) = \\mathbb{P}(D_i \\in B_1)\\mathbb{P}(Y_{i0} \\in B_2)\\] Working Across Probability Spaces Independence you will often here that treatment is independent of the potential outcomes. While you probably have an intuitive sense of what this means, it can be helpful to formally define this. To start, let's consider the following probability spaces: Expectations Many terms/properties can be understood as working across multiple probability spaces. \\[\\int _A fd\\mathbb{P} = \\int_{f(A)} x d\\mathbb{P}_f\\] To Do is \\(f(A)\\) a measurable set??","title":"Introduction"},{"location":"chapters/probability%20theory/introduction/#working-across-probability-spaces","text":"","title":"Working Across Probability Spaces"},{"location":"chapters/probability%20theory/introduction/#independence","text":"you will often here that treatment is independent of the potential outcomes. While you probably have an intuitive sense of what this means, it can be helpful to formally define this. To start, let's consider the following probability spaces:","title":"Independence"},{"location":"chapters/probability%20theory/introduction/#expectations","text":"Many terms/properties can be understood as working across multiple probability spaces. \\[\\int _A fd\\mathbb{P} = \\int_{f(A)} x d\\mathbb{P}_f\\] To Do is \\(f(A)\\) a measurable set??","title":"Expectations"},{"location":"chapters/probability%20theory/key_terms/","text":"Conditioning & Functors \\(\\sigma\\) -algebra of a Random Variable Information. What information about the sample space, \\(\\Omega\\) , does a random variable, \\(X\\) , convey. For any element of the Borel \\(\\sigma\\) -algebra we can tell whether that element occured. \\[X(\\omega) \\in B, \\quad \\textrm{or} \\quad X(\\omega) \\notin B\\] Therefore, we know \\[ \\omega \\in X^{-1}(B), \\quad \\textrm{or} \\quad \\omega \\notin X^{-1}(B)\\] We refer to this set of events as the \\(\\sigma\\) -algebra generated by \\(X\\) . Importantly, it is the set of events that we can tell whether or not they occured if we have \\(X\\) . Conditioning on Random Variables As we have explained else where, conditioning on an event can be understood as mapping a probility measure into another probability measure. \\[\\mathcal{C}:: \\mathcal{M} \\to \\mathcal{F}_+ \\to \\mathcal{M}\\] Events are isomphic to indicator random variables, where the \\(\\sigma\\) -algebra of the indicator random variable for the event \\(A\\) is the following \\[\\{\\emptyset, \\Omega, A, A^c \\}\\] Conditioning on a random variable can then be understood as \"lifting\" the above function! \\[\\tilde{C} :: \\mathcal{M} \\to (\\Omega \\to \\mathcal{R}) \\to \\mathcal{F}_{\\sigma(X)} \\to \\mathcal{M}\\] The question is, what do we do with this?!","title":"Key Terms"},{"location":"chapters/probability%20theory/key_terms/#sigma-algebra-of-a-random-variable","text":"Information. What information about the sample space, \\(\\Omega\\) , does a random variable, \\(X\\) , convey. For any element of the Borel \\(\\sigma\\) -algebra we can tell whether that element occured. \\[X(\\omega) \\in B, \\quad \\textrm{or} \\quad X(\\omega) \\notin B\\] Therefore, we know \\[ \\omega \\in X^{-1}(B), \\quad \\textrm{or} \\quad \\omega \\notin X^{-1}(B)\\] We refer to this set of events as the \\(\\sigma\\) -algebra generated by \\(X\\) . Importantly, it is the set of events that we can tell whether or not they occured if we have \\(X\\) .","title":"\\(\\sigma\\)-algebra of a Random Variable"},{"location":"chapters/probability%20theory/key_terms/#conditioning-on-random-variables","text":"As we have explained else where, conditioning on an event can be understood as mapping a probility measure into another probability measure. \\[\\mathcal{C}:: \\mathcal{M} \\to \\mathcal{F}_+ \\to \\mathcal{M}\\] Events are isomphic to indicator random variables, where the \\(\\sigma\\) -algebra of the indicator random variable for the event \\(A\\) is the following \\[\\{\\emptyset, \\Omega, A, A^c \\}\\] Conditioning on a random variable can then be understood as \"lifting\" the above function! \\[\\tilde{C} :: \\mathcal{M} \\to (\\Omega \\to \\mathcal{R}) \\to \\mathcal{F}_{\\sigma(X)} \\to \\mathcal{M}\\] The question is, what do we do with this?!","title":"Conditioning on Random Variables"},{"location":"chapters/probability%20theory/key_theorems/","text":"Gilvenko-Cantelli The claim is that \\(\\underset{x}{\\textrm{sup}}\\ g\\) (defined below!) converges \"almost surely\" to \\(0\\) . \\[\\begin{align*} &g :: \\mathcal{X} \\to \\Omega \\to \\mathcal{R} \\\\ &g \\ x \\ \\omega = F_n(x, \\omega) - F(x) \\\\ \\\\ &\\underset{x}{\\textrm{sup}}\\ g :: \\Omega \\to \\mathcal{R} \\end{align*}\\]","title":"Key Theorems"},{"location":"chapters/programming/introduction/","text":"","title":"Introduction"},{"location":"chapters/programming/category%20theory/natural%20transformations/","text":"\\[\\textrm{Nautral Transformations} :: \\{\\textrm{Functiioins}\\}\\]","title":"Natural Transformations"}]}