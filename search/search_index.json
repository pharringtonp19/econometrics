{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Summary","text":"Econometrics Introduction <p>Applied econometrics is fundamentally about interpretation -- how to interpret the result of a statistical procedure in a given context. Often, there isn't often a \"valid\" or \"correct\" interpretation. In this course, we'll introduce you to a broad set of statitical tools and real life examples so that you can further develop your own judgement. </p> <p></p> <p>Learning from data is a highly personalized process. The general setup of the process is reflected visually below. We have beliefs about the world; we have beliefs about how a statical procedure behaves under certain conditions; the statistical procedure is applied to a data set returning numerical \"results\" which ultimately shape our new beliefs about the world. </p> <p></p>"},{"location":"chapters/approximation/clusters/","title":"Clusters","text":""},{"location":"chapters/approximation/clusters/#convex-representation","title":"Convex Representation","text":"<p>The Caratheodori Theorem tells us that any point \\(x \\in \\textrm{Conv}(T)\\), where \\(T \\subset \\mathcal{R}^n\\), can be represented as a convex combination of at most \\(n+1\\) points.<sup>1</sup></p> <p>So for example let's say that our features \\(x \\in \\mathcal{R}^n\\). Then for any point in the convex hull of the training set can be represented as the convex combination of at most ten points</p> \\[x = \\sum _{i=1}^{11} \\alpha_i(x)x_i^*(x), \\quad x \\in \\mathcal{R}^{10}\\] <ol> <li> <p>Reference: See here \u21a9</p> </li> </ol>"},{"location":"chapters/approximation/curse_of_dimensionality/","title":"Curse of Dimensionality","text":"<p>A pure sampling phenomena </p>"},{"location":"chapters/approximation/curse_of_dimensionality/#numerical-integration","title":"Numerical Integration","text":"\\[\\int _{[0,1]^d}f dx\\]"},{"location":"chapters/approximation/curse_of_dimensionality/#monte-carlo-integration","title":"Monte-Carlo Integration","text":"\\[\\begin{align*}\\hat{\\theta} &amp;= \\frac{1}{n}\\sum _{i=1}^n \\big(f(X_i) + \\varepsilon_i) \\\\  &amp;= \\frac{1}{n}\\sum _{i=1}^n f(X_i) + \\frac{1}{n}\\sum _{i=1}^n \\varepsilon_i \\\\ \\\\  \\mathbb{E}[\\hat{\\theta}] &amp;= \\mathbb{E} \\Bigg[\\frac{1}{n}\\sum _{i=1}^n f(X_i) + \\frac{1}{n}\\sum _{i=1}^n \\varepsilon_i  \\Bigg] \\\\  &amp;= \\frac{1}{n}\\sum _{i=1}^n \\mathbb{E}\\big[f(X_i)\\big] + \\frac{1}{n}\\sum _{i=1}^n \\mathbb{E}\\underbrace{\\big[\\varepsilon _i\\big]}_{=0} \\\\ &amp;= \\mathbb{E}\\big[f(X_i)\\big] \\\\ \\\\  \\mathbb{E}\\big[(\\hat{\\theta} - \\theta_0 \\big)^2] &amp;= \\textrm{Var}(\\hat{\\theta})\\end{align*}\\]"},{"location":"chapters/approximation/introduction/","title":"Introduction","text":"<p>Abstract</p> <p>We're interested in the behavior/performance of Estimators/Algorithms </p> <p>In applied microeconometrics, where we are generally interested in the causal effect of some policy/intervention, we use estimators with the following signature. </p> \\[\\begin{align*} \\mathcal{A} :: \\{ \\mathcal{X} \\times \\mathcal{Y} \\}^n \\to \\mathcal{R}^p\\\\  \\end{align*}\\] <p>Occasionally, these estimators will have an analytical form. For example, if we are interested in the average outcome we may use the following estimator. </p> \\[\\begin{align*} &amp;\\mathcal{A} :: \\{ \\mathcal{X} \\times \\mathcal{Y} \\}^n \\to \\mathcal{R}\\\\  &amp;\\mathcal{A} \\big(\\{x_i, y_i \\})_{i=1}^n\\big) = \\frac{1}{n} \\sum y_i \\\\  \\end{align*}\\] <p>Or if we are interested in the linear approximation to the CEF we may use the following estimator. </p> \\[\\begin{align*} &amp;\\mathcal{A} :: \\{ \\mathcal{X} \\times \\mathcal{Y} \\}^n \\to \\mathcal{R}^p\\\\  &amp;\\mathcal{A} \\big(\\{x_i, y_i \\})_{i=1}^n\\big) = \\big( X^TX)^{-1}X^TY \\\\  \\end{align*}\\] <p>We may, though, be interested in more \"complex\" estimators that involve neural networks.</p> \\[\\begin{align*} &amp;\\mathcal{A} :: \\{ \\mathcal{X} \\times \\mathcal{Y} \\}^n \\to \\mathcal{R}\\\\  &amp;\\mathcal{A} \\big(\\{x_i, y_i \\})_{i=1}^n\\big) = \\sum f(\\theta_1, x_i) - f(\\theta_2, x_i) \\\\  &amp; \\quad \\textrm{where} \\ \\theta_i = m^*\\big(\\{x_i, y_i \\})_{i=1}^n\\big) \\end{align*}\\] To Do <p>Make the connection to kernel methods</p>"},{"location":"chapters/approximation/introduction/#probability-space","title":"Probability Space","text":"Warning <p>Is the Borel Sigma Algebra well defined here? Should we add restrictions on \\(\\mathcal{X}, \\mathcal{Y}\\)?</p> \\[\\Big( \\mathcal{X} \\times \\mathcal{Y}, \\mathcal{B}(\\mathcal{X} \\times \\mathcal{Y}), \\mathbb{P}\\Big)\\]"},{"location":"chapters/approximation/introduction/#function-space-hypothesis-class-model-class","title":"Function Space/ Hypothesis Class/ Model Class","text":"Warning <p>What additional restrictions should we place on \\(\\mathcal{H}\\)?</p> \\[ \\mathcal{H} := \\{h \\mid h : \\mathcal{X} \\to \\mathcal{Y} \\}\\]"},{"location":"chapters/approximation/introduction/#loss-function","title":"Loss Function","text":"Warning <p>Make note on parameterization</p> \\[\\begin{align*} &amp;l : \\mathcal{H} \\to \\mathcal{X} \\to \\mathcal{Y} \\\\ &amp;l(h, x, y) = (y - h(x))^2 \\end{align*}\\]"},{"location":"chapters/approximation/introduction/#population-risk","title":"Population Risk","text":"\\[\\begin{align*} &amp;L :: \\mathcal{H} \\to \\mathcal{R}_+ \\\\  &amp;L(h) := \\underset{(x,y)\\sim p}{\\mathbb{E}} \\big[l(h, x, y)\\big] \\\\ &amp;L(h)= \\int _{\\mathcal{X} \\times \\mathcal{Y}}l(h, X, Y)d\\mathbb{P} \\end{align*}\\]"},{"location":"chapters/approximation/introduction/#empirical-risk","title":"Empirical Risk","text":"\\[ \\begin{align*} &amp;\\hat{L} :: \\{X, Y\\}^n \\to \\Theta \\to \\mathcal{R}_+ \\\\ &amp;\\hat{L}(\\{x_i, y_i\\}_{i=1}^n, \\theta) = \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i) \\end{align*}\\] Partial Evaluation: Expectation <p>Partially evaluated at \\(\\theta\\), \\(\\hat{L}\\)  is a random variable. Taking its expectation</p> \\[ \\begin{align*} \\mathbb{E}\\big[ \\hat{L} _{\\theta}\\big] &amp;= \\mathbb{E}\\Big[ \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i)\\Big] \\\\  &amp;= \\frac{1}{n} \\sum _i \\mathbb{E}\\big[ l(\\theta, x_i, y_i)\\big] \\\\  &amp;= L(\\theta) \\end{align*}\\] Partial Evaluation: Variance \\[ \\begin{align*} \\textrm{Var}(\\hat{L} _{\\theta})  &amp;= \\textrm{Var}\\Big[ \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i)\\Big] \\\\  &amp;= \\frac{1}{n^2} \\sum _i \\textrm{Var}\\big[ l(\\theta, x_i, y_i)\\big] \\\\  &amp;= \\frac{\\textrm{Var}\\big[ l(\\theta, x_i, y_i)\\big]}{n} \\end{align*}\\] Partial Evaluation: Variance <p>Partially evaluated at \\(\\theta\\), \\(\\hat{L}\\)  is a random variable. Taking its expectation</p> \\[ \\begin{align*} \\mathbb{E}\\big[ \\hat{L} _{\\theta}\\big] &amp;= \\mathbb{E}\\Big[ \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i)\\Big] \\\\  &amp;= \\frac{1}{n} \\sum _i \\mathbb{E}\\big[ l(\\theta, x_i, y_i)\\big] \\\\  &amp;= L(\\theta) \\end{align*}\\]"},{"location":"chapters/approximation/introduction/#hmm","title":"Hmm","text":"<p>But we are not really interested in this relationship, becuase \\(\\hat{L}\\) is not partially evaluated in practice. In practice, we use our training data to determine \\(\\theta\\). That is we have an algorithm \\(\\mathcal{A}\\). </p>"},{"location":"chapters/approximation/introduction/#algorithm","title":"Algorithm","text":"\\[ \\begin{align*} \\mathcal{A} :: \\{\\mathcal{X}, \\mathcal{Y}\\}^n \\to \\Theta  \\end{align*} \\] <ul> <li>Empirical Risk Minimization</li> </ul> \\[ \\begin{align*} &amp;\\textrm{ERM} :: \\{X, Y\\}^n \\to \\Theta  \\\\ &amp;\\textrm{ERM}(\\{x_i, y_i\\}_{i=1}^n) = \\underset{\\theta \\in \\Theta}{\\textrm{minimize}} \\ \\hat{L}(\\{x_i, y_i\\}_{i=1}^n, \\theta)   \\end{align*}\\] <ul> <li> <p>Consistency: </p> \\[\\mathbb{P}_n ( \\| \\mathcal{A}_n - \\theta _0 \\| &gt; \\varepsilon) \\to 0  \\] </li> <li> <p>Which is just a random variable. We can evaluate it as follows: </p> \\[ \\begin{align*} \\mathbb{E} \\big[ L \\circ \\mathcal{A} \\big] \\end{align*} \\] </li> <li> <p>Excess Risk:</p> </li> </ul> \\[  \\begin{align*} &amp;E :: \\mathcal{H} \\to \\mathcal{R}_+ \\\\ &amp;E(h) = L(h) - \\underset{g \\in \\mathcal{H}}{\\inf} L(g) \\end{align*}\\]"},{"location":"chapters/approximation/kernels/","title":"Kernels","text":"<p>These notes are taken from the following lectures: Lecture</p>"},{"location":"chapters/approximation/kernels/#definitions","title":"Definitions","text":"Dual Space <p>Let \\(X\\) be a vector space. Then the dual space of \\(X\\), denoted by \\(X^*\\), is the set of linear bounded functions on \\(X\\). </p>"},{"location":"chapters/approximation/kernels/#reproducing-kernel-hilbert-spaces","title":"Reproducing Kernel Hilbert Spaces","text":"<ul> <li>Let \\(\\mathcal{X}\\) be a set </li> <li>Let \\(F(\\mathcal{X}, \\mathcal{R})\\) be the vector space of funcions defined on \\(\\mathcal{X}\\). i.e. </li> </ul> \\[f_1, f_2 \\in F(\\mathcal{X}, \\mathcal{R}) \\implies \\alpha f_1 + \\beta f_2 \\in F(\\mathcal{X}, \\mathcal{R})\\] <ul> <li> <p>Then \\(\\mathcal{H}(\\mathcal{X}, \\mathcal{R}) \\subset F(\\mathcal{X}, \\mathcal{R})\\) is a Reproducing Kernel Hilbert Space if </p> <ol> <li>\\(\\mathcal{H}(\\mathcal{X}, \\mathcal{R})\\) is a subspace of \\(F(\\mathcal{X}, \\mathcal{R})\\)</li> <li>\\(\\Big(\\mathcal{H}(\\mathcal{X}, \\mathcal{R}), \\langle \\cdot, \\cdot  \\rangle _{\\mathcal{H}} \\Big)\\) is a Hilbert Space</li> <li>Evaluation Functionals, \\(\\textrm{Apply}_x\\), are continuous</li> </ol> \\[ \\textrm{Apply} : \\mathcal{X} \\to \\mathcal{H}(\\mathcal{X}, \\mathcal{R}) \\to \\mathcal{R}  \\] </li> </ul> <p>Note: </p> <ul> <li>We can think of Euclidean Spaces as a function space.</li> </ul> \\[\\mathcal{R} ^n \\equiv \\Big( F(\\textrm{Fin} \\ n, \\mathcal{R}), \\langle z_1, z_2  \\rangle _{F} :=  \\sum _{i=1}^n z_1(i)z_2(i)\\Big)\\] <ul> <li>We can generalize this structure to  \\(l^2(\\mathcal{X})\\)</li> </ul> \\[\\begin{align*} l^2(\\mathcal{X}):= \\Big\\{ f \\mid f:\\mathcal{X} \\to \\mathcal{R}, \\quad  \\sum _{x\\in \\mathcal{X}} |f(x)|^2 &lt; \\infty \\Big\\} \\end{align*}\\] <ul> <li>This structure (set \\(+\\) the norm/inner product)<sup>1</sup> is an RKHS since </li> </ul> \\[\\| E_x \\| \\leq \\| f \\| _{l^2(\\mathcal{X})}\\] <p>Consider the following function </p> \\[\\begin{align*} &amp;\\Lambda :: \\mathcal{H} \\to \\mathcal{H} \\to \\mathcal{R}\\\\  &amp;\\Lambda \\ y \\ x = \\langle x, y \\rangle _{\\mathcal{H}} \\end{align*}\\] <p>We can re-write the signature of the function as follows:</p> \\[\\begin{align*} &amp;\\Lambda :: \\mathcal{H} \\to \\mathcal{H}^*\\\\  \\end{align*}\\] <p>If \\(H\\) is a RKHS, then by definition, \\(\\textrm{Apply} \\ x\\) is a linear bounded functional. By Reisz representation theorem, </p> \\[\\textrm{Apply} \\ x \\ f = \\langle r \\ x, f \\rangle _{\\mathcal{H}} = f \\ x \\] <p>Then we can define the Kernel as follows: </p> \\[\\begin{align*} &amp;K :: \\mathcal{X} \\to \\mathcal{X} \\to \\mathcal{R}  \\\\  &amp;K \\ x \\ y = r \\ x \\ y = \\langle r \\ y,  r \\ x \\rangle  \\end{align*}\\] <ol> <li> <p>I really think the key part of this structure is the inner product\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/fundamentals/difference-in-means/","title":"Difference-in-Means","text":"<p>In the previous note, we motivated why we are interested in estimating the average treatment effect. Given this objective, a natural starting point, it would seem, is to estamate this effect by taking the difference between the average outcomes in the treated group and the average outcomes in the control group. </p> <p>Assuming for the moment, that we observe the entire population (i.e we're not thinking about sampling from a larger population), our difference-in-means strategy can be formally defined as the following. These conditional expectations correspond to the average outcomes for those individuals in the treated/control group respectively. We use the binary variable \\(D_i\\) to denote the treatment status. (\\(D_i=1\\)) means that the person is treated.</p> \\[\\mathbb{E}[Y_i \\vert D_i = 1] - \\mathbb{E}[Y_i \\vert D_i=0]\\] <p>Whenever some proposes something, the first question that comes to mind usually is whether the proposition is a good idea. In our context, this would be - does the difference in means recover the average treatment effect? To assess this, we need to relate the terms in the above expression to the potential outcome function. Let's start with the first term, the expected outcome with the treatment for those who receive treatment. Read that sentence again. It's not easy to read but it conveys important information. There are two key pieces of information that are related to condtioning on \\(D_i=1\\). The first is that we are observing the potential outcome associated with treatment \\(\\tilde{Y}_i(1)\\), and we observe this outcome for those individuals in the treated group. With this in mind, we can re-write the above expression as follows:</p> \\[\\begin{align*}\\mathbb{E}[Y_i \\vert D_i = 1] - \\mathbb{E}[Y_i \\vert D_i=0] &amp;= \\mathbb{E}[\\tilde{Y}_i(1) \\vert D_i = 1] - \\mathbb{E}[\\tilde{Y}_i(0) \\vert D_i = 1] \\end{align*}\\] <p>To be able to interpret this term further, we're goind to add and subtract the following term \\(\\textcolor{blue}{\\mathbb{E}[\\tilde{Y}_i(0) \\vert D_i=1]}\\). </p> \\[\\begin{align*}\\Big( \\mathbb{E}[\\tilde{Y}_i(1) \\vert D_i = 1] - \\textcolor{blue}{\\mathbb{E}[\\tilde{Y}_i(0) \\vert D_i=1]}\\Big) + \\Big(\\textcolor{blue}{\\mathbb{E}[\\tilde{Y}_i(0) \\vert D_i=1]} - \\mathbb{E}[\\tilde{Y}_i(0) \\vert D_i = 1]\\Big) \\end{align*}\\] <p>We know have two bracketed expressions. The first captures the the average treatment effect on the treated group. The second captures the average difference between the treated and control groups in a counterfacutal world where no one is treated. We refer to this second expression as the Selection Bias.</p> <p>We've shown that the difference-in-means is equivalent to the average treatment on the treated plus the selection bias. </p> Consider <p>What is the average treatment effect on the treated and selection bias in a randomized control trial?</p>"},{"location":"chapters/fundamentals/potential%20outcomes/","title":"Potential Outcomes","text":"<p>The causal effect of a treatment is the difference in some outcome when a person receives the treatment and when a person does not receive the treatment. Re-reading the line above, hopefully it jumps out to you that we cannot in fact observe a person in both of the states of the world. For instance, if we are interested in understanding the effects of a college education on annual earnings at the age of 40, we observe each person either with a college education or without one. This highlights that the funamental issue of a causal inference is a missing data problem. </p> <p>While we cannot ever know the causal effects at the individual level, its nevertheless important to be able to define them. To do so, we introduce the potential outcome function which maps levels of the treatment variable into some outcome set. Continuing with our example of college education, we would define the potential outcome function as follows. </p> \\[\\tilde{Y}_i : \\{\\textrm{College Education}, \\textrm{No College Education}\\} \\to \\mathcal{R}\\] <p>We can define the individual level treatment effect then as the difference, where we have replaced the college/no college status with an binary variable (i.e. a variable which takes the values either 1 or 0). </p> \\[\\tilde{Y}_i(1) -\\tilde{Y}_i(0)\\] <p>With this set-up, we can then define the average treatment effect as the expectation of this difference over the entire population of interest. In many contexts, we will be interested in estimating this term. In our running example, this would be the return on education averaged over the entire population of interest.</p> \\[\\textrm{ATE} = \\mathbb{E}[\\tilde{Y}_i(1) -\\tilde{Y}_i(0)]\\]"},{"location":"chapters/fundamentals/residualized%20regression/","title":"Residualized Regression","text":""},{"location":"chapters/fundamentals/residualized%20regression/#instrumental-variables","title":"Instrumental Variables","text":"<p>Linear instrumental variables consists of a the following two step process. We first regress the treatment variable \\(D_i\\) on the controls \\(X_i\\) and the instrument \\(Z_i\\). We then regress the outcome variable \\(Y_i\\) on the predicted treatment \\(\\hat{D}_i\\) and the controls.</p> \\[\\begin{align*} D_i &amp;= \\gamma_1 X_i + \\gamma_2 Z_i + v_i \\\\ Y_i &amp;= \\beta_1 \\hat{D}_i + \\beta_2 X_i + \\varepsilon_i \\end{align*}\\] <p>As emphasized in class, I prefer to interpret the coefficients in a linear model via a residualized approach. For one, it makes the source of the variation clear. We can interpret the coefficient \\(\\beta_1\\) in the second equation above via a residualized regression as follows where \\(\\bar{\\hat{D}}\\) is the predicted predicted values! Not a typo. We first predict treatment given the controls and the instrument. We then predict this predicted value given only the controls. Via the law of iterated expectations it is equivalent to the predicted treatment given the controls.</p> \\[\\begin{align*} Y_i &amp;= \\beta_1 (\\hat{D}_i - \\bar{\\hat{D}}_i) + u_i \\\\  \\end{align*}\\] <p>As I also emphasize in class, I tent to think of linear models as approximations to the underlying conditional expectation function. Therefore, we can re-write the above regression in its nonparametric form as follows:</p> \\[\\begin{align*}  Y_i &amp;= \\beta_1 (\\mathbb{E}[D_i \\vert X_i, Z_i] - \\mathbb{E}[D_i \\vert X_i] ) + u_i \\\\  \\end{align*}\\] <p>When I see a linear IV model in a paper, I try to interpret is as an approximation to the above regression. </p>"},{"location":"chapters/fundamentals/selection%20on%20observables/","title":"Selection on Observables","text":""},{"location":"chapters/fundamentals/selection%20on%20observables/#do-we-need-controls","title":"Do We Need Controls?","text":"<p>The question that has been lurking in the back of your head up to this point is whether it would be appropriate to use a difference-in-means estimator when the selection on observable assumption holds. That is, if we can think of our data as coming from a \"local\" randomized control trial, does the difference in means estimator recover the average treatment effect?</p> <p>Let's think through this. Applying the Law of Total Probability, we can express the first term in the estimator as follows:</p> \\[\\begin{align*}\\mathbb{E}[Y_i \\vert D_i=1] &amp;= \\sum \\mathbb{E}[Y_i \\vert D_i=1, X_i=x] p_{X\\vert D=1}(x) \\\\ &amp;= \\sum \\mathbb{E}[\\tilde{Y}_i(1) \\vert D_i=1, X_i=x] p_{X\\vert D=1}(x) \\\\ &amp;= \\sum \\mathbb{E}[\\tilde{Y}_i(1) \\vert X_i=x] p_{X\\vert D=1}(x)\\end{align*}\\] <p>The immediate issue that jumps out to us is that the conditional distribution \\(p_{X \\vert D=1}(x)\\) may not equal the unconditional distribution, \\(p_X(x)\\), in which case our estimator would be biased. Another way to frame this problem is as follows: We observe a random variable following a distribution \\(q(x)\\), but we are interested in taking the expected value with respect to a different distribution \\(v(x)\\). That is:</p> \\[\\begin{align*} \\textrm{Observe} :=  \\sum f(x) q(x)\\\\ \\textrm{Estimand} :=  \\sum f(x) v(x)\\\\ \\end{align*}\\] <p>One approach in this context is to transform the random variable via a correction term. That is, we introduce a new random variable which is a scaled version of our original random variable \\(h(x)= f(x)\\frac{v(x)}{q(x)}\\), and take the average of this random variable with respect to \\(q(x)\\).</p> \\[\\begin{align*} \\textrm{Observe} :=  \\sum h(x) q(x) = f(x)\\frac{v(x)}{q(x)} q(x) =  \\sum f(x) v(x) = \\textrm{Estimand}  \\end{align*}\\] <p>In our context, \\(q(x) = p_{X \\vert D=1}(x)\\) and \\(v(x) = p(x)\\). Therefore our \"correction term\" is the ratio of these two values: the unconditional probability of treatment and the propensity score.</p> \\[\\begin{align*} \\textrm{Correction Term} &amp;:= \\frac{p(x)}{ p_{X \\vert D=1}(x)} \\\\ &amp;=   p_X(x)   \\div \\frac{ p_{X,D}(x, 1)}{p_D(1)} \\\\  &amp;= \\frac{p_D(1) p_X(x)}{ p_{X,D}(x, 1)}   \\\\  &amp;= \\frac{p_D(1)}{p_{D \\vert X=x}(1)} \\end{align*}\\] <p>Let's now check that if we use this correction term, the difference-in-means estimator will return the average treatment effect. To do so, let \\(W_i = \\frac{Y_i p_D(1)}{p_{D\\vert X_i}(1)}\\)</p> \\[\\begin{align*} \\mathbb{E}[W_i \\vert D_i=1] &amp;= \\sum \\mathbb{E}[W_i \\vert D_i=1, X_i=x] p_{X\\vert D=1}(x) \\\\ &amp;= \\sum \\mathbb{E}[\\frac{Y_i p_D(1)}{p_{D\\vert X_i=x}(1)} \\vert D_i=1, X_i=x] p_{X\\vert D=1}(x) \\\\ &amp;= p_D(1)\\sum \\mathbb{E}[Y_i \\vert D_i=1, X_i=x]\\frac{p_{X\\vert D=1}(x)}{p_{D\\vert X_i=x}(1)}\\\\ &amp;= p_D(1)\\sum \\mathbb{E}[\\tilde{Y}_i(1) \\vert D_i=1, X_i=x]\\frac{p_{X\\vert D=1}(x)}{p_{D\\vert X_i=x}(1)}\\\\ &amp;= p_D(1)\\sum \\mathbb{E}[\\tilde{Y}_i(1) \\vert X_i=x]\\frac{p_{X\\vert D=1}(x)}{p_{D\\vert X_i=x}(1)} \\\\ &amp;= p_D(1)\\sum \\mathbb{E}[\\tilde{Y}_i(1) \\vert X_i=x] \\frac{\\frac{p_{X,D}(x,1)}{p_{D}(1)}}{\\frac{p_{X,D}(x,1)}{p_{X}(x)}} \\\\ &amp;= p_D(1)\\sum \\mathbb{E}[\\tilde{Y}_i(1) \\vert X_i=x] \\frac{p_X(x)}{p_D(1)} \\\\ &amp;= \\sum \\mathbb{E}[\\tilde{Y}_i(1) \\vert X_i=x] p_X(x) \\\\  &amp;= \\mathbb{E}[\\tilde{Y}_i(1)] \\end{align*}\\]"},{"location":"chapters/fundamentals/selection%20on%20observables/#can-linear-regression-be-wrong-in-this-context","title":"Can Linear Regression be Wrong in this Context?","text":"<p>The above section highlights that even if we can think of our data as generated by local randomized control trails, we need to account for the relative distribution over the covariates. The most immediate question that follows is: does linear regression adjust the relative distribution?</p>"},{"location":"chapters/optimization/differentiation/","title":"Differentiation","text":"<p>\"If you have a tricky problem to solve, finding a language in which the solution to that problem is compositional is like the most important step. Well, defining the problem clearly is the most important step. The second one is finding a vocabulary in which it's compositional.\"<sup>1</sup></p> <p>This entire section is taken from Conal Elliot's presentation titled: \"Automatic Differentiation Made Easy Via Category Theory\"</p> <ul> <li> <p>A Derivative is a linear map </p> </li> <li> <p>Cateogy Theory is the abstract algebra of functions</p> </li> <li> <p>Let \\(a,b\\) be Banach spaces (complete normed vector spaces)</p> </li> </ul> \\[\\mathcal{D} :: (a \\to b) \\to (a \\to (a -\\circ b))\\] <ul> <li>Terminology: The derivative of \\(f\\) at \\(a\\)</li> </ul> \\[\\underset{\\varepsilon \\to 0}{\\lim} \\frac{ \\| f \\ (a + \\varepsilon) - f \\ a + \\mathcal{D} \\ f \\ a \\ \\varepsilon \\|}{\\| \\varepsilon \\|} = 0\\] <p>Differentiation preserves parallel composition </p> \\[\\begin{align*}&amp;(\\triangle) :: (a \\to b) \\to (a \\to d) \\to (a \\to b \\times d) \\\\  &amp;(f \\ \\triangle \\ g) \\ a = (f \\ a, g \\ a)  \\\\  &amp;\\mathcal{D} \\ (f \\ \\triangle \\ g)  = \\mathcal{D} \\ f \\ \\triangle \\ \\mathcal{D} \\ g \\end{align*}\\] \\[\\begin{align*} &amp;\\hat{\\mathcal{D}} :: (a \\to b) \\to (a \\to (b \\times (a \\to b))) \\\\  &amp; \\hat{\\mathcal{D}} \\ f = f \\ \\triangle \\ \\mathcal{D} \\ f\\end{align*}\\] <ol> <li> <p>Reference: See here \u21a9</p> </li> </ol>"},{"location":"chapters/optimization/overparam/","title":"Overparameterization","text":""},{"location":"chapters/optimization/overparam/#belkins-claim","title":"Belkin's Claim:","text":"<p>(1) If we have a solution manifold and (2) if this manifold has curvature then the loss function is not locally convex. </p> <p>Proof</p> <p>Assume we have a solution manifold. </p> \\[f(y) \\geq f(x) + \\nabla f(x)^T(y-x)\\] Lemma <p>Minimizers of Convex Function form a Convex Set</p> <ul> <li>Let \\(x_1, x_2\\) be minimizers of a convex function \\(f\\). i.e. \\(\\forall x \\ f(x) \\geq x_1, x_2\\)</li> <li>Then \\(\\forall \\alpha \\in (0,1), \\alpha x_1 + (1-\\alpha x_2)\\) is also a minimizer of \\(f\\).</li> </ul>"},{"location":"chapters/optimization/papers/","title":"Papers","text":"<ul> <li>The loss landscape of overparameterized neural networks</li> </ul>"},{"location":"chapters/other/LLM/introduction/","title":"Introduction","text":"References <ul> <li>A Neural Probabilistic Language Model</li> </ul>"},{"location":"chapters/other/LLM/introduction/#aim","title":"Aim","text":"<p>The aim is to \"learn\" \\(\\mathbb{P}\\). </p> \\[\\Big(\\{\\textrm{Sequences of Words}\\}, \\mathcal{F}, \\mathbb{P} \\Big)\\]"},{"location":"chapters/other/LLM/introduction/#high-level-approach","title":"High Level Approach","text":"<p>We can achieve this objective as follows: </p> <ul> <li>Learn a distribution over the first word of a sequence. </li> </ul> \\[\\big(\\mathcal{V}, \\mathcal{F}, \\mathbb{P} \\big)\\] <ul> <li>Learn the conditional distribution </li> </ul> \\[\\textrm{Model} :: \\textrm{Params} \\to \\{\\textrm{context}\\} \\to \\mathbb{P}_{\\mid \\textrm{context}}\\] To Do <p>This should be made more exact</p>"},{"location":"chapters/other/LLM/introduction/#a-neural-probabilistic-language-model","title":"A Neural Probabilistic Language Model","text":"<p>Essence</p> <ul> <li>transfer probability mass</li> <li>In the proposed model, it will so generalize because \u201csimilar\u201d words are expected to have a similar feature vector, and because the probability function is a smooth function of these feature values, a small change in the features will induce a small change in the probability</li> </ul> <p>We can construct the condition distribution as follows. </p> <ul> <li>First we introduce an embedding function. Given that our vocab is finite, we can represent this function as a matrix. i.e. \\(h\\) is isomorphic to \\(\\theta \\in {| \\mathcal{V} | \\times m}\\)</li> </ul> \\[h :: \\mathcal{V} \\to \\mathcal{R}^p\\] <ul> <li> <p>We introduce a function \\(g\\) which maps subsequences of these embeddings into a conditional distribution</p> </li> <li> <p>Given this level of detail we could augment the signature of our model as follows:</p> </li> </ul> \\[\\textrm{Model} :: \\textrm{Embedding Functions} \\to \\textrm{Forward Functions} \\to \\{\\textrm{context}\\} \\to \\mathbb{P}_{\\mid \\textrm{context}}\\] To Do <p>What should the name of this forward function be?</p>"},{"location":"chapters/other/LLM/introduction/#key-insights","title":"Key Insights","text":"<ul> <li>\"In high dimensions, it is crucial to distribute probability mass where it matters rather than uniformly in all directions around each training point.\"<sup>1</sup></li> </ul> <ol> <li> <p>A Neural Probabilistic Language Model \u21a9</p> </li> </ol>"},{"location":"chapters/other/LLM/rhlf/","title":"RHLF","text":""},{"location":"chapters/other/LLM/rhlf/#reinforcement-learning-overview","title":"Reinforcement Learning Overview","text":"<p>David Silver in his introductory class on RL makes the following distinctions between reinforcement learning and supervised learning:</p> <ol> <li>\"There is no supervisor, only a reward signal\". I am not a fan of this distinction because in supervised learning we don't have a supervisor. Generally, we're not trying to perfectly interpolate the data. Rather, we have a reward signal of the loss function \\((y - f_{\\theta}(x))^2\\)</li> <li>\"Feedback is delayed, not instantaneous\"</li> <li>\"Time really matters (sequential, non i.i.d data)\" and \"Agent's actions affect the subsequent data it receives\"</li> </ol>"},{"location":"chapters/other/LLM/rhlf/#the-rl-problem","title":"The RL Problem","text":"<p>Let \\(\\Theta\\) be a parameter space and \\(\\theta \\in \\Theta\\) a parameter vector that governs the dynamics of a stochastic process:  \\(\\big(\\Omega, \\mathcal{F}, \\mathbb{P}_{\\theta}\\big)\\). We are interested in identifying the parameter \\(\\theta^*\\) that maximizes the expected cumulative reward over time. Formally, the optimization problem can be stated as:</p> <p>We can define the following two random variables (Trajectories, and Rewards of Trajectories)</p> \\[\\begin{align*} \\tau : \\Omega \\to \\textrm{Path Space} \\\\ R : \\textrm{Path Space}  \\to \\mathcal{R} \\end{align*}\\] <p>Then the expected reward is denoted by:</p> \\[\\begin{align*} \\int _{\\Omega} R \\circ \\tau \\mathbb{P}_{\\theta} \\end{align*}\\]"},{"location":"chapters/other/LLM/rhlf/#policy-search","title":"Policy Search","text":"<p>The idea behind policy search is that we are going to approximate  the above value by constructing a new probability space which is just the `n' product of the original probability space:</p> \\[\\begin{align*} \\big( \\Omega_n, \\mathcal{F}_n, \\mathbb{P}_{\\theta, n}) \\end{align*}\\] <p>On this probability space, we can define our estimator:</p> \\[\\begin{align*} \\hat{V} &amp;: \\Omega_n \\to \\mathcal{R} \\\\ \\omega_n &amp;\\longmapsto \\sum _i R \\circ \\tau_i \\end{align*}\\]"},{"location":"chapters/other/LLM/rhlf/#rhlf","title":"RHLF","text":"<ul> <li>the challenge with this set-up is the effective domain of the reward model<ul> <li>We use the KL constraint to try and account for this issue</li> </ul> </li> </ul>"},{"location":"chapters/probability%20theory/clusters/","title":"Clusters","text":""},{"location":"chapters/probability%20theory/clusters/#propensity-score","title":"Propensity Score","text":"<p>The propensity score is observable: </p> \\[\\pi(X_i) = \\mathbb{P}(D_i = 1 \\mid X_i)\\] <p>That is, given access to the following probability space, we can compute the probability measure. </p> \\[\\big( \\mathcal{X}\\times \\mathcal{D} \\times \\mathcal{Y}, \\mathcal{F}, \\mathcal{P} \\big)\\] To Do <p>Add in the details </p>"},{"location":"chapters/probability%20theory/conditional_expectation/","title":"Conditional Expectation","text":"<p>Although the CEF is random variable, it's not \"defined\" pointwise. </p> \\[\\begin{align*} &amp;\\mathbb{E}[Y|X] :: \\Omega \\to \\mathcal{R} \\\\  &amp; \\mathcal{I} \\ \\mathbb{P} \\ A \\ Y = \\mathcal{I} \\ \\mathbb{P} \\ A \\  \\mathbb{E}[Y|X] \\quad \\forall \\ A \\in \\sigma(X) \\end{align*}\\] <p>The law of iteration of expectations is part of the definition</p> \\[\\begin{align*} &amp; \\mathcal{I} \\ \\mathbb{P} \\ \\Omega \\ Y = \\mathcal{I} \\ \\mathbb{P} \\ \\Omega \\  \\mathbb{E}[Y|X] \\end{align*}\\]"},{"location":"chapters/probability%20theory/conditioning/","title":"Conditioning","text":""},{"location":"chapters/probability%20theory/conditioning/#conditioning","title":"Conditioning","text":"<p>Given a measurable space, \\((\\Omega, \\mathcal{F})\\), let \\(\\mathcal{M}\\) be the set of probability measures defined on this space. Then conditioning can be defined as follows:</p> \\[\\begin{align*} &amp;C :: \\mathcal{M} \\to \\mathcal{F}_+ \\to \\mathcal{M} \\\\ &amp; C \\ \\mathbb{P} \\ A  \\ B = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(A)} \\end{align*}\\] <p>Two things to note here: </p> <ol> <li>Notice the dependent structure between \\(\\mathcal{M}\\) and \\(\\mathcal{F}_+\\). The measure restricts the set of events that we can condition on. </li> <li>Notice also that if we rearrange the signature of the function, as done below, \\(C \\  \\Omega\\) would be the identity function.</li> </ol> \\[C ::\\mathcal{F}_+ \\to \\mathcal{M} \\to \\mathcal{M}\\]"},{"location":"chapters/probability%20theory/convergence/","title":"Convergence","text":"References <ul> <li>STATS 203 - Large Sample Theory </li> <li>Master Program: Probability Theory - Lecture 3: Applications of independence</li> </ul>"},{"location":"chapters/probability%20theory/convergence/#introduction","title":"Introduction","text":"TLDR <p>The defining feature of convergence is the restriction on the underlying probability space. </p> <ul> <li>In convergence in Law/distribution, we do not require \\(X_n\\) and \\(X\\) to be defined on the same probability space. </li> <li>In convergence in probability or expectation, we require that for each \\(n\\),  \\(X_n\\) and \\(X\\) are defined on the same probability space, but this probability space is allowed to change with \\(n\\). </li> <li>In convergence almost surley, the underlying probability space for \\(X_n\\) and \\(X\\) must be the same and fixed for all \\(n\\). </li> </ul>"},{"location":"chapters/probability%20theory/convergence/#weak-law-of-large-numbers","title":"Weak Law of Large Numbers","text":"Presentation <p>It seems like there are at least two distinct ways to illustrate this result. The first places more emphasis on random variables, the second, which I do below, places more emphasis on empirical measures. The relationship between these two approaches is the following: </p> \\[\\mathcal{I} \\ (x \\mapsto x) \\ \\hat{\\mathbb{P}}_n \\equiv \\frac{X_1 + X_2 + \\dots + X_n}{n}\\] <p>We begin by defining a probability space parameterized by \\(n\\). </p> \\[\\Big(\\mathcal{R}^n,  \\mathcal{B}(\\mathcal{R}^n), \\mathbb{P}^n\\Big)\\] <p>Projection Random Variables</p> <p>Defined as follows:</p> \\[\\omega \\in \\Omega_n \\longmapsto X_i(\\omega) = \\omega[i] \\] <p>with \\(n\\) i.i.d projection random variables.</p> \\[ \\begin{align*} \\hat{\\mathbb{P}}_n &amp;:: \\mathcal{R}^n \\to \\mathcal{B}(\\mathcal{R}) \\to [0,1] \\\\  \\mathcal{I}_n &amp;:: (\\mathcal{R}^n \\to \\mathcal{R}) \\to (\\mathcal{B}(\\mathcal{R}^n) \\to [0,1]) \\to \\mathcal{R} \\\\ \\mathcal{I}_1 \\ (x \\mapsto x) \\ \\hat{\\mathbb{P}}_n &amp;:: \\mathcal{R}^n \\to [0,1]  \\end{align*}\\] <p>From this, we can define the following sequence of random variables: </p> \\[\\begin{align*} d_n  &amp;:: \\mathcal{R}^n \\to [0,1] \\\\ &amp; := \\mathcal{I}_1 \\ (x \\mapsto x) \\ \\hat{\\mathbb{P}}_n - \\mathcal{I}_1 \\ (x \\mapsto x) \\ \\mathbb{P} \\end{align*}\\] <p>We can show that the following result.</p> \\[\\underset{n \\to \\infty}{\\lim} \\mathcal{I}_n \\ d_n \\ \\mathbb{P}_n = 0 \\] <p>Result</p> <p>We say</p> \\[\\mathcal{I}_1 \\ (x \\mapsto x) \\ \\hat{\\mathbb{P}}_n \\ \\textrm{converges in} \\ L_2 \\ \\textrm{to} \\ \\mathcal{I}_1 \\ (x \\mapsto x) \\ \\mathbb{P}\\]"},{"location":"chapters/probability%20theory/convergence/#convergence-in-distribution-or-law","title":"Convergence in Distribution (or Law)","text":"<ul> <li>A random variable \\(X_n\\) convergences in distribution (or in law )to \\(X\\) when the corresponding sequence of CDFs converge \"pointwise\" to the CDF of \\(X\\)</li> </ul> \\[\\begin{align} X_n \\to_D X \\implies F_{X_n} \\to F_X  \\end{align}\\] Helly-Bray Theorem TLDR <p>If the parameters of our model converge in distribution or law, then we have an asymptotically unbiased estimate.</p> <p>Let \\(\\theta_n \\to_D \\theta\\). Let \\(g\\) be a continuous and bounded function. Then: </p> \\[I \\ \\Omega_1 \\ \\mathcal{P}_n \\ (g \\circ \\theta_n) \\to I \\ \\Omega_2 \\ \\mathcal{P} \\ (g \\circ \\theta)\\] <p>Note, if we define our estimator \\(\\hat{\\beta}_n\\) as follows. </p> \\[\\hat{\\beta}_n := g \\circ \\theta_n = \\int f(\\theta_n, X) d\\mathbb{P}_X\\] <p>Then this theorem tells us that </p> \\[\\int \\hat{\\beta}_n d\\mathbb{P}_n \\to \\beta\\]"},{"location":"chapters/probability%20theory/convergence/#convergence-in-probability","title":"Convergence in Probability","text":"<p>If the limit is a random variable, then we care aboout the joint distribution!</p> <ul> <li>A random variable \\(X_n\\) converges in probability to \\(X\\) when for all \\(\\varepsilon &gt; 0\\) the following holds:</li> </ul> \\[\\begin{align} \\underset{n \\to \\infty}{\\lim}\\mathbb{P}(\\|X_n - X \\| &gt; \\varepsilon)= 0 \\end{align}\\]"},{"location":"chapters/probability%20theory/convergence/#applications","title":"Applications","text":""},{"location":"chapters/probability%20theory/convergence/#weak-law-of-large-numbers_1","title":"Weak Law of Large Numbers","text":"<p>Proof given here</p> <p>Let \\(X_1, \\dots, X_n\\) be a family of i.i.d random variables with a finite second moment. Then </p> \\[\\underset{n \\to \\infty}{\\lim} \\mathbb{E}\\Big[\\Big( \\frac{X_1 + \\dots + X_n}{n} - \\mathbb{E}[X]\\Big)^2\\Big] = 0\\]"},{"location":"chapters/probability%20theory/expectations/","title":"Expectation","text":""},{"location":"chapters/probability%20theory/expectations/#introduction","title":"Introduction","text":"<p>Integration can be thought of as a function that takes a probability measure, a set, and a random variable </p> \\[\\begin{align*} &amp;\\mathcal{I} ::  \\mathcal{F} \\to \\mathcal{M}  \\to \\{\\Omega \\to \\mathcal{R} \\} \\to \\mathcal{R} \\cup \\{   \\pm \\infty\\}\\\\  &amp;\\mathcal{I}  \\ A \\  \\mathbb{P}  \\ X = \\int _A X d\\mathbb{P} \\end{align*}\\]"},{"location":"chapters/probability%20theory/expectations/#lebesgue-integral","title":"Lebesgue Integral","text":"<p>The lebesgue intergral is simply the following where \\(\\lambda\\) denotes the lebesgue measure</p> \\[\\mathcal{I}_{\\lambda}\\]"},{"location":"chapters/probability%20theory/expectations/#linearity","title":"Linearity","text":"<p>You have probably heard in various classes that integration is a linear function. What people mean by this is that the following higher-order function is linear </p> \\[\\mathcal{I} \\ \\mathcal{P} \\ A \\]"},{"location":"chapters/probability%20theory/expectations/#working-across-probability-spaces","title":"Working Across Probability Spaces","text":"\\[\\int _A fd\\mathbb{P} = \\int_{f(A)} x d\\mathbb{P}_f\\] To Do <p>is \\(f(A)\\) a measurable set??</p>"},{"location":"chapters/probability%20theory/independence/","title":"Independence","text":""},{"location":"chapters/probability%20theory/independence/#events","title":"Events","text":"<p>Given a probability space</p> \\[\\big(\\Omega, \\mathcal{F}, \\mathbb{P}\\big)\\] <ul> <li>Two Events: \\(A, B\\) are independent under \\(\\mathbb{P}\\) if </li> </ul> \\[\\mathbb{P}( A \\cap B) = \\mathbb{P}(A) \\mathbb{P}(B)\\] <ul> <li>Finite Collection of Events: \\(A_1, A_2, \\dots, A_n\\) are independent if </li> </ul> \\[\\forall I_0 \\subset \\{1,2, \\dots, n\\}, \\quad \\mathbb{P}\\big(\\cap _{i \\in I_0} A_i \\big) = \\prod _{i \\in I_0}  \\mathbb{P}(A_i)\\] <ul> <li> <p>Arbitrary Collection of Events: \\(\\{A_i, i \\in I\\}\\)</p> <ul> <li>Independent if for any finite subset, the independent condition defined above holds</li> </ul> </li> </ul>"},{"location":"chapters/probability%20theory/independence/#sub-sigma-algebras","title":"Sub-\\(\\sigma\\)-algebras","text":"<ul> <li>Two sub-\\(\\sigma\\)-algebras: \\(\\mathcal{F}_1, \\mathcal{F}_2\\) </li> </ul> \\[\\forall A_1 \\in \\mathcal{F}_1, \\forall A_2 \\in \\mathcal{F}_2, \\quad \\mathbb{P}( A_1 \\cap A_2) = \\mathbb{P}(A_1) \\mathbb{P}(A_2)\\] <ul> <li>Arbitrary Collection of Sub-\\(\\sigma\\)-algebras: \\(\\{\\mathcal{F}_i, i \\in I\\}\\)</li> </ul> <p>Are independent if for any subset of \\(\\Omega\\) selected from any sub-\\(\\sigma\\)-algebras, that collection of subsets is indepdent as defined above</p>"},{"location":"chapters/probability%20theory/independence/#random-variables","title":"Random Variables","text":"<p>\\(X, Y\\) are independent if the \\(\\sigma\\)-algebras generated by these random variables are independent as defined above</p> <ul> <li>\\(\\sigma\\)-algebra generated by the random variable \\(X\\)</li> </ul> \\[\\sigma(X) := \\{A \\in \\mathcal{F} \\mid A = X^{-1}(B) \\  \\textrm{for} \\ B \\in \\mathcal{B}(\\mathcal{R})\\}\\]"},{"location":"chapters/probability%20theory/inference/","title":"Inference","text":""},{"location":"chapters/probability%20theory/inference/#the-inference-ladder","title":"The Inference Ladder","text":"<p>Our estimator can be defined via the following components:</p> \\[\\begin{align*}\\theta_n &amp;:: \\Omega_n \\to \\mathcal{R}^{d(n)} \\\\ \\\\  f_n &amp;:: \\mathcal{R}^{d(n)}  \\to \\mathcal{X} \\to \\mathcal{R} \\\\ \\\\  \\gamma _n &amp;:: (\\mathcal{X} \\to \\mathcal{R}) \\to \\Omega_n \\to \\mathcal{R} \\end{align*}\\] <p>Our estimator is constructed by composing these elements as follows: </p> \\[\\begin{align*}\\theta_n &amp;:: \\Omega_n \\to \\mathcal{R}^{d(n)} \\\\ \\\\  f_n \\circ \\theta_n &amp;:: \\Omega_n \\to \\mathcal{X} \\to \\mathcal{R} \\\\ \\\\  \\gamma _n \\circ f_n \\circ \\theta_n &amp;:: \\Omega_n \\to \\Omega_n \\to \\mathcal{R} \\end{align*}\\] Random Function(als) <p>A random variable has the following type signature: </p> \\[Z :: \\Omega \\to \\mathcal{R}\\] <p>A random function as the following type signature: </p> \\[Z :: \\Omega \\to \\mathcal{X} \\to \\mathcal{R}\\] <p>A random funtional as the following type signature: </p> \\[Z :: \\Omega \\to (\\mathcal{X} \\to \\mathcal{R}) \\to \\mathcal{R}\\] <p>From this, we observe that \\(f_n \\circ \\theta_n\\) is a random function and that \\(\\gamma _n\\) is a random functional</p> Other \\[\\begin{align*}  \\hat{g} :: \\Omega_n \\to \\mathcal{X} \\to \\mathcal{R}\\end{align*}\\]"},{"location":"chapters/probability%20theory/inference/#partial-convergence","title":"Partial Convergence","text":""},{"location":"chapters/probability%20theory/introduction/","title":"Introduction","text":"<p>In many applied contexts we are interested in the behavior of the estimator and the interpretation of the estimate. Our starting point for this discussion is a probability space. Well, actually several of them, but they can be understood as transformations of the original one, so let's start there. </p> <ul> <li> <p>Unobserved</p> \\[\\Big( \\mathcal{Y} \\times \\mathcal{Y} \\times \\mathcal{X} \\times \\mathcal{D}, \\mathcal{F}, \\mathbb{P}_0\\Big)\\] </li> <li> <p>Observed</p> \\[\\Big( \\mathcal{Y} \\times  \\mathcal{X} \\times \\mathcal{D}, \\mathcal{F} , \\mathbb{P}\\Big)\\] </li> <li> <p>\\(Sample\\)</p> \\[\\begin{align*} &amp;D_{i}:: \\Omega \\to \\mathcal{R} \\\\  &amp;D_{i} \\ (\\_ \\ \\_ \\ d) =  d \\end{align*}\\] </li> <li> <p>\\(Y_i\\)</p> \\[\\begin{align*} &amp;Y_{i}:: \\Omega \\to \\mathcal{R} \\\\  &amp;Y_{i} \\ (y_0 \\ y_1 \\ d) =  dy_1 + (1-d)y_0 \\end{align*}\\] </li> </ul> <p>Given this probability space, we can then define the random variables of interest as follows: </p> <ul> <li> <p>\\(Y_{i0}\\)</p> \\[\\begin{align*} &amp;Y_{i0}:: \\Omega \\to \\mathcal{R} \\\\  &amp;Y_{i0} \\ (y_0 \\ \\_ \\ \\_) =  y_0  \\end{align*}\\] </li> <li> <p>\\(Y_{i1}\\)</p> \\[\\begin{align*} &amp;Y_{i1}:: \\Omega \\to \\mathcal{R} \\\\  &amp;Y_{i1} \\ (\\_ \\ y_1 \\ \\_) =  y_1 \\end{align*}\\] </li> <li> <p>\\(D_i\\)</p> \\[\\begin{align*} &amp;D_{i}:: \\Omega \\to \\mathcal{R} \\\\  &amp;D_{i} \\ (\\_ \\ \\_ \\ d) =  d \\end{align*}\\] </li> <li> <p>\\(Y_i\\)</p> \\[\\begin{align*} &amp;Y_{i}:: \\Omega \\to \\mathcal{R} \\\\  &amp;Y_{i} \\ (y_0 \\ y_1 \\ d) =  dy_1 + (1-d)y_0 \\end{align*}\\] </li> </ul> <p>We can say that treatment is indepdent of the potential outcome if the corresponding \\(\\sigma\\)-algebras are independent. More intuitively, this is equivalent (as shown here).</p> \\[\\forall B_1,B_2 \\in \\mathcal{B}(\\mathcal{R}), \\quad \\mathbb{P}(D_i \\in B_1, Y_{i0} \\in B_2) = \\mathbb{P}(D_i \\in B_1)\\mathbb{P}(Y_{i0} \\in B_2)\\]"},{"location":"chapters/probability%20theory/introduction/#working-across-probability-spaces","title":"Working Across Probability Spaces","text":""},{"location":"chapters/probability%20theory/introduction/#independence","title":"Independence","text":"<p>you will often here that treatment is independent of the potential outcomes. While you probably have an intuitive sense of what this means, it can be helpful to formally define this. To start, let's consider the following probability spaces: </p>"},{"location":"chapters/probability%20theory/introduction/#expectations","title":"Expectations","text":"<p>Many terms/properties can be understood as working across multiple probability spaces.</p> \\[\\int _A fd\\mathbb{P} = \\int_{f(A)} x d\\mathbb{P}_f\\] To Do <p>is \\(f(A)\\) a measurable set??</p>"},{"location":"chapters/probability%20theory/key_terms/","title":"Key Terms","text":"<p>Conditioning &amp; Functors</p>"},{"location":"chapters/probability%20theory/key_terms/#sigma-algebra-of-a-random-variable","title":"\\(\\sigma\\)-algebra of a Random Variable","text":"<p>Information. What information about the sample space, \\(\\Omega\\), does a random variable, \\(X\\), convey. </p> <p>For any element of the Borel \\(\\sigma\\)-algebra we can tell whether that element occured. </p> \\[X(\\omega) \\in B, \\quad \\textrm{or} \\quad  X(\\omega) \\notin B\\] <p>Therefore, we know </p> \\[ \\omega \\in X^{-1}(B), \\quad \\textrm{or} \\quad  \\omega \\notin X^{-1}(B)\\] <p>We refer to this set of events as the \\(\\sigma\\)-algebra generated by \\(X\\). Importantly, it is the set of events that we can tell whether or not they occured if we have \\(X\\).</p>"},{"location":"chapters/probability%20theory/key_terms/#conditioning-on-random-variables","title":"Conditioning on Random Variables","text":"<ul> <li>As we have explained else where, conditioning on an event can be understood as mapping a probility measure into another probability measure. </li> </ul> \\[\\mathcal{C}:: \\mathcal{M} \\to \\mathcal{F}_+ \\to \\mathcal{M}\\] <ul> <li>Events are isomphic to indicator random variables, where the \\(\\sigma\\)-algebra of the indicator random variable for the event \\(A\\) is the following</li> </ul> \\[\\{\\emptyset, \\Omega, A, A^c \\}\\] <ul> <li>Conditioning on a random variable can then be understood as \"lifting\" the above function!</li> </ul> \\[\\tilde{C} :: \\mathcal{M} \\to (\\Omega \\to \\mathcal{R}) \\to \\mathcal{F}_{\\sigma(X)} \\to \\mathcal{M}\\] <p>The question is, what do we do with this?!</p>"},{"location":"chapters/probability%20theory/key_theorems/","title":"Key Theorems","text":"Gilvenko-Cantelli <p>The claim is that \\(\\underset{x}{\\textrm{sup}}\\ g\\) (defined below!) converges \"almost surely\" to \\(0\\).</p> \\[\\begin{align*} &amp;g :: \\mathcal{X} \\to \\Omega \\to \\mathcal{R} \\\\ &amp;g \\ x \\ \\omega = F_n(x, \\omega) - F(x) \\\\ \\\\  &amp;\\underset{x}{\\textrm{sup}}\\ g :: \\Omega \\to \\mathcal{R} \\end{align*}\\]"},{"location":"chapters/programming/category%20theory/natural%20transformations/","title":"Natural Transformations","text":"\\[\\textrm{Nautral Transformations} :: \\{\\textrm{Functiioins}\\}\\]"}]}