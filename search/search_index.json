{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Econometrics Introduction Applied econometrics is fundamentally about interpretation -- how to interpret the result of a statistical procedure in a given context. Often, there isn't often a \"valid\" or \"correct\" interpretation. Much like a work of art, there are multiple interpretions available to statistical results. In line with this thinking, we're not going to emphasize a particular way to interpret results. Rather, we would like to introduce you to the statistical and computional tools as well as provide you with a broad set of real life examples so that you can further develop your own judgement . Learning from Data Learning from data, especially finite data where the estimand of interest has a causal interpretation is a highly personalized process. The general setup of the process is reflected visually below. We have beliefs about the world; we have beliefs about how a statical procedure behaves under certain conditions; the statistical procedure is applied to a data set providing use with numerical \"results\" which ultimately shape our new beliefs about the world. Models We will never assume that the relationship is linear and moreover, we're not going to be centrally focused on fitting linear models Under selection on observables, linear regression can perform poorly: Reproduced Here We'll work from a more general setup \\[Y_i(D) = f(X_i, D) + \\varepsilon_i(D)\\] Components Approximation TBD Getting started Optimization TBD Getting started Probability Theory TBD Getting started Programming TBD Getting started","title":"Summary"},{"location":"#learning-from-data","text":"Learning from data, especially finite data where the estimand of interest has a causal interpretation is a highly personalized process. The general setup of the process is reflected visually below. We have beliefs about the world; we have beliefs about how a statical procedure behaves under certain conditions; the statistical procedure is applied to a data set providing use with numerical \"results\" which ultimately shape our new beliefs about the world.","title":"Learning from Data"},{"location":"#models","text":"We will never assume that the relationship is linear and moreover, we're not going to be centrally focused on fitting linear models Under selection on observables, linear regression can perform poorly: Reproduced Here We'll work from a more general setup \\[Y_i(D) = f(X_i, D) + \\varepsilon_i(D)\\]","title":"Models"},{"location":"chapters/approximation/asymptotics/","text":"","title":"Asymptotics"},{"location":"chapters/approximation/clusters/","text":"Convex Representation The Caratheodori Theorem tells us that any point \\(x \\in \\textrm{Conv}(T)\\) , where \\(T \\subset \\mathcal{R}^n\\) , can be represented as a convex combination of at most \\(n+1\\) points. 1 So for example let's say that our features \\(x \\in \\mathcal{R}^n\\) . Then for any point in the convex hull of the training set can be represented as the convex combination of at most ten points \\[x = \\sum _{i=1}^{11} \\alpha_i(x)x_i^*(x), \\quad x \\in \\mathcal{R}^{10}\\] Reference: See here \u21a9","title":"Clusters"},{"location":"chapters/approximation/clusters/#convex-representation","text":"The Caratheodori Theorem tells us that any point \\(x \\in \\textrm{Conv}(T)\\) , where \\(T \\subset \\mathcal{R}^n\\) , can be represented as a convex combination of at most \\(n+1\\) points. 1 So for example let's say that our features \\(x \\in \\mathcal{R}^n\\) . Then for any point in the convex hull of the training set can be represented as the convex combination of at most ten points \\[x = \\sum _{i=1}^{11} \\alpha_i(x)x_i^*(x), \\quad x \\in \\mathcal{R}^{10}\\] Reference: See here \u21a9","title":"Convex Representation"},{"location":"chapters/approximation/curse_of_dimensionality/","text":"A pure sampling phenomena Numerical Integration \\[\\int _{[0,1]^d}f dx\\] Monte-Carlo Integration \\[\\begin{align*}\\hat{\\theta} &= \\frac{1}{n}\\sum _{i=1}^n \\big(f(X_i) + \\varepsilon_i) \\\\ &= \\frac{1}{n}\\sum _{i=1}^n f(X_i) + \\frac{1}{n}\\sum _{i=1}^n \\varepsilon_i \\\\ \\\\ \\mathbb{E}[\\hat{\\theta}] &= \\mathbb{E} \\Bigg[\\frac{1}{n}\\sum _{i=1}^n f(X_i) + \\frac{1}{n}\\sum _{i=1}^n \\varepsilon_i \\Bigg] \\\\ &= \\frac{1}{n}\\sum _{i=1}^n \\mathbb{E}\\big[f(X_i)\\big] + \\frac{1}{n}\\sum _{i=1}^n \\mathbb{E}\\underbrace{\\big[\\varepsilon _i\\big]}_{=0} \\\\ &= \\mathbb{E}\\big[f(X_i)\\big] \\\\ \\\\ \\mathbb{E}\\big[(\\hat{\\theta} - \\theta_0 \\big)^2] &= \\textrm{Var}(\\hat{\\theta})\\end{align*}\\]","title":"Curse of Dimensionality"},{"location":"chapters/approximation/curse_of_dimensionality/#numerical-integration","text":"\\[\\int _{[0,1]^d}f dx\\]","title":"Numerical Integration"},{"location":"chapters/approximation/curse_of_dimensionality/#monte-carlo-integration","text":"\\[\\begin{align*}\\hat{\\theta} &= \\frac{1}{n}\\sum _{i=1}^n \\big(f(X_i) + \\varepsilon_i) \\\\ &= \\frac{1}{n}\\sum _{i=1}^n f(X_i) + \\frac{1}{n}\\sum _{i=1}^n \\varepsilon_i \\\\ \\\\ \\mathbb{E}[\\hat{\\theta}] &= \\mathbb{E} \\Bigg[\\frac{1}{n}\\sum _{i=1}^n f(X_i) + \\frac{1}{n}\\sum _{i=1}^n \\varepsilon_i \\Bigg] \\\\ &= \\frac{1}{n}\\sum _{i=1}^n \\mathbb{E}\\big[f(X_i)\\big] + \\frac{1}{n}\\sum _{i=1}^n \\mathbb{E}\\underbrace{\\big[\\varepsilon _i\\big]}_{=0} \\\\ &= \\mathbb{E}\\big[f(X_i)\\big] \\\\ \\\\ \\mathbb{E}\\big[(\\hat{\\theta} - \\theta_0 \\big)^2] &= \\textrm{Var}(\\hat{\\theta})\\end{align*}\\]","title":"Monte-Carlo Integration"},{"location":"chapters/approximation/introduction/","text":"Abstract We're interested in the behavior/performance of Estimators/Algorithms In applied microeconometrics, where we are generally interested in the causal effect of some policy/intervention, we use estimators with the following signature. \\[\\begin{align*} \\mathcal{A} :: \\{ \\mathcal{X} \\times \\mathcal{Y} \\}^n \\to \\mathcal{R}^p\\\\ \\end{align*}\\] Occasionally, these estimators will have an analytical form. For example, if we are interested in the average outcome we may use the following estimator. \\[\\begin{align*} &\\mathcal{A} :: \\{ \\mathcal{X} \\times \\mathcal{Y} \\}^n \\to \\mathcal{R}\\\\ &\\mathcal{A} \\big(\\{x_i, y_i \\})_{i=1}^n\\big) = \\frac{1}{n} \\sum y_i \\\\ \\end{align*}\\] Or if we are interested in the linear approximation to the CEF we may use the following estimator. \\[\\begin{align*} &\\mathcal{A} :: \\{ \\mathcal{X} \\times \\mathcal{Y} \\}^n \\to \\mathcal{R}^p\\\\ &\\mathcal{A} \\big(\\{x_i, y_i \\})_{i=1}^n\\big) = \\big( X^TX)^{-1}X^TY \\\\ \\end{align*}\\] We may, though, be interested in more \"complex\" estimators that involve neural networks. \\[\\begin{align*} &\\mathcal{A} :: \\{ \\mathcal{X} \\times \\mathcal{Y} \\}^n \\to \\mathcal{R}\\\\ &\\mathcal{A} \\big(\\{x_i, y_i \\})_{i=1}^n\\big) = \\sum f(\\theta_1, x_i) - f(\\theta_2, x_i) \\\\ & \\quad \\textrm{where} \\ \\theta_i = m^*\\big(\\{x_i, y_i \\})_{i=1}^n\\big) \\end{align*}\\] To Do Make the connection to kernel methods Probability Space Warning Is the Borel Sigma Algebra well defined here? Should we add restrictions on \\(\\mathcal{X}, \\mathcal{Y}\\) ? \\[\\Big( \\mathcal{X} \\times \\mathcal{Y}, \\mathcal{B}(\\mathcal{X} \\times \\mathcal{Y}), \\mathbb{P}\\Big)\\] Function Space/ Hypothesis Class/ Model Class Warning What additional restrictions should we place on \\(\\mathcal{H}\\) ? \\[ \\mathcal{H} := \\{h \\mid h : \\mathcal{X} \\to \\mathcal{Y} \\}\\] Loss Function Warning Make note on parameterization \\[\\begin{align*} &l : \\mathcal{H} \\to \\mathcal{X} \\to \\mathcal{Y} \\\\ &l(h, x, y) = (y - h(x))^2 \\end{align*}\\] Population Risk \\[\\begin{align*} &L :: \\mathcal{H} \\to \\mathcal{R}_+ \\\\ &L(h) := \\underset{(x,y)\\sim p}{\\mathbb{E}} \\big[l(h, x, y)\\big] \\\\ &L(h)= \\int _{\\mathcal{X} \\times \\mathcal{Y}}l(h, X, Y)d\\mathbb{P} \\end{align*}\\] Empirical Risk \\[ \\begin{align*} &\\hat{L} :: \\{X, Y\\}^n \\to \\Theta \\to \\mathcal{R}_+ \\\\ &\\hat{L}(\\{x_i, y_i\\}_{i=1}^n, \\theta) = \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i) \\end{align*}\\] Partial Evaluation: Expectation Partially evaluated at \\(\\theta\\) , \\(\\hat{L}\\) is a random variable. Taking its expectation \\[ \\begin{align*} \\mathbb{E}\\big[ \\hat{L} _{\\theta}\\big] &= \\mathbb{E}\\Big[ \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i)\\Big] \\\\ &= \\frac{1}{n} \\sum _i \\mathbb{E}\\big[ l(\\theta, x_i, y_i)\\big] \\\\ &= L(\\theta) \\end{align*}\\] Partial Evaluation: Variance \\[ \\begin{align*} \\textrm{Var}(\\hat{L} _{\\theta}) &= \\textrm{Var}\\Big[ \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i)\\Big] \\\\ &= \\frac{1}{n^2} \\sum _i \\textrm{Var}\\big[ l(\\theta, x_i, y_i)\\big] \\\\ &= \\frac{\\textrm{Var}\\big[ l(\\theta, x_i, y_i)\\big]}{n} \\end{align*}\\] Partial Evaluation: Variance Partially evaluated at \\(\\theta\\) , \\(\\hat{L}\\) is a random variable. Taking its expectation \\[ \\begin{align*} \\mathbb{E}\\big[ \\hat{L} _{\\theta}\\big] &= \\mathbb{E}\\Big[ \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i)\\Big] \\\\ &= \\frac{1}{n} \\sum _i \\mathbb{E}\\big[ l(\\theta, x_i, y_i)\\big] \\\\ &= L(\\theta) \\end{align*}\\] Hmm But we are not really interested in this relationship, becuase \\(\\hat{L}\\) is not partially evaluated in practice. In practice, we use our training data to determine \\(\\theta\\) . That is we have an algorithm \\(\\mathcal{A}\\) . Algorithm \\[ \\begin{align*} \\mathcal{A} :: \\{\\mathcal{X}, \\mathcal{Y}\\}^n \\to \\Theta \\end{align*} \\] Empirical Risk Minimization \\[ \\begin{align*} &\\textrm{ERM} :: \\{X, Y\\}^n \\to \\Theta \\\\ &\\textrm{ERM}(\\{x_i, y_i\\}_{i=1}^n) = \\underset{\\theta \\in \\Theta}{\\textrm{minimize}} \\ \\hat{L}(\\{x_i, y_i\\}_{i=1}^n, \\theta) \\end{align*}\\] Consistency: \\[\\mathbb{P}_n ( \\| \\mathcal{A}_n - \\theta _0 \\| > \\varepsilon) \\to 0 \\] Which is just a random variable. We can evaluate it as follows: \\[ \\begin{align*} \\mathbb{E} \\big[ L \\circ \\mathcal{A} \\big] \\end{align*} \\] Excess Risk: \\[ \\begin{align*} &E :: \\mathcal{H} \\to \\mathcal{R}_+ \\\\ &E(h) = L(h) - \\underset{g \\in \\mathcal{H}}{\\inf} L(g) \\end{align*}\\]","title":"Introduction"},{"location":"chapters/approximation/introduction/#probability-space","text":"Warning Is the Borel Sigma Algebra well defined here? Should we add restrictions on \\(\\mathcal{X}, \\mathcal{Y}\\) ? \\[\\Big( \\mathcal{X} \\times \\mathcal{Y}, \\mathcal{B}(\\mathcal{X} \\times \\mathcal{Y}), \\mathbb{P}\\Big)\\]","title":"Probability Space"},{"location":"chapters/approximation/introduction/#function-space-hypothesis-class-model-class","text":"Warning What additional restrictions should we place on \\(\\mathcal{H}\\) ? \\[ \\mathcal{H} := \\{h \\mid h : \\mathcal{X} \\to \\mathcal{Y} \\}\\]","title":"Function Space/ Hypothesis Class/ Model Class"},{"location":"chapters/approximation/introduction/#loss-function","text":"Warning Make note on parameterization \\[\\begin{align*} &l : \\mathcal{H} \\to \\mathcal{X} \\to \\mathcal{Y} \\\\ &l(h, x, y) = (y - h(x))^2 \\end{align*}\\]","title":"Loss Function"},{"location":"chapters/approximation/introduction/#population-risk","text":"\\[\\begin{align*} &L :: \\mathcal{H} \\to \\mathcal{R}_+ \\\\ &L(h) := \\underset{(x,y)\\sim p}{\\mathbb{E}} \\big[l(h, x, y)\\big] \\\\ &L(h)= \\int _{\\mathcal{X} \\times \\mathcal{Y}}l(h, X, Y)d\\mathbb{P} \\end{align*}\\]","title":"Population Risk"},{"location":"chapters/approximation/introduction/#empirical-risk","text":"\\[ \\begin{align*} &\\hat{L} :: \\{X, Y\\}^n \\to \\Theta \\to \\mathcal{R}_+ \\\\ &\\hat{L}(\\{x_i, y_i\\}_{i=1}^n, \\theta) = \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i) \\end{align*}\\] Partial Evaluation: Expectation Partially evaluated at \\(\\theta\\) , \\(\\hat{L}\\) is a random variable. Taking its expectation \\[ \\begin{align*} \\mathbb{E}\\big[ \\hat{L} _{\\theta}\\big] &= \\mathbb{E}\\Big[ \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i)\\Big] \\\\ &= \\frac{1}{n} \\sum _i \\mathbb{E}\\big[ l(\\theta, x_i, y_i)\\big] \\\\ &= L(\\theta) \\end{align*}\\] Partial Evaluation: Variance \\[ \\begin{align*} \\textrm{Var}(\\hat{L} _{\\theta}) &= \\textrm{Var}\\Big[ \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i)\\Big] \\\\ &= \\frac{1}{n^2} \\sum _i \\textrm{Var}\\big[ l(\\theta, x_i, y_i)\\big] \\\\ &= \\frac{\\textrm{Var}\\big[ l(\\theta, x_i, y_i)\\big]}{n} \\end{align*}\\] Partial Evaluation: Variance Partially evaluated at \\(\\theta\\) , \\(\\hat{L}\\) is a random variable. Taking its expectation \\[ \\begin{align*} \\mathbb{E}\\big[ \\hat{L} _{\\theta}\\big] &= \\mathbb{E}\\Big[ \\frac{1}{n} \\sum _i l(\\theta, x_i, y_i)\\Big] \\\\ &= \\frac{1}{n} \\sum _i \\mathbb{E}\\big[ l(\\theta, x_i, y_i)\\big] \\\\ &= L(\\theta) \\end{align*}\\]","title":"Empirical Risk"},{"location":"chapters/approximation/introduction/#hmm","text":"But we are not really interested in this relationship, becuase \\(\\hat{L}\\) is not partially evaluated in practice. In practice, we use our training data to determine \\(\\theta\\) . That is we have an algorithm \\(\\mathcal{A}\\) .","title":"Hmm"},{"location":"chapters/approximation/introduction/#algorithm","text":"\\[ \\begin{align*} \\mathcal{A} :: \\{\\mathcal{X}, \\mathcal{Y}\\}^n \\to \\Theta \\end{align*} \\] Empirical Risk Minimization \\[ \\begin{align*} &\\textrm{ERM} :: \\{X, Y\\}^n \\to \\Theta \\\\ &\\textrm{ERM}(\\{x_i, y_i\\}_{i=1}^n) = \\underset{\\theta \\in \\Theta}{\\textrm{minimize}} \\ \\hat{L}(\\{x_i, y_i\\}_{i=1}^n, \\theta) \\end{align*}\\] Consistency: \\[\\mathbb{P}_n ( \\| \\mathcal{A}_n - \\theta _0 \\| > \\varepsilon) \\to 0 \\] Which is just a random variable. We can evaluate it as follows: \\[ \\begin{align*} \\mathbb{E} \\big[ L \\circ \\mathcal{A} \\big] \\end{align*} \\] Excess Risk: \\[ \\begin{align*} &E :: \\mathcal{H} \\to \\mathcal{R}_+ \\\\ &E(h) = L(h) - \\underset{g \\in \\mathcal{H}}{\\inf} L(g) \\end{align*}\\]","title":"Algorithm"},{"location":"chapters/approximation/kernels/","text":"These notes are taken from the following lectures: Lecture Definitions Dual Space Let \\(X\\) be a vector space. Then the dual space of \\(X\\) , denoted by \\(X^*\\) , is the set of linear bounded functions on \\(X\\) . Reproducing Kernel Hilbert Spaces Let \\(\\mathcal{X}\\) be a set Let \\(F(\\mathcal{X}, \\mathcal{R})\\) be the vector space of funcions defined on \\(\\mathcal{X}\\) . i.e. \\[f_1, f_2 \\in F(\\mathcal{X}, \\mathcal{R}) \\implies \\alpha f_1 + \\beta f_2 \\in F(\\mathcal{X}, \\mathcal{R})\\] Then \\(\\mathcal{H}(\\mathcal{X}, \\mathcal{R}) \\subset F(\\mathcal{X}, \\mathcal{R})\\) is a Reproducing Kernel Hilbert Space if \\(\\mathcal{H}(\\mathcal{X}, \\mathcal{R})\\) is a subspace of \\(F(\\mathcal{X}, \\mathcal{R})\\) \\(\\Big(\\mathcal{H}(\\mathcal{X}, \\mathcal{R}), \\langle \\cdot, \\cdot \\rangle _{\\mathcal{H}} \\Big)\\) is a Hilbert Space Evaluation Functionals, \\(\\textrm{Apply}_x\\) , are continuous \\[ \\textrm{Apply} : \\mathcal{X} \\to \\mathcal{H}(\\mathcal{X}, \\mathcal{R}) \\to \\mathcal{R} \\] Note : We can think of Euclidean Spaces as a function space. \\[\\mathcal{R} ^n \\equiv \\Big( F(\\textrm{Fin} \\ n, \\mathcal{R}), \\langle z_1, z_2 \\rangle _{F} := \\sum _{i=1}^n z_1(i)z_2(i)\\Big)\\] We can generalize this structure to \\(l^2(\\mathcal{X})\\) \\[\\begin{align*} l^2(\\mathcal{X}):= \\Big\\{ f \\mid f:\\mathcal{X} \\to \\mathcal{R}, \\quad \\sum _{x\\in \\mathcal{X}} |f(x)|^2 < \\infty \\Big\\} \\end{align*}\\] This structure (set \\(+\\) the norm/inner product) 1 is an RKHS since \\[\\| E_x \\| \\leq \\| f \\| _{l^2(\\mathcal{X})}\\] Consider the following function \\[\\begin{align*} &\\Lambda :: \\mathcal{H} \\to \\mathcal{H} \\to \\mathcal{R}\\\\ &\\Lambda \\ y \\ x = \\langle x, y \\rangle _{\\mathcal{H}} \\end{align*}\\] We can re-write the signature of the function as follows: \\[\\begin{align*} &\\Lambda :: \\mathcal{H} \\to \\mathcal{H}^*\\\\ \\end{align*}\\] If \\(H\\) is a RKHS, then by definition, \\(\\textrm{Apply} \\ x\\) is a linear bounded functional. By Reisz representation theorem, \\[\\textrm{Apply} \\ x \\ f = \\langle r \\ x, f \\rangle _{\\mathcal{H}} = f \\ x \\] Then we can define the Kernel as follows: \\[\\begin{align*} &K :: \\mathcal{X} \\to \\mathcal{X} \\to \\mathcal{R} \\\\ &K \\ x \\ y = r \\ x \\ y = \\langle r \\ y, r \\ x \\rangle \\end{align*}\\] I really think the key part of this structure is the inner product \u21a9","title":"Kernels"},{"location":"chapters/approximation/kernels/#definitions","text":"Dual Space Let \\(X\\) be a vector space. Then the dual space of \\(X\\) , denoted by \\(X^*\\) , is the set of linear bounded functions on \\(X\\) .","title":"Definitions"},{"location":"chapters/approximation/kernels/#reproducing-kernel-hilbert-spaces","text":"Let \\(\\mathcal{X}\\) be a set Let \\(F(\\mathcal{X}, \\mathcal{R})\\) be the vector space of funcions defined on \\(\\mathcal{X}\\) . i.e. \\[f_1, f_2 \\in F(\\mathcal{X}, \\mathcal{R}) \\implies \\alpha f_1 + \\beta f_2 \\in F(\\mathcal{X}, \\mathcal{R})\\] Then \\(\\mathcal{H}(\\mathcal{X}, \\mathcal{R}) \\subset F(\\mathcal{X}, \\mathcal{R})\\) is a Reproducing Kernel Hilbert Space if \\(\\mathcal{H}(\\mathcal{X}, \\mathcal{R})\\) is a subspace of \\(F(\\mathcal{X}, \\mathcal{R})\\) \\(\\Big(\\mathcal{H}(\\mathcal{X}, \\mathcal{R}), \\langle \\cdot, \\cdot \\rangle _{\\mathcal{H}} \\Big)\\) is a Hilbert Space Evaluation Functionals, \\(\\textrm{Apply}_x\\) , are continuous \\[ \\textrm{Apply} : \\mathcal{X} \\to \\mathcal{H}(\\mathcal{X}, \\mathcal{R}) \\to \\mathcal{R} \\] Note : We can think of Euclidean Spaces as a function space. \\[\\mathcal{R} ^n \\equiv \\Big( F(\\textrm{Fin} \\ n, \\mathcal{R}), \\langle z_1, z_2 \\rangle _{F} := \\sum _{i=1}^n z_1(i)z_2(i)\\Big)\\] We can generalize this structure to \\(l^2(\\mathcal{X})\\) \\[\\begin{align*} l^2(\\mathcal{X}):= \\Big\\{ f \\mid f:\\mathcal{X} \\to \\mathcal{R}, \\quad \\sum _{x\\in \\mathcal{X}} |f(x)|^2 < \\infty \\Big\\} \\end{align*}\\] This structure (set \\(+\\) the norm/inner product) 1 is an RKHS since \\[\\| E_x \\| \\leq \\| f \\| _{l^2(\\mathcal{X})}\\] Consider the following function \\[\\begin{align*} &\\Lambda :: \\mathcal{H} \\to \\mathcal{H} \\to \\mathcal{R}\\\\ &\\Lambda \\ y \\ x = \\langle x, y \\rangle _{\\mathcal{H}} \\end{align*}\\] We can re-write the signature of the function as follows: \\[\\begin{align*} &\\Lambda :: \\mathcal{H} \\to \\mathcal{H}^*\\\\ \\end{align*}\\] If \\(H\\) is a RKHS, then by definition, \\(\\textrm{Apply} \\ x\\) is a linear bounded functional. By Reisz representation theorem, \\[\\textrm{Apply} \\ x \\ f = \\langle r \\ x, f \\rangle _{\\mathcal{H}} = f \\ x \\] Then we can define the Kernel as follows: \\[\\begin{align*} &K :: \\mathcal{X} \\to \\mathcal{X} \\to \\mathcal{R} \\\\ &K \\ x \\ y = r \\ x \\ y = \\langle r \\ y, r \\ x \\rangle \\end{align*}\\] I really think the key part of this structure is the inner product \u21a9","title":"Reproducing Kernel Hilbert Spaces"},{"location":"chapters/optimization/differentiation/","text":"\"If you have a tricky problem to solve, finding a language in which the solution to that problem is compositional is like the most important step. Well, defining the problem clearly is the most important step. The second one is finding a vocabulary in which it's compositional.\" 1 This entire section is taken from Conal Elliot's presentation titled: \" Automatic Differentiation Made Easy Via Category Theory \" A Derivative is a linear map Cateogy Theory is the abstract algebra of functions Let \\(a,b\\) be Banach spaces (complete normed vector spaces) \\[\\mathcal{D} :: (a \\to b) \\to (a \\to (a -\\circ b))\\] Terminology: The derivative of \\(f\\) at \\(a\\) \\[\\underset{\\varepsilon \\to 0}{\\lim} \\frac{ \\| f \\ (a + \\varepsilon) - f \\ a + \\mathcal{D} \\ f \\ a \\ \\varepsilon \\|}{\\| \\varepsilon \\|} = 0\\] Differentiation preserves parallel composition \\[\\begin{align*}&(\\triangle) :: (a \\to b) \\to (a \\to d) \\to (a \\to b \\times d) \\\\ &(f \\ \\triangle \\ g) \\ a = (f \\ a, g \\ a) \\\\ &\\mathcal{D} \\ (f \\ \\triangle \\ g) = \\mathcal{D} \\ f \\ \\triangle \\ \\mathcal{D} \\ g \\end{align*}\\] \\[\\begin{align*} &\\hat{\\mathcal{D}} :: (a \\to b) \\to (a \\to (b \\times (a \\to b))) \\\\ & \\hat{\\mathcal{D}} \\ f = f \\ \\triangle \\ \\mathcal{D} \\ f\\end{align*}\\] Reference: See here \u21a9","title":"Differentiation"},{"location":"chapters/optimization/introduction/","text":"","title":"Introduction"},{"location":"chapters/optimization/overparam/","text":"Belkin's Claim : (1) If we have a solution manifold and (2) if this manifold has curvature then the loss function is not locally convex. Proof Assume we have a solution manifold. \\[f(y) \\geq f(x) + \\nabla f(x)^T(y-x)\\] Lemma Minimizers of Convex Function form a Convex Set Let \\(x_1, x_2\\) be minimizers of a convex function \\(f\\) . i.e. \\(\\forall x \\ f(x) \\geq x_1, x_2\\) Then \\(\\forall \\alpha \\in (0,1), \\alpha x_1 + (1-\\alpha x_2)\\) is also a minimizer of \\(f\\) .","title":"Overparameterization"},{"location":"chapters/optimization/overparam/#belkins-claim","text":"(1) If we have a solution manifold and (2) if this manifold has curvature then the loss function is not locally convex. Proof Assume we have a solution manifold. \\[f(y) \\geq f(x) + \\nabla f(x)^T(y-x)\\] Lemma Minimizers of Convex Function form a Convex Set Let \\(x_1, x_2\\) be minimizers of a convex function \\(f\\) . i.e. \\(\\forall x \\ f(x) \\geq x_1, x_2\\) Then \\(\\forall \\alpha \\in (0,1), \\alpha x_1 + (1-\\alpha x_2)\\) is also a minimizer of \\(f\\) .","title":"Belkin's Claim:"},{"location":"chapters/optimization/papers/","text":"The loss landscape of overparameterized neural networks","title":"Papers"},{"location":"chapters/other/LLM/introduction/","text":"References A Neural Probabilistic Language Model Aim The aim is to \"learn\" \\(\\mathbb{P}\\) . \\[\\Big(\\{\\textrm{Sequences of Words}\\}, \\mathcal{F}, \\mathbb{P} \\Big)\\] High Level Approach We can achieve this objective as follows: Learn a distribution over the first word of a sequence. \\[\\big(\\mathcal{V}, \\mathcal{F}, \\mathbb{P} \\big)\\] Learn the conditional distribution \\[\\textrm{Model} :: \\textrm{Params} \\to \\{\\textrm{context}\\} \\to \\mathbb{P}_{\\mid \\textrm{context}}\\] To Do This should be made more exact A Neural Probabilistic Language Model Essence transfer probability mass In the proposed model, it will so generalize because \u201csimilar\u201d words are expected to have a similar feature vector, and because the probability function is a smooth function of these feature values, a small change in the features will induce a small change in the probability We can construct the condition distribution as follows. First we introduce an embedding function. Given that our vocab is finite, we can represent this function as a matrix. i.e. \\(h\\) is isomorphic to \\(\\theta \\in {| \\mathcal{V} | \\times m}\\) \\[h :: \\mathcal{V} \\to \\mathcal{R}^p\\] We introduce a function \\(g\\) which maps subsequences of these embeddings into a conditional distribution Given this level of detail we could augment the signature of our model as follows: \\[\\textrm{Model} :: \\textrm{Embedding Functions} \\to \\textrm{Forward Functions} \\to \\{\\textrm{context}\\} \\to \\mathbb{P}_{\\mid \\textrm{context}}\\] To Do What should the name of this forward function be? Key Insights \"In high dimensions, it is crucial to distribute probability mass where it matters rather than uniformly in all directions around each training point.\" 1 A Neural Probabilistic Language Model \u21a9","title":"Introduction"},{"location":"chapters/other/LLM/introduction/#aim","text":"The aim is to \"learn\" \\(\\mathbb{P}\\) . \\[\\Big(\\{\\textrm{Sequences of Words}\\}, \\mathcal{F}, \\mathbb{P} \\Big)\\]","title":"Aim"},{"location":"chapters/other/LLM/introduction/#high-level-approach","text":"We can achieve this objective as follows: Learn a distribution over the first word of a sequence. \\[\\big(\\mathcal{V}, \\mathcal{F}, \\mathbb{P} \\big)\\] Learn the conditional distribution \\[\\textrm{Model} :: \\textrm{Params} \\to \\{\\textrm{context}\\} \\to \\mathbb{P}_{\\mid \\textrm{context}}\\] To Do This should be made more exact","title":"High Level Approach"},{"location":"chapters/other/LLM/introduction/#a-neural-probabilistic-language-model","text":"Essence transfer probability mass In the proposed model, it will so generalize because \u201csimilar\u201d words are expected to have a similar feature vector, and because the probability function is a smooth function of these feature values, a small change in the features will induce a small change in the probability We can construct the condition distribution as follows. First we introduce an embedding function. Given that our vocab is finite, we can represent this function as a matrix. i.e. \\(h\\) is isomorphic to \\(\\theta \\in {| \\mathcal{V} | \\times m}\\) \\[h :: \\mathcal{V} \\to \\mathcal{R}^p\\] We introduce a function \\(g\\) which maps subsequences of these embeddings into a conditional distribution Given this level of detail we could augment the signature of our model as follows: \\[\\textrm{Model} :: \\textrm{Embedding Functions} \\to \\textrm{Forward Functions} \\to \\{\\textrm{context}\\} \\to \\mathbb{P}_{\\mid \\textrm{context}}\\] To Do What should the name of this forward function be?","title":"A Neural Probabilistic Language Model"},{"location":"chapters/other/LLM/introduction/#key-insights","text":"\"In high dimensions, it is crucial to distribute probability mass where it matters rather than uniformly in all directions around each training point.\" 1 A Neural Probabilistic Language Model \u21a9","title":"Key Insights"},{"location":"chapters/probability%20theory/clusters/","text":"Propensity Score The propensity score is observable: \\[\\pi(X_i) = \\mathbb{P}(D_i = 1 \\mid X_i)\\] That is, given access to the following probability space, we can compute the probability measure. \\[\\big( \\mathcal{X}\\times \\mathcal{D} \\times \\mathcal{Y}, \\mathcal{F}, \\mathcal{P} \\big)\\] To Do Add in the details","title":"Clusters"},{"location":"chapters/probability%20theory/clusters/#propensity-score","text":"The propensity score is observable: \\[\\pi(X_i) = \\mathbb{P}(D_i = 1 \\mid X_i)\\] That is, given access to the following probability space, we can compute the probability measure. \\[\\big( \\mathcal{X}\\times \\mathcal{D} \\times \\mathcal{Y}, \\mathcal{F}, \\mathcal{P} \\big)\\] To Do Add in the details","title":"Propensity Score"},{"location":"chapters/probability%20theory/conditional_expectation/","text":"Although the CEF is random variable, it's not \"defined\" pointwise. \\[\\begin{align*} &\\mathbb{E}[Y|X] :: \\Omega \\to \\mathcal{R} \\\\ & \\mathcal{I} \\ \\mathbb{P} \\ A \\ Y = \\mathcal{I} \\ \\mathbb{P} \\ A \\ \\mathbb{E}[Y|X] \\quad \\forall \\ A \\in \\sigma(X) \\end{align*}\\] The law of iteration of expectations is part of the definition \\[\\begin{align*} & \\mathcal{I} \\ \\mathbb{P} \\ \\Omega \\ Y = \\mathcal{I} \\ \\mathbb{P} \\ \\Omega \\ \\mathbb{E}[Y|X] \\end{align*}\\]","title":"Conditional Expectation"},{"location":"chapters/probability%20theory/conditioning/","text":"Conditioning Given a measurable space, \\((\\Omega, \\mathcal{F})\\) , let \\(\\mathcal{M}\\) be the set of probability measures defined on this space. Then conditioning can be defined as follows: \\[\\begin{align*} &C :: \\mathcal{M} \\to \\mathcal{F}_+ \\to \\mathcal{M} \\\\ & C \\ \\mathbb{P} \\ A \\ B = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(A)} \\end{align*}\\] Two things to note here: Notice the dependent structure between \\(\\mathcal{M}\\) and \\(\\mathcal{F}_+\\) . The measure restricts the set of events that we can condition on. Notice also that if we rearrange the signature of the function, as done below, \\(C \\ \\Omega\\) would be the identity function. \\[C ::\\mathcal{F}_+ \\to \\mathcal{M} \\to \\mathcal{M}\\]","title":"Conditioning"},{"location":"chapters/probability%20theory/conditioning/#conditioning","text":"Given a measurable space, \\((\\Omega, \\mathcal{F})\\) , let \\(\\mathcal{M}\\) be the set of probability measures defined on this space. Then conditioning can be defined as follows: \\[\\begin{align*} &C :: \\mathcal{M} \\to \\mathcal{F}_+ \\to \\mathcal{M} \\\\ & C \\ \\mathbb{P} \\ A \\ B = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(A)} \\end{align*}\\] Two things to note here: Notice the dependent structure between \\(\\mathcal{M}\\) and \\(\\mathcal{F}_+\\) . The measure restricts the set of events that we can condition on. Notice also that if we rearrange the signature of the function, as done below, \\(C \\ \\Omega\\) would be the identity function. \\[C ::\\mathcal{F}_+ \\to \\mathcal{M} \\to \\mathcal{M}\\]","title":"Conditioning"},{"location":"chapters/probability%20theory/convergence/","text":"Reference: STATS 203 - Large Sample Theory TLDR The defining feature of convergence is the restriction on the underlying probability space. In convergence in Law/distribution, we do not require \\(X_n\\) and \\(X\\) to be defined on the same probability space. In convergence in probability or expectation, we require that for each \\(n\\) , \\(X_n\\) and \\(X\\) are defined on the same probability space. i.e. they share the same probability space, but this probability space is allowed to change with \\(n\\) . In convergence almost surley, the underlying probability space for \\(X_n\\) and \\(X\\) must be the same and fixed for all \\(n\\) . Motivation We are interested in the following function \\[ \\begin{align*} &J :: \\{n\\} \\to P(\\Omega_n) \\to (\\theta^{d(n)} \\to \\mathcal{R}) \\to \\\\ &J \\ \\Omega_n \\ \\mathbb{P}_n \\ (\\hat{g}_n \\circ \\hat{\\theta}_n) \\end{align*}\\] Convergence in Distribution (or Law) A random variable \\(X_n\\) convergences in distribution (or in law )to \\(X\\) when the corresponding sequence of CDFs converge \"pointwise\" to the CDF of \\(X\\) \\[\\begin{align} X_n \\to_D X \\implies F_{X_n} \\to F_X \\end{align}\\] Helly-Bray Theorem TLDR If the parameters of our model converge in distribution or law, then we have an asymptotically unbiased estimate. Let \\(\\theta_n \\to_D \\theta\\) . Let \\(g\\) be a continuous and bounded function. Then: \\[I \\ \\Omega_1 \\ \\mathcal{P}_n \\ (g \\circ \\theta_n) \\to I \\ \\Omega_2 \\ \\mathcal{P} \\ (g \\circ \\theta)\\] Note, if we define our estimator \\(\\hat{\\beta}_n\\) as follows. \\[\\hat{\\beta}_n := g \\circ \\theta_n = \\int f(\\theta_n, X) d\\mathbb{P}_X\\] Then this theorem tells us that \\[\\int \\hat{\\beta}_n d\\mathbb{P}_n \\to \\beta\\] Convergence in Probability If the limit is a random variable, then we care aboout the joint distribution! A random variable \\(X_n\\) converges in probability to \\(X\\) when for all \\(\\varepsilon > 0\\) the following holds: \\[\\begin{align} \\underset{n \\to \\infty}{\\lim}\\mathbb{P}(\\|X_n - X \\| > \\varepsilon)= 0 \\end{align}\\] Applications Weak Law of Large Numbers Proof given here Let \\(X_1, \\dots, X_n\\) be a family of i.i.d random variables with a finite second moment. Then \\[\\underset{n \\to \\infty}{\\lim} \\mathbb{E}\\Big[\\Big( \\frac{X_1 + \\dots + X_n}{n} - \\mathbb{E}[X]\\Big)^2\\Big] = 0\\]","title":"Convergence"},{"location":"chapters/probability%20theory/convergence/#motivation","text":"We are interested in the following function \\[ \\begin{align*} &J :: \\{n\\} \\to P(\\Omega_n) \\to (\\theta^{d(n)} \\to \\mathcal{R}) \\to \\\\ &J \\ \\Omega_n \\ \\mathbb{P}_n \\ (\\hat{g}_n \\circ \\hat{\\theta}_n) \\end{align*}\\]","title":"Motivation"},{"location":"chapters/probability%20theory/convergence/#convergence-in-distribution-or-law","text":"A random variable \\(X_n\\) convergences in distribution (or in law )to \\(X\\) when the corresponding sequence of CDFs converge \"pointwise\" to the CDF of \\(X\\) \\[\\begin{align} X_n \\to_D X \\implies F_{X_n} \\to F_X \\end{align}\\] Helly-Bray Theorem TLDR If the parameters of our model converge in distribution or law, then we have an asymptotically unbiased estimate. Let \\(\\theta_n \\to_D \\theta\\) . Let \\(g\\) be a continuous and bounded function. Then: \\[I \\ \\Omega_1 \\ \\mathcal{P}_n \\ (g \\circ \\theta_n) \\to I \\ \\Omega_2 \\ \\mathcal{P} \\ (g \\circ \\theta)\\] Note, if we define our estimator \\(\\hat{\\beta}_n\\) as follows. \\[\\hat{\\beta}_n := g \\circ \\theta_n = \\int f(\\theta_n, X) d\\mathbb{P}_X\\] Then this theorem tells us that \\[\\int \\hat{\\beta}_n d\\mathbb{P}_n \\to \\beta\\]","title":"Convergence in Distribution (or Law)"},{"location":"chapters/probability%20theory/convergence/#convergence-in-probability","text":"If the limit is a random variable, then we care aboout the joint distribution! A random variable \\(X_n\\) converges in probability to \\(X\\) when for all \\(\\varepsilon > 0\\) the following holds: \\[\\begin{align} \\underset{n \\to \\infty}{\\lim}\\mathbb{P}(\\|X_n - X \\| > \\varepsilon)= 0 \\end{align}\\]","title":"Convergence in Probability"},{"location":"chapters/probability%20theory/convergence/#applications","text":"","title":"Applications"},{"location":"chapters/probability%20theory/convergence/#weak-law-of-large-numbers","text":"Proof given here Let \\(X_1, \\dots, X_n\\) be a family of i.i.d random variables with a finite second moment. Then \\[\\underset{n \\to \\infty}{\\lim} \\mathbb{E}\\Big[\\Big( \\frac{X_1 + \\dots + X_n}{n} - \\mathbb{E}[X]\\Big)^2\\Big] = 0\\]","title":"Weak Law of Large Numbers"},{"location":"chapters/probability%20theory/expectations/","text":"Introduction Integration can be thought of as a function that takes a probability measure, a set, and a random variable \\[\\begin{align*} &\\mathcal{I} :: \\mathcal{F} \\to \\mathcal{M} \\to \\{\\Omega \\to \\mathcal{R} \\} \\to \\mathcal{R} \\cup \\{ \\pm \\infty\\}\\\\ &\\mathcal{I} \\ A \\ \\mathbb{P} \\ X = \\int _A X d\\mathbb{P} \\end{align*}\\] Lebesgue Integral The lebesgue intergral is simply the following where \\(\\lambda\\) denotes the lebesgue measure \\[\\mathcal{I}_{\\lambda}\\] Linearity You have probably heard in various classes that integration is a linear function. What people mean by this is that the following higher-order function is linear \\[\\mathcal{I} \\ \\mathcal{P} \\ A \\] Working Across Probability Spaces \\[\\int _A fd\\mathbb{P} = \\int_{f(A)} x d\\mathbb{P}_f\\] To Do is \\(f(A)\\) a measurable set??","title":"Expectation"},{"location":"chapters/probability%20theory/expectations/#introduction","text":"Integration can be thought of as a function that takes a probability measure, a set, and a random variable \\[\\begin{align*} &\\mathcal{I} :: \\mathcal{F} \\to \\mathcal{M} \\to \\{\\Omega \\to \\mathcal{R} \\} \\to \\mathcal{R} \\cup \\{ \\pm \\infty\\}\\\\ &\\mathcal{I} \\ A \\ \\mathbb{P} \\ X = \\int _A X d\\mathbb{P} \\end{align*}\\]","title":"Introduction"},{"location":"chapters/probability%20theory/expectations/#lebesgue-integral","text":"The lebesgue intergral is simply the following where \\(\\lambda\\) denotes the lebesgue measure \\[\\mathcal{I}_{\\lambda}\\]","title":"Lebesgue Integral"},{"location":"chapters/probability%20theory/expectations/#linearity","text":"You have probably heard in various classes that integration is a linear function. What people mean by this is that the following higher-order function is linear \\[\\mathcal{I} \\ \\mathcal{P} \\ A \\]","title":"Linearity"},{"location":"chapters/probability%20theory/expectations/#working-across-probability-spaces","text":"\\[\\int _A fd\\mathbb{P} = \\int_{f(A)} x d\\mathbb{P}_f\\] To Do is \\(f(A)\\) a measurable set??","title":"Working Across Probability Spaces"},{"location":"chapters/probability%20theory/independence/","text":"Events Given a probability space \\[\\big(\\Omega, \\mathcal{F}, \\mathbb{P}\\big)\\] Two Events : \\(A, B\\) are independent under \\(\\mathbb{P}\\) if \\[\\mathbb{P}( A \\cap B) = \\mathbb{P}(A) \\mathbb{P}(B)\\] Finite Collection of Events : \\(A_1, A_2, \\dots, A_n\\) are independent if \\[\\forall I_0 \\subset \\{1,2, \\dots, n\\}, \\quad \\mathbb{P}\\big(\\cap _{i \\in I_0} A_i \\big) = \\prod _{i \\in I_0} \\mathbb{P}(A_i)\\] Arbitrary Collection of Events : \\(\\{A_i, i \\in I\\}\\) Independent if for any finite subset, the independent condition defined above holds Sub- \\(\\sigma\\) -algebras Two sub- \\(\\sigma\\) -algebras : \\(\\mathcal{F}_1, \\mathcal{F}_2\\) \\[\\forall A_1 \\in \\mathcal{F}_1, \\forall A_2 \\in \\mathcal{F}_2, \\quad \\mathbb{P}( A_1 \\cap A_2) = \\mathbb{P}(A_1) \\mathbb{P}(A_2)\\] Arbitrary Collection of Sub- \\(\\sigma\\) -algebras : \\(\\{\\mathcal{F}_i, i \\in I\\}\\) Are independent if for any subset of \\(\\Omega\\) selected from any sub- \\(\\sigma\\) -algebras, that collection of subsets is indepdent as defined above Random Variables \\(X, Y\\) are independent if the \\(\\sigma\\) -algebras generated by these random variables are independent as defined above \\(\\sigma\\) -algebra generated by the random variable \\(X\\) \\[\\sigma(X) := \\{A \\in \\mathcal{F} \\mid A = X^{-1}(B) \\ \\textrm{for} \\ B \\in \\mathcal{B}(\\mathcal{R})\\}\\]","title":"Independence"},{"location":"chapters/probability%20theory/independence/#events","text":"Given a probability space \\[\\big(\\Omega, \\mathcal{F}, \\mathbb{P}\\big)\\] Two Events : \\(A, B\\) are independent under \\(\\mathbb{P}\\) if \\[\\mathbb{P}( A \\cap B) = \\mathbb{P}(A) \\mathbb{P}(B)\\] Finite Collection of Events : \\(A_1, A_2, \\dots, A_n\\) are independent if \\[\\forall I_0 \\subset \\{1,2, \\dots, n\\}, \\quad \\mathbb{P}\\big(\\cap _{i \\in I_0} A_i \\big) = \\prod _{i \\in I_0} \\mathbb{P}(A_i)\\] Arbitrary Collection of Events : \\(\\{A_i, i \\in I\\}\\) Independent if for any finite subset, the independent condition defined above holds","title":"Events"},{"location":"chapters/probability%20theory/independence/#sub-sigma-algebras","text":"Two sub- \\(\\sigma\\) -algebras : \\(\\mathcal{F}_1, \\mathcal{F}_2\\) \\[\\forall A_1 \\in \\mathcal{F}_1, \\forall A_2 \\in \\mathcal{F}_2, \\quad \\mathbb{P}( A_1 \\cap A_2) = \\mathbb{P}(A_1) \\mathbb{P}(A_2)\\] Arbitrary Collection of Sub- \\(\\sigma\\) -algebras : \\(\\{\\mathcal{F}_i, i \\in I\\}\\) Are independent if for any subset of \\(\\Omega\\) selected from any sub- \\(\\sigma\\) -algebras, that collection of subsets is indepdent as defined above","title":"Sub-\\(\\sigma\\)-algebras"},{"location":"chapters/probability%20theory/independence/#random-variables","text":"\\(X, Y\\) are independent if the \\(\\sigma\\) -algebras generated by these random variables are independent as defined above \\(\\sigma\\) -algebra generated by the random variable \\(X\\) \\[\\sigma(X) := \\{A \\in \\mathcal{F} \\mid A = X^{-1}(B) \\ \\textrm{for} \\ B \\in \\mathcal{B}(\\mathcal{R})\\}\\]","title":"Random Variables"},{"location":"chapters/probability%20theory/inference/","text":"The Inference Ladder Our estimator can be defined via the following components: \\[\\begin{align*}\\theta_n &:: \\Omega_n \\to \\mathcal{R}^{d(n)} \\\\ \\\\ f_n &:: \\mathcal{R}^{d(n)} \\to \\mathcal{X} \\to \\mathcal{R} \\\\ \\\\ \\gamma _n &:: (\\mathcal{X} \\to \\mathcal{R}) \\to \\Omega_n \\to \\mathcal{R} \\end{align*}\\] Our estimator is constructed by composing these elements as follows: \\[\\begin{align*}\\theta_n &:: \\Omega_n \\to \\mathcal{R}^{d(n)} \\\\ \\\\ f_n \\circ \\theta_n &:: \\Omega_n \\to \\mathcal{X} \\to \\mathcal{R} \\\\ \\\\ \\gamma _n \\circ f_n \\circ \\theta_n &:: \\Omega_n \\to \\Omega_n \\to \\mathcal{R} \\end{align*}\\] Random Function(als) A random variable has the following type signature: \\[Z :: \\Omega \\to \\mathcal{R}\\] A random function as the following type signature: \\[Z :: \\Omega \\to \\mathcal{X} \\to \\mathcal{R}\\] A random funtional as the following type signature: \\[Z :: \\Omega \\to (\\mathcal{X} \\to \\mathcal{R}) \\to \\mathcal{R}\\] From this, we observe that \\(f_n \\circ \\theta_n\\) is a random function and that \\(\\gamma _n\\) is a random functional Other \\[\\begin{align*} \\hat{g} :: \\Omega_n \\to \\mathcal{X} \\to \\mathcal{R}\\end{align*}\\] Partial Convergence","title":"Inference"},{"location":"chapters/probability%20theory/inference/#the-inference-ladder","text":"Our estimator can be defined via the following components: \\[\\begin{align*}\\theta_n &:: \\Omega_n \\to \\mathcal{R}^{d(n)} \\\\ \\\\ f_n &:: \\mathcal{R}^{d(n)} \\to \\mathcal{X} \\to \\mathcal{R} \\\\ \\\\ \\gamma _n &:: (\\mathcal{X} \\to \\mathcal{R}) \\to \\Omega_n \\to \\mathcal{R} \\end{align*}\\] Our estimator is constructed by composing these elements as follows: \\[\\begin{align*}\\theta_n &:: \\Omega_n \\to \\mathcal{R}^{d(n)} \\\\ \\\\ f_n \\circ \\theta_n &:: \\Omega_n \\to \\mathcal{X} \\to \\mathcal{R} \\\\ \\\\ \\gamma _n \\circ f_n \\circ \\theta_n &:: \\Omega_n \\to \\Omega_n \\to \\mathcal{R} \\end{align*}\\] Random Function(als) A random variable has the following type signature: \\[Z :: \\Omega \\to \\mathcal{R}\\] A random function as the following type signature: \\[Z :: \\Omega \\to \\mathcal{X} \\to \\mathcal{R}\\] A random funtional as the following type signature: \\[Z :: \\Omega \\to (\\mathcal{X} \\to \\mathcal{R}) \\to \\mathcal{R}\\] From this, we observe that \\(f_n \\circ \\theta_n\\) is a random function and that \\(\\gamma _n\\) is a random functional Other \\[\\begin{align*} \\hat{g} :: \\Omega_n \\to \\mathcal{X} \\to \\mathcal{R}\\end{align*}\\]","title":"The Inference Ladder"},{"location":"chapters/probability%20theory/inference/#partial-convergence","text":"","title":"Partial Convergence"},{"location":"chapters/probability%20theory/introduction/","text":"In many applied contexts we are interested in the behavior of the estimator and the interpretation of the estimate. Our starting point for this discussion is a probability space. Well, actually several of them, but they can be understood as transformations of the original one, so let's start there. Unobserved \\[\\Big( \\mathcal{Y} \\times \\mathcal{Y} \\times \\mathcal{X} \\times \\mathcal{D}, \\mathcal{F}, \\mathbb{P}_0\\Big)\\] Observed \\[\\Big( \\mathcal{Y} \\times \\mathcal{X} \\times \\mathcal{D}, \\mathcal{F} , \\mathbb{P}\\Big)\\] \\(Sample\\) \\[\\begin{align*} &D_{i}:: \\Omega \\to \\mathcal{R} \\\\ &D_{i} \\ (\\_ \\ \\_ \\ d) = d \\end{align*}\\] \\(Y_i\\) \\[\\begin{align*} &Y_{i}:: \\Omega \\to \\mathcal{R} \\\\ &Y_{i} \\ (y_0 \\ y_1 \\ d) = dy_1 + (1-d)y_0 \\end{align*}\\] Given this probability space, we can then define the random variables of interest as follows: \\(Y_{i0}\\) \\[\\begin{align*} &Y_{i0}:: \\Omega \\to \\mathcal{R} \\\\ &Y_{i0} \\ (y_0 \\ \\_ \\ \\_) = y_0 \\end{align*}\\] \\(Y_{i1}\\) \\[\\begin{align*} &Y_{i1}:: \\Omega \\to \\mathcal{R} \\\\ &Y_{i1} \\ (\\_ \\ y_1 \\ \\_) = y_1 \\end{align*}\\] \\(D_i\\) \\[\\begin{align*} &D_{i}:: \\Omega \\to \\mathcal{R} \\\\ &D_{i} \\ (\\_ \\ \\_ \\ d) = d \\end{align*}\\] \\(Y_i\\) \\[\\begin{align*} &Y_{i}:: \\Omega \\to \\mathcal{R} \\\\ &Y_{i} \\ (y_0 \\ y_1 \\ d) = dy_1 + (1-d)y_0 \\end{align*}\\] We can say that treatment is indepdent of the potential outcome if the corresponding \\(\\sigma\\) -algebras are independent. More intuitively, this is equivalent (as shown here ). \\[\\forall B_1,B_2 \\in \\mathcal{B}(\\mathcal{R}), \\quad \\mathbb{P}(D_i \\in B_1, Y_{i0} \\in B_2) = \\mathbb{P}(D_i \\in B_1)\\mathbb{P}(Y_{i0} \\in B_2)\\] Working Across Probability Spaces Independence you will often here that treatment is independent of the potential outcomes. While you probably have an intuitive sense of what this means, it can be helpful to formally define this. To start, let's consider the following probability spaces: Expectations Many terms/properties can be understood as working across multiple probability spaces. \\[\\int _A fd\\mathbb{P} = \\int_{f(A)} x d\\mathbb{P}_f\\] To Do is \\(f(A)\\) a measurable set??","title":"Introduction"},{"location":"chapters/probability%20theory/introduction/#working-across-probability-spaces","text":"","title":"Working Across Probability Spaces"},{"location":"chapters/probability%20theory/introduction/#independence","text":"you will often here that treatment is independent of the potential outcomes. While you probably have an intuitive sense of what this means, it can be helpful to formally define this. To start, let's consider the following probability spaces:","title":"Independence"},{"location":"chapters/probability%20theory/introduction/#expectations","text":"Many terms/properties can be understood as working across multiple probability spaces. \\[\\int _A fd\\mathbb{P} = \\int_{f(A)} x d\\mathbb{P}_f\\] To Do is \\(f(A)\\) a measurable set??","title":"Expectations"},{"location":"chapters/probability%20theory/key_terms/","text":"\\(\\sigma\\) -algebra of a Random Variable Information. What information about the sample space, \\(\\Omega\\) , does a random variable, \\(X\\) , convey. For any element of the Borel \\(\\sigma\\) -algebra we can tell whether that element occured. \\[X(\\omega) \\in B, \\quad \\textrm{or} \\quad X(\\omega) \\notin B\\] Therefore, we know \\[ \\omega \\in X^{-1}(B), \\quad \\textrm{or} \\quad \\omega \\notin X^{-1}(B)\\] We refer to this set of events as the \\(\\sigma\\) -algebra generated by \\(X\\) . Importantly, it is the set of events that we can tell whether or not they occured if we have \\(X\\) .","title":"Key Terms"},{"location":"chapters/probability%20theory/key_terms/#sigma-algebra-of-a-random-variable","text":"Information. What information about the sample space, \\(\\Omega\\) , does a random variable, \\(X\\) , convey. For any element of the Borel \\(\\sigma\\) -algebra we can tell whether that element occured. \\[X(\\omega) \\in B, \\quad \\textrm{or} \\quad X(\\omega) \\notin B\\] Therefore, we know \\[ \\omega \\in X^{-1}(B), \\quad \\textrm{or} \\quad \\omega \\notin X^{-1}(B)\\] We refer to this set of events as the \\(\\sigma\\) -algebra generated by \\(X\\) . Importantly, it is the set of events that we can tell whether or not they occured if we have \\(X\\) .","title":"\\(\\sigma\\)-algebra of a Random Variable"},{"location":"chapters/probability%20theory/key_theorems/","text":"Gilvenko-Cantelli The claim is that \\(\\underset{x}{\\textrm{sup}}\\ g\\) (defined below!) converges \"almost surely\" to \\(0\\) . \\[\\begin{align*} &g :: \\mathcal{X} \\to \\Omega \\to \\mathcal{R} \\\\ &g \\ x \\ \\omega = F_n(x, \\omega) - F(x) \\\\ \\\\ &\\underset{x}{\\textrm{sup}}\\ g :: \\Omega \\to \\mathcal{R} \\end{align*}\\]","title":"Key Theorems"},{"location":"chapters/programming/introduction/","text":"","title":"Introduction"},{"location":"chapters/programming/category%20theory/natural%20transformations/","text":"\\[\\textrm{Nautral Transformations} :: \\{\\textrm{Functiioins}\\}\\]","title":"Natural Transformations"}]}